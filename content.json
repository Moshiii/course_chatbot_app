{"main_notes.pdf": [{"page_number": 0, "text": "CS229LectureNotes\nAndrewNg\nUpdatedbyTengyuMa\n"}, {"page_number": 1, "text": "Contents\nISupervisedlearning5\n1Linearregression8\n1.1LMSalgorithm..........................\n9\n1.2Thenormalequations.......................\n13\n1.2.1Matrixderivatives.....................\n13\n1.2.2Leastsquaresrevisited..................\n14\n1.3Probabilisticinterpretation....................\n15\n1.4Locallyweightedlinearregression(optionalreading)......\n17\n2andlogisticregression20\n2.1Logisticregression........................\n20\n2.2Digression:theperceptronlearningalgorithn..........\n23\n2.3Anotheralgorithmformaximizing\n`\n(\n\n).............\n24\n3Generalizedlinearmodels26\n3.1Theexponentialfamily......................\n26\n3.2ConstructingGLMs........................\n28\n3.2.1Ordinaryleastsquares..................\n29\n3.2.2Logisticregression....................\n30\n3.2.3Softmaxregression....................\n30\n4Generativelearningalgorithms35\n4.1Gaussiandiscriminantanalysis..................\n36\n4.1.1Themultivariatenormaldistribution..........\n36\n4.1.2TheGaussiandiscriminantanalysismodel.......\n39\n4.1.3Discussion:GDAandlogisticregression........\n41\n4.2Naivebayes............................\n42\n4.2.1Laplacesmoothing....................\n45\n4.2.2Eventmodelsfortextc...........\n47\n1\n"}, {"page_number": 2, "text": "CS229Spring2022\n2\n5Kernelmethods49\n5.1Featuremaps...........................\n49\n5.2LMS(leastmeansquares)withfeatures.............\n50\n5.3LMSwiththekerneltrick....................\n50\n5.4Propertiesofkernels.......................\n54\n6Supportvectormachines60\n6.1Margins:intuition.........................\n60\n6.2Notation(optionreading)....................\n62\n6.3Functionalandgeometricmargins(optionreading)......\n62\n6.4Theoptimalmargin(optionreading).........\n64\n6.5Lagrangeduality(optionalreading)...............\n66\n6.6Optimalmarginthedualform(optionreading)..\n69\n6.7Regularizationandthenon-separablecase(optionalreading).\n73\n6.8TheSMOalgorithm(optionalreading).............\n74\n6.8.1Coordinateascent.....................\n75\n6.8.2SMO............................\n76\nIIDeeplearning80\n7Deeplearning81\n7.1Supervisedlearningwithnon-linearmodels...........\n81\n7.2Neuralnetworks..........................\n83\n7.3Backpropagation.........................\n92\n7.3.1Preliminary:chainrule..................\n93\n7.3.2One-neuronneuralnetworks...............\n93\n7.3.3Two-layerneuralnetworks:alow-levelunpackedcom-\nputation..........................\n94\n7.3.4Two-layerneuralnetworkwithvectornotation.....\n97\n7.3.5Multi-layerneuralnetworks...............\n99\n7.4Vectorizationovertrainingexamples..............\n99\nIIIGeneralizationandregularization102\n8Generalization103\n8.1Bias-variance.......................\n105\n8.1.1Amathematicaldecomposition(forregression).....\n110\n8.2Thedoubledescentphenomenon.................\n111\n"}, {"page_number": 3, "text": "CS229Spring2022\n3\n8.3Samplecomplexitybounds(optionalreadings).........\n116\n8.3.1Preliminaries.......................\n116\n8.3.2Thecaseof\nH\n....................\n118\n8.3.3Thecaseof\nH\n..................\n121\n9Regularizationandmodelselection125\n9.1Regularization...........................\n125\n9.2Implicitregularization...................\n127\n9.3Modelselectionviacrossvalidation...............\n129\n9.4Bayesianstatisticsandregularization..............\n132\nIVUnsupervisedlearning134\n10Clusteringandthe\nk\n-meansalgorithm135\n11EMalgorithms138\n11.1EMformixtureofGaussians...................\n138\n11.2Jensen'sinequality........................\n141\n11.3GeneralEMalgorithms......................\n142\n11.3.1OtherinterpretationofELBO..............\n148\n11.4MixtureofGaussiansrevisited..................\n148\n11.5Variationalinferenceandvariationalauto-encoder(optional\nreading)..............................\n150\n12Principalcomponentsanalysis155\n13Independentcomponentsanalysis161\n13.1ICAambiguities..........................\n162\n13.2Densitiesandlineartransformations...............\n163\n13.3ICAalgorithm...........................\n164\n14Self-supervisedlearningandfoundationmodels167\n14.1Pretrainingandadaptation....................\n167\n14.2Pretrainingmethodsincomputervision.............\n169\n14.3Pretrainedlargelanguagemodels................\n171\n14.3.1Zero-shotlearningandin-contextlearning.......\n173\n"}, {"page_number": 4, "text": "CS229Spring2022\n4\nVReinforcementLearningandControl175\n15Reinforcementlearning176\n15.1Markovdecisionprocesses....................\n177\n15.2Valueiterationandpolicyiteration...............\n179\n15.3LearningamodelforanMDP..................\n181\n15.4ContinuousstateMDPs.....................\n183\n15.4.1Discretization.......................\n183\n15.4.2Valuefunctionapproximation..............\n186\n15.5ConnectionsbetweenPolicyandValueIteration(Optional)..\n190\n16LQR,DDPandLQG193\n16.1Finite-horizonMDPs.......................\n193\n16.2LinearQuadraticRegulation(LQR)...............\n197\n16.3Fromnon-lineardynamicstoLQR...............\n200\n16.3.1Linearizationofdynamics................\n201\n16.3.2tialDynamicProgramming(DDP).......\n201\n16.4LinearQuadraticGaussian(LQG)................\n203\n17PolicyGradient(REINFORCE)207\n"}, {"page_number": 5, "text": "PartI\nSupervisedlearning\n5\n"}, {"page_number": 6, "text": "6\nLet'sstartbytalkingaboutafewexamplesofsupervisedlearningprob-\nlems.Supposewehaveadatasetgivingthelivingareasandpricesof47\nhousesfromPortland,Oregon:\nLivingarea(feet\n2\n)\nPrice(1000\n$\ns)\n2104\n400\n1600\n330\n2400\n369\n1416\n232\n3000\n540\n.\n.\n.\n.\n.\n.\nWecanplotthisdata:\nGivendatalikethis,howcanwelearntopredictthepricesofotherhouses\ninPortland,asafunctionofthesizeoftheirlivingareas?\nToestablishnotationforfutureuse,we'lluse\nx\n(\ni\n)\ntodenotethe\\input\"\nvariables(livingareainthisexample),alsocalledinput\nfeatures\n,and\ny\n(\ni\n)\ntodenotethe\\output\"or\ntarget\nvariablethatwearetryingtopredict\n(price).Apair(\nx\n(\ni\n)\n;y\n(\ni\n)\n)iscalleda\ntrainingexample\n,andthedataset\nthatwe'llbeusingtolearn|alistof\nn\ntrainingexamples\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=\n1\n;:::;n\ng\n|iscalleda\ntrainingset\n.Notethatthesuperscript\\(\ni\n)\"inthe\nnotationissimplyanindexintothetrainingset,andhasnothingtodowith\nexponentiation.Wewillalsouse\nX\ndenotethespaceofinputvalues,and\nY\nthespaceofoutputvalues.Inthisexample,\nX\n=\nY\n=\nR\n.\nTodescribethesupervisedlearningproblemslightlymoreformally,our\ngoalis,givenatrainingset,tolearnafunction\nh\n:\nX7!Y\nsothat\nh\n(\nx\n)isa\n\\good\"predictorforthecorrespondingvalueof\ny\n.Forhistoricalreasons,this\n"}, {"page_number": 7, "text": "7\nfunction\nh\niscalleda\nhypothesis\n.Seenpictorially,theprocessistherefore\nlikethis:\nWhenthetargetvariablethatwe'retryingtopredictiscontinuous,such\nasinourhousingexample,wecallthelearningproblema\nregression\nprob-\nlem.When\ny\ncantakeononlyasmallnumberofdiscretevalues(suchas\nif,giventhelivingarea,wewantedtopredictifadwellingisahouseoran\napartment,say),wecallita\n\nproblem.\n"}, {"page_number": 8, "text": "Chapter1\nLinearregression\nTomakeourhousingexamplemoreinteresting,let'sconsideraslightlyricher\ndatasetinwhichwealsoknowthenumberofbedroomsineachhouse:\nLivingarea(feet\n2\n)\n#bedrooms\nPrice(1000\n$\ns)\n2104\n3\n400\n1600\n3\n330\n2400\n3\n369\n1416\n2\n232\n3000\n4\n540\n.\n.\n.\n.\n.\n.\n.\n.\n.\nHere,the\nx\n'saretwo-dimensionalvectorsin\nR\n2\n.Forinstance,\nx\n(\ni\n)\n1\nisthe\nlivingareaofthe\ni\n-thhouseinthetrainingset,and\nx\n(\ni\n)\n2\nisitsnumberof\nbedrooms.(Ingeneral,whendesigningalearningproblem,itwillbeupto\nyoutodecidewhatfeaturestochoose,soifyouareoutinPortlandgathering\nhousingdata,youmightalsodecidetoincludeotherfeaturessuchaswhether\neachhousehasathenumberofbathrooms,andsoon.We'llsay\nmoreaboutfeatureselectionlater,butfornowlet'stakethefeaturesas\ngiven.)\nToperformsupervisedlearning,wemustdecidehowwe'regoingtorep-\nresentfunctions/hypotheses\nh\ninacomputer.Asaninitialchoice,let'ssay\nwedecidetoapproximate\ny\nasalinearfunctionof\nx\n:\nh\n\n(\nx\n)=\n\n0\n+\n\n1\nx\n1\n+\n\n2\nx\n2\nHere,the\n\ni\n'sarethe\nparameters\n(alsocalled\nweights\n)parameterizingthe\nspaceoflinearfunctionsmappingfrom\nX\nto\nY\n.Whenthereisnoriskof\n8\n"}, {"page_number": 9, "text": "9\nconfusion,wewilldropthe\n\nsubscriptin\nh\n\n(\nx\n),andwriteitmoresimplyas\nh\n(\nx\n).Tosimplifyournotation,wealsointroducetheconventionofletting\nx\n0\n=1(thisisthe\ninterceptterm\n),sothat\nh\n(\nx\n)=\nd\nX\ni\n=0\n\ni\nx\ni\n=\n\nT\nx;\nwhereontheright-handsideaboveweareviewing\n\nand\nx\nbothasvectors,\nandhere\nd\nisthenumberofinputvariables(notcounting\nx\n0\n).\nNow,givenatrainingset,howdowepick,orlearn,theparameters\n\n?\nOnereasonablemethodseemstobetomake\nh\n(\nx\n)closeto\ny\n,atleastfor\nthetrainingexampleswehave.Toformalizethis,wewillafunction\nthatmeasures,foreachvalueofthe\n\n's,howclosethe\nh\n(\nx\n(\ni\n)\n)'saretothe\ncorresponding\ny\n(\ni\n)\n's.Wethe\ncostfunction\n:\nJ\n(\n\n)=\n1\n2\nn\nX\ni\n=1\n(\nh\n\n(\nx\n(\ni\n)\n)\n\ny\n(\ni\n)\n)\n2\n:\nIfyou'veseenlinearregressionbefore,youmayrecognizethisasthefamiliar\nleast-squarescostfunctionthatgivesrisetothe\nordinaryleastsquares\nregressionmodel.Whetherornotyouhaveseenitpreviously,let'skeep\ngoing,andwe'lleventuallyshowthistobeaspecialcaseofamuchbroader\nfamilyofalgorithms.\n1.1LMSalgorithm\nWewanttochoose\n\nsoastominimize\nJ\n(\n\n).Todoso,let'suseasearch\nalgorithmthatstartswithsome\\initialguess\"for\n\n,andthatrepeatedly\nchanges\n\ntomake\nJ\n(\n\n)smaller,untilhopefullyweconvergetoavalueof\n\nthatminimizes\nJ\n(\n\n).Spally,let'sconsiderthe\ngradientdescent\nalgorithm,whichstartswithsomeinitial\n\n,andrepeatedlyperformsthe\nupdate:\n\nj\n:=\n\nj\n\n\n@\n@\nj\nJ\n(\n\n)\n:\n(Thisupdateissimultaneouslyperformedforallvaluesof\nj\n=0\n;:::;d\n.)\nHere,\n\niscalledthe\nlearningrate\n.Thisisaverynaturalalgorithmthat\nrepeatedlytakesastepinthedirectionofsteepestdecreaseof\nJ\n.\nInordertoimplementthisalgorithm,wehavetoworkoutwhatisthe\npartialderivativetermontherighthandside.Let'sworkitoutforthe\n"}, {"page_number": 10, "text": "10\ncaseofifwehaveonlyonetrainingexample(\nx;y\n),sothatwecanneglect\nthesumintheof\nJ\n.Wehave:\n@\n@\nj\nJ\n(\n\n)=\n@\n@\nj\n1\n2\n(\nh\n\n(\nx\n)\n\ny\n)\n2\n=2\n\n1\n2\n(\nh\n\n(\nx\n)\n\ny\n)\n\n@\n@\nj\n(\nh\n\n(\nx\n)\n\ny\n)\n=(\nh\n\n(\nx\n)\n\ny\n)\n\n@\n@\nj\n \nd\nX\ni\n=0\n\ni\nx\ni\n\ny\n!\n=(\nh\n\n(\nx\n)\n\ny\n)\nx\nj\nForasingletrainingexample,thisgivestheupdaterule:\n1\n\nj\n:=\n\nj\n+\n\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nj\n:\nTheruleiscalledthe\nLMS\nupdaterule(LMSstandsfor\\leastmeansquares\"),\nandisalsoknownasthe\nWidro\nlearningrule.Thisrulehasseveral\npropertiesthatseemnaturalandintuitive.Forinstance,themagnitudeof\ntheupdateisproportionaltothe\nerror\nterm(\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n));thus,forin-\nstance,ifweareencounteringatrainingexampleonwhichourprediction\nnearlymatchestheactualvalueof\ny\n(\ni\n)\n,thenwethatthereislittleneed\ntochangetheparameters;incontrast,alargerchangetotheparameterswill\nbemadeifourprediction\nh\n\n(\nx\n(\ni\n)\n)hasalargeerror(i.e.,ifitisveryfarfrom\ny\n(\ni\n)\n).\nWe'dderivedtheLMSruleforwhentherewasonlyasingletraining\nexample.Therearetwowaystomodifythismethodforatrainingsetof\nmorethanoneexample.Theisreplaceitwiththefollowingalgorithm:\nRepeatuntilconvergence\nf\n\nj\n:=\n\nj\n+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nj\n;\n(forevery\nj\n)(1.1)\ng\n1\nWeusethenotation\\\na\n:=\nb\n\"todenoteanoperation(inacomputerprogram)in\nwhichwe\nset\nthevalueofavariable\na\ntobeequaltothevalueof\nb\n.Inotherwords,this\noperationoverwrites\na\nwiththevalueof\nb\n.Incontrast,wewillwrite\\\na\n=\nb\n\"whenweare\nassertingastatementoffact,thatthevalueof\na\nisequaltothevalueof\nb\n.\n"}, {"page_number": 11, "text": "11\nBygroupingtheupdatesofthecoordinatesintoanupdateofthevector\n\n,wecanrewriteupdate(1.1)inaslightlymoresuccinctway:\n\n:=\n\n+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nThereadercaneasilyverifythatthequantityinthesummationinthe\nupdateruleaboveisjust\n@J\n(\n\n)\n=@\nj\n(fortheoriginalionof\nJ\n).So,this\nissimplygradientdescentontheoriginalcostfunction\nJ\n.Thismethodlooks\nateveryexampleintheentiretrainingsetoneverystep,andiscalled\nbatch\ngradientdescent\n.Notethat,whilegradientdescentcanbesusceptible\ntolocalminimaingeneral,theoptimizationproblemwehaveposedhere\nforlinearregressionhasonlyoneglobal,andnootherlocal,optima;thus\ngradientdescentalwaysconverges(assumingthelearningrate\n\nisnottoo\nlarge)totheglobalminimum.Indeed,\nJ\nisaconvexquadraticfunction.\nHereisanexampleofgradientdescentasitisruntominimizeaquadratic\nfunction.\nTheellipsesshownabovearethecontoursofaquadraticfunction.Also\nshownisthetrajectorytakenbygradientdescent,whichwasinitializedat\n(48,30).The\nx\n'sinthe(joinedbystraightlines)markthesuccessive\nvaluesof\n\nthatgradientdescentwentthrough.\nWhenwerunbatchgradientdescentto\n\nonourpreviousdataset,\ntolearntopredicthousingpriceasafunctionoflivingarea,weobtain\n\n0\n=71\n:\n27,\n\n1\n=0\n:\n1345.Ifweplot\nh\n\n(\nx\n)asafunctionof\nx\n(area),along\nwiththetrainingdata,weobtainthefollowing\n"}, {"page_number": 12, "text": "12\nIfthenumberofbedroomswereincludedasoneoftheinputfeaturesaswell,\nweget\n\n0\n=89\n:\n60\n;\n1\n=0\n:\n1392,\n\n2\n=\n\n8\n:\n738.\nTheaboveresultswereobtainedwithbatchgradientdescent.Thereis\nanalternativetobatchgradientdescentthatalsoworksverywell.Consider\nthefollowingalgorithm:\nLoop\nf\nfor\ni\n=1to\nn\n,\nf\n\nj\n:=\n\nj\n+\n\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nj\n;\n(forevery\nj\n)(1.2)\ng\ng\nBygroupingtheupdatesofthecoordinatesintoanupdateofthevector\n\n,wecanrewriteupdate(1.2)inaslightlymoresuccinctway:\n\n:=\n\n+\n\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nInthisalgorithm,werepeatedlyrunthroughthetrainingset,andeach\ntimeweencounteratrainingexample,weupdatetheparametersaccording\ntothegradientoftheerrorwithrespecttothatsingletrainingexampleonly.\nThisalgorithmiscalled\nstochasticgradientdescent\n(also\nincremental\ngradientdescent\n).Whereasbatchgradientdescenthastoscanthrough\ntheentiretrainingsetbeforetakingasinglestep|acostlyoperationif\nn\nis\nlarge|stochasticgradientdescentcanstartmakingprogressrightaway,and\n"}, {"page_number": 13, "text": "13\ncontinuestomakeprogresswitheachexampleitlooksat.Often,stochastic\ngradientdescentgets\n\n\\close\"totheminimummuchfasterthanbatchgra-\ndientdescent.(Notehoweverthatitmaynever\\converge\"totheminimum,\nandtheparameters\n\nwillkeeposcillatingaroundtheminimumof\nJ\n(\n\n);but\ninpracticemostofthevaluesneartheminimumwillbereasonablygood\napproximationstothetrueminimum.\n2\n)Forthesereasons,particularlywhen\nthetrainingsetislarge,stochasticgradientdescentisoftenpreferredover\nbatchgradientdescent.\n1.2Thenormalequations\nGradientdescentgivesonewayofminimizing\nJ\n.Let'sdiscussasecondway\nofdoingso,thistimeperformingtheminimizationexplicitlyandwithout\nresortingtoaniterativealgorithm.Inthismethod,wewillminimize\nJ\nby\nexplicitlytakingitsderivativeswithrespecttothe\n\nj\n's,andsettingthemto\nzero.Toenableustodothiswithouthavingtowritereamsofalgebraand\npagesfullofmatricesofderivatives,let'sintroducesomenotationfordoing\ncalculuswithmatrices.\n1.2.1Matrixderivatives\nForafunction\nf\n:\nR\nn\n\nd\n7!\nR\nmappingfrom\nn\n-by-\nd\nmatricestothereal\nnumbers,wethederivativeof\nf\nwithrespectto\nA\ntobe:\nr\nA\nf\n(\nA\n)=\n2\n6\n4\n@f\n@A\n11\n\n@f\n@A\n1\nd\n.\n.\n.\n.\n.\n.\n.\n.\n.\n@f\n@A\nn\n1\n\n@f\n@A\nnd\n3\n7\n5\nThus,thegradient\nr\nA\nf\n(\nA\n)isitselfan\nn\n-by-\nd\nmatrix,whose(\ni;j\n)-elementis\n@f=@A\nij\n.Forexample,suppose\nA\n=\n\nA\n11\nA\n12\nA\n21\nA\n22\n\nisa2-by-2matrix,and\nthefunction\nf\n:\nR\n2\n\n2\n7!\nR\nisgivenby\nf\n(\nA\n)=\n3\n2\nA\n11\n+5\nA\n2\n12\n+\nA\n21\nA\n22\n:\n2\nByslowlylettingthelearningrate\n\ndecreasetozeroasthealgorithmruns,itisalso\npossibletoensurethattheparameterswillconvergetotheglobalminimumratherthan\nmerelyoscillatearoundtheminimum.\n"}, {"page_number": 14, "text": "14\nHere,\nA\nij\ndenotesthe(\ni;j\n)entryofthematrix\nA\n.Wethenhave\nr\nA\nf\n(\nA\n)=\n\n3\n2\n10\nA\n12\nA\n22\nA\n21\n\n:\n1.2.2Leastsquaresrevisited\nArmedwiththetoolsofmatrixderivatives,letusnowproceedtoin\nclosed-formthevalueof\n\nthatminimizes\nJ\n(\n\n).Webeginbyre-writing\nJ\nin\nmatrix-vectorialnotation.\nGivenatrainingset,the\ndesignmatrix\nX\ntobethe\nn\n-by-\nd\nmatrix\n(actually\nn\n-by-\nd\n+1,ifweincludetheinterceptterm)thatcontainsthe\ntrainingexamples'inputvaluesinitsrows:\nX\n=\n2\n6\n6\n6\n4\n|(\nx\n(1)\n)\nT\n|\n|(\nx\n(2)\n)\nT\n|\n.\n.\n.\n|(\nx\n(\nn\n)\n)\nT\n|\n3\n7\n7\n7\n5\n:\nAlso,let\n~y\nbethe\nn\n-dimensionalvectorcontainingallthetargetvaluesfrom\nthetrainingset:\n~y\n=\n2\n6\n6\n6\n4\ny\n(1)\ny\n(2)\n.\n.\n.\ny\n(\nn\n)\n3\n7\n7\n7\n5\n:\nNow,since\nh\n\n(\nx\n(\ni\n)\n)=(\nx\n(\ni\n)\n)\nT\n\n,wecaneasilyverifythat\nX\n\n~y\n=\n2\n6\n4\n(\nx\n(1)\n)\nT\n\n.\n.\n.\n(\nx\n(\nn\n)\n)\nT\n\n3\n7\n5\n\n2\n6\n4\ny\n(1)\n.\n.\n.\ny\n(\nn\n)\n3\n7\n5\n=\n2\n6\n4\nh\n\n(\nx\n(1)\n)\n\ny\n(1)\n.\n.\n.\nh\n\n(\nx\n(\nn\n)\n)\n\ny\n(\nn\n)\n3\n7\n5\n:\nThus,usingthefactthatforavector\nz\n,wehavethat\nz\nT\nz\n=\nP\ni\nz\n2\ni\n:\n1\n2\n(\nX\n\n~y\n)\nT\n(\nX\n\n~y\n)=\n1\n2\nn\nX\ni\n=1\n(\nh\n\n(\nx\n(\ni\n)\n)\n\ny\n(\ni\n)\n)\n2\n=\nJ\n(\n\n)\n"}, {"page_number": 15, "text": "15\nFinally,tominimize\nJ\n,let'sitsderivativeswithrespectto\n\n.Hence,\nr\n\nJ\n(\n\n)=\nr\n\n1\n2\n(\nX\n\n~y\n)\nT\n(\nX\n\n~y\n)\n=\n1\n2\nr\n\n\n(\nX\n)\nT\nX\n\n(\nX\n)\nT\n~y\n\n~y\nT\n(\nX\n)+\n~y\nT\n~y\n\n=\n1\n2\nr\n\n\n\nT\n(\nX\nT\nX\n)\n\n\n~y\nT\n(\nX\n)\n\n~y\nT\n(\nX\n)\n\n=\n1\n2\nr\n\n\n\nT\n(\nX\nT\nX\n)\n\n\n2(\nX\nT\n~y\n)\nT\n\n\n=\n1\n2\n\n2\nX\nT\nX\n\n2\nX\nT\n~y\n\n=\nX\nT\nX\n\nX\nT\n~y\nInthethirdstep,weusedthefactthat\na\nT\nb\n=\nb\nT\na\n,andinthestep\nusedthefacts\nr\nx\nb\nT\nx\n=\nb\nand\nr\nx\nx\nT\nAx\n=2\nAx\nforsymmetricmatrix\nA\n(for\nmoredetails,seeSection4.3of\\LinearAlgebraReviewandReference\").To\nminimize\nJ\n,wesetitsderivativestozero,andobtainthe\nnormalequations\n:\nX\nT\nX\n=\nX\nT\n~y\nThus,thevalueof\n\nthatminimizes\nJ\n(\n\n)isgiveninclosedformbythe\nequation\n\n=(\nX\nT\nX\n)\n\n1\nX\nT\n~y:\n3\n1.3Probabilisticinterpretation\nWhenfacedwitharegressionproblem,whymightlinearregression,and\nspwhymighttheleast-squarescostfunction\nJ\n,beareasonable\nchoice?Inthissection,wewillgiveasetofprobabilisticassumptions,under\nwhichleast-squaresregressionisderivedasaverynaturalalgorithm.\nLetusassumethatthetargetvariablesandtheinputsarerelatedviathe\nequation\ny\n(\ni\n)\n=\n\nT\nx\n(\ni\n)\n+\n\n(\ni\n)\n;\n3\nNotethatintheabovestep,weareimplicitlyassumingthat\nX\nT\nX\nisaninvertible\nmatrix.Thiscanbecheckedbeforecalculatingtheinverse.Ifeitherthenumberof\nlinearlyindependentexamplesisfewerthanthenumberoffeatures,orifthefeatures\narenotlinearlyindependent,then\nX\nT\nX\nwillnotbeinvertible.Eveninsuchcases,itis\npossibletothesituationwithadditionaltechniques,whichweskiphereforthesake\nofsimplicty.\n"}, {"page_number": 16, "text": "16\nwhere\n\n(\ni\n)\nisanerrortermthatcaptureseitherunmodeled(suchas\niftherearesomefeaturesverypertinenttopredictinghousingprice,but\nthatwe'dleftoutoftheregression),orrandomnoise.Letusfurtherassume\nthatthe\n\n(\ni\n)\naredistributedIID(independentlyandidenticallydistributed)\naccordingtoaGaussiandistribution(alsocalledaNormaldistribution)with\nmeanzeroandsomevariance\n\u02d9\n2\n.Wecanwritethisassumptionas\\\n\n(\ni\n)\n\u02d8\nN\n(0\n;\u02d9\n2\n).\"I.e.,thedensityof\n\n(\ni\n)\nisgivenby\np\n(\n\n(\ni\n)\n)=\n1\np\n2\n\u02c7\u02d9\nexp\n\n\n(\n\n(\ni\n)\n)\n2\n2\n\u02d9\n2\n\n:\nThisimpliesthat\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)=\n1\np\n2\n\u02c7\u02d9\nexp\n\n\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n2\n\u02d9\n2\n\n:\nThenotation\\\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\"indicatesthatthisisthedistributionof\ny\n(\ni\n)\ngiven\nx\n(\ni\n)\nandparameterizedby\n\n.Notethatweshouldnotconditionon\n\n(\\\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n)\"),since\n\nisnotarandomvariable.Wecanalsowritethe\ndistributionof\ny\n(\ni\n)\nas\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n\u02d8N\n(\n\nT\nx\n(\ni\n)\n;\u02d9\n2\n).\nGiven\nX\n(thedesignmatrix,whichcontainsallthe\nx\n(\ni\n)\n's)and\n\n,what\nisthedistributionofthe\ny\n(\ni\n)\n's?Theprobabilityofthedataisgivenby\np\n(\n~y\nj\nX\n;\n\n).Thisquantityistypicallyviewedafunctionof\n~y\n(andperhaps\nX\n),\nforaxedvalueof\n\n.Whenwewishtoexplicitlyviewthisasafunctionof\n\n,wewillinsteadcallitthe\nlikelihood\nfunction:\nL\n(\n\n)=\nL\n(\n\n;\nX;~y\n)=\np\n(\n~y\nj\nX\n;\n\n)\n:\nNotethatbytheindependenceassumptiononthe\n\n(\ni\n)\n's(andhencealsothe\ny\n(\ni\n)\n'sgiventhe\nx\n(\ni\n)\n's),thiscanalsobewritten\nL\n(\n\n)=\nn\nY\ni\n=1\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\n=\nn\nY\ni\n=1\n1\np\n2\n\u02c7\u02d9\nexp\n\n\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n2\n\u02d9\n2\n\n:\nNow,giventhisprobabilisticmodelrelatingthe\ny\n(\ni\n)\n'sandthe\nx\n(\ni\n)\n's,what\nisareasonablewayofchoosingourbestguessoftheparameters\n\n?The\nprincipalof\nmaximumlikelihood\nsaysthatweshouldchoose\n\nsoasto\nmakethedataashighprobabilityaspossible.I.e.,weshouldchoose\n\nto\nmaximize\nL\n(\n\n).\n"}, {"page_number": 17, "text": "17\nInsteadofmaximizing\nL\n(\n\n),wecanalsomaximizeanystrictlyincreasing\nfunctionof\nL\n(\n\n).Inparticular,thederivationswillbeabitsimplerifwe\ninsteadmaximizethe\nloglikelihood\n`\n(\n\n):\n`\n(\n\n)=log\nL\n(\n\n)\n=log\nn\nY\ni\n=1\n1\np\n2\n\u02c7\u02d9\nexp\n\n\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n2\n\u02d9\n2\n\n=\nn\nX\ni\n=1\nlog\n1\np\n2\n\u02c7\u02d9\nexp\n\n\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n2\n\u02d9\n2\n\n=\nn\nlog\n1\np\n2\n\u02c7\u02d9\n\n1\n\u02d9\n2\n\n1\n2\nn\nX\ni\n=1\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n:\nHence,maximizing\n`\n(\n\n)givesthesameanswerasminimizing\n1\n2\nn\nX\ni\n=1\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n;\nwhichwerecognizetobe\nJ\n(\n\n),ouroriginalleast-squarescostfunction.\nTosummarize:Underthepreviousprobabilisticassumptionsonthedata,\nleast-squaresregressioncorrespondstothemaximumlikelihoodesti-\nmateof\n\n.Thisisthusonesetofassumptionsunderwhichleast-squaresre-\ngressioncanbeasaverynaturalmethodthat'sjustdoingmaximum\nlikelihoodestimation.(Notehoweverthattheprobabilisticassumptionsare\nbynomeans\nnecessary\nforleast-squarestobeaperfectlygoodandrational\nprocedure,andtheremay|andindeedthereare|othernaturalassumptions\nthatcanalsobeusedtojustifyit.)\nNotealsothat,inourpreviousdiscussion,ourchoiceof\n\ndidnot\ndependonwhatwas\n\u02d9\n2\n,andindeedwe'dhavearrivedatthesameresult\nevenif\n\u02d9\n2\nwereunknown.Wewillusethisfactagainlater,whenwetalk\nabouttheexponentialfamilyandgeneralizedlinearmodels.\n1.4Locallyweightedlinearregression(optional\nreading)\nConsidertheproblemofpredicting\ny\nfrom\nx\n2\nR\n.Theleftmostbelow\nshowstheresultofa\ny\n=\n\n0\n+\n\n1\nx\ntoadataset.Weseethatthedata\ndoesn'treallylieonstraightline,andsotheisnotverygood.\n"}, {"page_number": 18, "text": "18\nInstead,ifwehadaddedanextrafeature\nx\n2\n,and\ny\n=\n\n0\n+\n\n1\nx\n+\n\n2\nx\n2\n,\nthenweobtainaslightlybettertothedata.(SeemiddleNaively,it\nmightseemthatthemorefeaturesweadd,thebetter.However,thereisalso\nadangerinaddingtoomanyfeatures:Therightmostistheresultof\na5-thorderpolynomial\ny\n=\nP\n5\nj\n=0\n\nj\nx\nj\n.Weseethateventhoughthe\ncurvepassesthroughthedataperfectly,wewouldnotexpectthisto\nbeaverygoodpredictorof,say,housingprices(\ny\n)fortlivingareas\n(\nx\n).Withoutformallywhatthesetermsmean,we'llsaythe\nontheleftshowsaninstanceof\n\n|inwhichthedataclearly\nshowsstructurenotcapturedbythemodel|andtheontherightis\nanexampleof\nov\n.(Laterinthisclass,whenwetalkaboutlearning\ntheorywe'llformalizesomeofthesenotions,andalsomorecarefully\njustwhatitmeansforahypothesistobegoodorbad.)\nAsdiscussedpreviously,andasshownintheexampleabove,thechoiceof\nfeaturesisimportanttoensuringgoodperformanceofalearningalgorithm.\n(Whenwetalkaboutmodelselection,we'llalsoseealgorithmsforautomat-\nicallychoosingagoodsetoffeatures.)Inthissection,letustalk\naboutthelocallyweightedlinearregression(LWR)algorithmwhich,assum-\ningthereisttrainingdata,makesthechoiceoffeatureslesscritical.\nThistreatmentwillbebrief,sinceyou'llgetachancetoexploresomeofthe\npropertiesoftheLWRalgorithmyourselfinthehomework.\nIntheoriginallinearregressionalgorithm,tomakeapredictionataquery\npoint\nx\n(i.e.,toevaluate\nh\n(\nx\n)),wewould:\n1.\nFit\n\ntominimize\nP\ni\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n.\n2.\nOutput\n\nT\nx\n.\nIncontrast,thelocallyweightedlinearregressionalgorithmdoesthefol-\nlowing:\n1.\nFit\n\ntominimize\nP\ni\nw\n(\ni\n)\n(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\n.\n2.\nOutput\n\nT\nx\n.\n"}, {"page_number": 19, "text": "19\nHere,the\nw\n(\ni\n)\n'sarenon-negativevalued\nweights\n.Intuitively,if\nw\n(\ni\n)\nislarge\nforaparticularvalueof\ni\n,theninpicking\n\n,we'lltryhardtomake(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\nsmall.If\nw\n(\ni\n)\nissmall,thenthe(\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n)\n2\nerrortermwillbe\nprettymuchignoredinthe\nAfairlystandardchoicefortheweightsis\n4\nw\n(\ni\n)\n=exp\n\n\n(\nx\n(\ni\n)\n\nx\n)\n2\n2\n\u02dd\n2\n\nNotethattheweightsdependontheparticularpoint\nx\natwhichwe'retrying\ntoevaluate\nx\n.Moreover,if\nj\nx\n(\ni\n)\n\nx\nj\nissmall,then\nw\n(\ni\n)\niscloseto1;and\nif\nj\nx\n(\ni\n)\n\nx\nj\nislarge,then\nw\n(\ni\n)\nissmall.Hence,\n\nischosengivingamuch\nhigher\\weight\"tothe(errorson)trainingexamplesclosetothequerypoint\nx\n.(Notealsothatwhiletheformulafortheweightstakesaformthatis\ncosmeticallysimilartothedensityofaGaussiandistribution,the\nw\n(\ni\n)\n'sdo\nnotdirectlyhaveanythingtodowithGaussians,andinparticularthe\nw\n(\ni\n)\narenotrandomvariables,normallydistributedorotherwise.)Theparameter\n\u02dd\ncontrolshowquicklytheweightofatrainingexamplefallswithdistance\nofits\nx\n(\ni\n)\nfromthequerypoint\nx\n;\n\u02dd\niscalledthe\nbandwidth\nparameter,and\nisalsosomethingthatyou'llgettoexperimentwithinyourhomework.\nLocallyweightedlinearregressionistheexamplewe'reseeingofa\nnon-parametric\nalgorithm.The(unweighted)linearregressionalgorithm\nthatwesawearlierisknownasa\nparametric\nlearningalgorithm,because\nithasanumberofparameters(the\n\ni\n's),whicharetothe\ndata.Oncewe'vethe\n\ni\n'sandstoredthemaway,wenolongerneedto\nkeepthetrainingdataaroundtomakefuturepredictions.Incontrast,to\nmakepredictionsusinglocallyweightedlinearregression,weneedtokeep\ntheentiretrainingsetaround.Theterm\\non-parametric\"(roughly)refers\ntothefactthattheamountofweneedtokeepinordertorepresentthe\nhypothesis\nh\ngrowslinearlywiththesizeofthetrainingset.\n4\nIf\nx\nisvector-valued,thisisgeneralizedtobe\nw\n(\ni\n)\n=exp(\n\n(\nx\n(\ni\n)\n\nx\n)\nT\n(\nx\n(\ni\n)\n\nx\n)\n=\n(2\n\u02dd\n2\n)),\nor\nw\n(\ni\n)\n=exp(\n\n(\nx\n(\ni\n)\n\nx\n)\nT\n\n\n1\n(\nx\n(\ni\n)\n\nx\n)\n=\n(2\n\u02dd\n2\n)),foranappropriatechoiceof\n\u02dd\nor\n"}, {"page_number": 20, "text": "Chapter2\nandlogistic\nregression\nLet'snowtalkabouttheproblem.Thisisjustliketheregression\nproblem,exceptthatthevalues\ny\nwenowwanttopredicttakeononly\nasmallnumberofdiscretevalues.Fornow,wewillfocusonthe\nbinary\n\nprobleminwhich\ny\ncantakeononlytwovalues,0and1.\n(Mostofwhatwesayherewillalsogeneralizetothemultiple-classcase.)\nForinstance,ifwearetryingtobuildaspamforemail,then\nx\n(\ni\n)\nmaybesomefeaturesofapieceofemail,and\ny\nmaybe1ifitisapiece\nofspammail,and0otherwise.0isalsocalledthe\nnegativeclass\n,and1\nthe\npositiveclass\n,andtheyaresometimesalsodenotedbythesymbols\\-\"\nand\\+.\"Given\nx\n(\ni\n)\n,thecorresponding\ny\n(\ni\n)\nisalsocalledthe\nlabel\nforthe\ntrainingexample.\n2.1Logisticregression\nWecouldapproachtheproblemignoringthefactthat\ny\nis\ndiscrete-valued,anduseouroldlinearregressionalgorithmtotrytopredict\ny\ngiven\nx\n.However,itiseasytoconstructexampleswherethismethod\nperformsverypoorly.Intuitively,italsodoesn'tmakesensefor\nh\n\n(\nx\n)totake\nvalueslargerthan1orsmallerthan0whenweknowthat\ny\n2f\n0\n;\n1\ng\n.\nTothis,let'schangetheformforourhypotheses\nh\n\n(\nx\n).Wewillchoose\nh\n\n(\nx\n)=\ng\n(\n\nT\nx\n)=\n1\n1+\ne\n\n\nT\nx\n;\nwhere\ng\n(\nz\n)=\n1\n1+\ne\n\nz\n20\n"}, {"page_number": 21, "text": "21\niscalledthe\nlogisticfunction\northe\nsigmoidfunction\n.Hereisaplot\nshowing\ng\n(\nz\n):\nNoticethat\ng\n(\nz\n)tendstowards1as\nz\n!1\n,and\ng\n(\nz\n)tendstowards0as\nz\n!\n.Moreover,g(z),andhencealso\nh\n(\nx\n),isalwaysboundedbetween\n0and1.Asbefore,wearekeepingtheconventionofletting\nx\n0\n=1,sothat\n\nT\nx\n=\n\n0\n+\nP\nd\nj\n=1\n\nj\nx\nj\n.\nFornow,let'stakethechoiceof\ng\nasgiven.Otherfunctionsthatsmoothly\nincreasefrom0to1canalsobeused,butforacoupleofreasonsthatwe'llsee\nlater(whenwetalkaboutGLMs,andwhenwetalkaboutgenerativelearning\nalgorithms),thechoiceofthelogisticfunctionisafairlynaturalone.Before\nmovingon,here'sausefulpropertyofthederivativeofthesigmoidfunction,\nwhichwewriteas\ng\n0\n:\ng\n0\n(\nz\n)=\nd\ndz\n1\n1+\ne\n\nz\n=\n1\n(1+\ne\n\nz\n)\n2\n\ne\n\nz\n\n=\n1\n(1+\ne\n\nz\n)\n\n\n1\n\n1\n(1+\ne\n\nz\n)\n\n=\ng\n(\nz\n)(1\n\ng\n(\nz\n))\n:\nSo,giventhelogisticregressionmodel,howdowe\n\nforit?Following\nhowwesawleastsquaresregressioncouldbederivedasthemaximumlike-\nlihoodestimatorunderasetofassumptions,let'sendowour\nmodelwithasetofprobabilisticassumptions,andthentheparameters\nviamaximumlikelihood.\n"}, {"page_number": 22, "text": "22\nLetusassumethat\nP\n(\ny\n=1\nj\nx\n;\n\n)=\nh\n\n(\nx\n)\nP\n(\ny\n=0\nj\nx\n;\n\n)=1\n\nh\n\n(\nx\n)\nNotethatthiscanbewrittenmorecompactlyas\np\n(\ny\nj\nx\n;\n\n)=(\nh\n\n(\nx\n))\ny\n(1\n\nh\n\n(\nx\n))\n1\n\ny\nAssumingthatthe\nn\ntrainingexamplesweregeneratedindependently,we\ncanthenwritedownthelikelihoodoftheparametersas\nL\n(\n\n)=\np\n(\n~y\nj\nX\n;\n\n)\n=\nn\nY\ni\n=1\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\n=\nn\nY\ni\n=1\n\nh\n\n(\nx\n(\ni\n)\n)\n\ny\n(\ni\n)\n\n1\n\nh\n\n(\nx\n(\ni\n)\n)\n\n1\n\ny\n(\ni\n)\nAsbefore,itwillbeeasiertomaximizetheloglikelihood:\n`\n(\n\n)=log\nL\n(\n\n)\n=\nn\nX\ni\n=1\ny\n(\ni\n)\nlog\nh\n(\nx\n(\ni\n)\n)+(1\n\ny\n(\ni\n)\n)log(1\n\nh\n(\nx\n(\ni\n)\n))\nHowdowemaximizethelikelihood?Similartoourderivationinthecase\noflinearregression,wecanusegradientascent.Writteninvectorialnotation,\nourupdateswillthereforebegivenby\n\n:=\n\n+\n\nr\n\n`\n(\n\n).(Notethepositive\nratherthannegativesignintheupdateformula,sincewe'remaximizing,\nratherthanminimizing,afunctionnow.)Let'sstartbyworkingwithjust\nonetrainingexample(\nx;y\n),andtakederivativestoderivethestochastic\ngradientascentrule:\n@\n@\nj\n`\n(\n\n)=\n\ny\n1\ng\n(\n\nT\nx\n)\n\n(1\n\ny\n)\n1\n1\n\ng\n(\n\nT\nx\n)\n\n@\n@\nj\ng\n(\n\nT\nx\n)\n=\n\ny\n1\ng\n(\n\nT\nx\n)\n\n(1\n\ny\n)\n1\n1\n\ng\n(\n\nT\nx\n)\n\ng\n(\n\nT\nx\n)(1\n\ng\n(\n\nT\nx\n))\n@\n@\nj\n\nT\nx\n=\n\ny\n(1\n\ng\n(\n\nT\nx\n))\n\n(1\n\ny\n)\ng\n(\n\nT\nx\n)\n\nx\nj\n=(\ny\n\nh\n\n(\nx\n))\nx\nj\n"}, {"page_number": 23, "text": "23\nAbove,weusedthefactthat\ng\n0\n(\nz\n)=\ng\n(\nz\n)(1\n\ng\n(\nz\n)).Thisthereforegivesus\nthestochasticgradientascentrule\n\nj\n:=\n\nj\n+\n\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nj\nIfwecomparethistotheLMSupdaterule,weseethatitlooksidentical;but\nthisis\nnot\nthesamealgorithm,because\nh\n\n(\nx\n(\ni\n)\n)isnowasanon-linear\nfunctionof\n\nT\nx\n(\ni\n)\n.Nonetheless,it'salittlesurprisingthatweendupwith\nthesameupdateruleforarathertalgorithmandlearningproblem.\nIsthiscoincidence,oristhereadeeperreasonbehindthis?We'llanswerthis\nwhenwegettoGLMmodels.\n2.2Digression:theperceptronlearningalgo-\nrithn\nWenowdigresstotalkaboutanalgorithmthat'sofsomehistorical\ninterest,andthatwewillalsoreturntolaterwhenwetalkaboutlearning\ntheory.Considermodifyingthelogisticregressionmethodto\\force\"itto\noutputvaluesthatareeither0or1orexactly.Todoso,itseemsnaturalto\nchangetheof\ng\ntobethethresholdfunction:\ng\n(\nz\n)=\n\u02c6\n1if\nz\n\n0\n0if\nz<\n0\nIfwethenlet\nh\n\n(\nx\n)=\ng\n(\n\nT\nx\n)asbeforebutusingthismoionof\ng\n,andifweusetheupdaterule\n\nj\n:=\n\nj\n+\n\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\nj\n:\nthenwehavethe\nperceptronlearningalgorithn\n.\nInthe1960s,this\\perceptron\"wasarguedtobearoughmodelforhow\nindividualneuronsinthebrainwork.Givenhowsimplethealgorithmis,it\nwillalsoprovideastartingpointforouranalysiswhenwetalkaboutlearning\ntheorylaterinthisclass.Notehoweverthateventhoughtheperceptronmay\nbecosmeticallysimilartotheotheralgorithmswetalkedabout,itisactually\naveryttypeofalgorithmthanlogisticregressionandleastsquares\nlinearregression;inparticular,itistoendowtheperceptron'spredic-\ntionswithmeaningfulprobabilisticinterpretations,orderivetheperceptron\nasamaximumlikelihoodestimationalgorithm.\n"}, {"page_number": 24, "text": "24\n2.3Anotheralgorithmformaximizing\n`\n(\n\n)\nReturningtologisticregressionwith\ng\n(\nz\n)beingthesigmoidfunction,let's\nnowtalkaboutatalgorithmformaximizing\n`\n(\n\n).\nTogetusstarted,let'sconsiderNewton'smethodforazeroofa\nfunction.Sp,supposewehavesomefunction\nf\n:\nR\n7!\nR\n,andwe\nwishtoavalueof\n\nsothat\nf\n(\n\n)=0.Here,\n\n2\nR\nisarealnumber.\nNewton'smethodperformsthefollowingupdate:\n\n:=\n\n\nf\n(\n\n)\nf\n0\n(\n\n)\n:\nThismethodhasanaturalinterpretationinwhichwecanthinkofitas\napproximatingthefunction\nf\nviaalinearfunctionthatistangentto\nf\nat\nthecurrentguess\n\n,solvingforwherethatlinearfunctionequalstozero,and\nlettingthenextguessfor\n\nbewherethatlinearfunctioniszero.\nHere'sapictureoftheNewton'smethodinaction:\nIntheleftmostweseethefunction\nf\nplottedalongwiththeline\ny\n=0.We'retryingto\n\nsothat\nf\n(\n\n)=0;thevalueof\n\nthatachievesthis\nisabout1.3.Supposeweinitializedthealgorithmwith\n\n=4\n:\n5.Newton's\nmethodthenastraightlinetangentto\nf\nat\n\n=4\n:\n5,andsolvesforthe\nwherethatlineevaluatesto0.(MiddleThisgiveusthenextguess\nfor\n\n,whichisabout2.8.Therightmostshowstheresultofrunning\nonemoreiteration,whichtheupdates\n\ntoabout1.8.Afterafewmore\niterations,werapidlyapproach\n\n=1\n:\n3.\nNewton'smethodgivesawayofgettingto\nf\n(\n\n)=0.Whatifwewantto\nuseittomaximizesomefunction\n`\n?Themaximaof\n`\ncorrespondtopoints\nwhereitsderivative\n`\n0\n(\n\n)iszero.So,byletting\nf\n(\n\n)=\n`\n0\n(\n\n),wecanuse\nthesamealgorithmtomaximize\n`\n,andweobtainupdaterule:\n\n:=\n\n\n`\n0\n(\n\n)\n`\n00\n(\n\n)\n:\n(Somethingtothinkabout:Howwouldthischangeifwewantedtouse\nNewton'smethodtominimizeratherthanmaximizeafunction?)\n"}, {"page_number": 25, "text": "25\nLastly,inourlogisticregressionsetting,\n\nisvector-valued,soweneedto\ngeneralizeNewton'smethodtothissetting.ThegeneralizationofNewton's\nmethodtothismultidimensionalsetting(alsocalledtheNewton-Raphson\nmethod)isgivenby\n\n:=\n\n\nH\n\n1\nr\n\n`\n(\n\n)\n:\nHere,\nr\n\n`\n(\n\n)is,asusual,thevectorofpartialderivativesof\n`\n(\n\n)withrespect\ntothe\n\ni\n's;and\nH\nisan\nd\n-by-\nd\nmatrix(actually,\nd\n+1\n\nby\n\nd+1,assumingthat\nweincludetheinterceptterm)calledthe\nHessian\n,whoseentriesaregiven\nby\nH\nij\n=\n@\n2\n`\n(\n\n)\n@\ni\n@\nj\n:\nNewton'smethodtypicallyenjoysfasterconvergencethan(batch)gra-\ndientdescent,andrequiresmanyfeweriterationstogetveryclosetothe\nminimum.OneiterationofNewton'scan,however,bemoreexpensivethan\noneiterationofgradientdescent,sinceitrequiresandinvertingan\nd\n-by-\nd\nHessian;butsolongas\nd\nisnottoolarge,itisusuallymuchfaster\noverall.WhenNewton'smethodisappliedtomaximizethelogisticregres-\nsionloglikelihoodfunction\n`\n(\n\n),theresultingmethodisalsocalled\nFisher\nscoring\n.\n"}, {"page_number": 26, "text": "Chapter3\nGeneralizedlinearmodels\nSofar,we'veseenaregressionexample,andaexample.Inthe\nregressionexample,wehad\ny\nj\nx\n;\n\n\u02d8N\n(\n\u02d9\n2\n),andintheone,\ny\nj\nx\n;\n\n\u02d8\nBernoulli(\n\u02da\n),forsomeappropriateof\n\nand\n\u02da\nasfunctions\nof\nx\nand\n\n.Inthissection,wewillshowthatbothofthesemethodsare\nspecialcasesofabroaderfamilyofmodels,calledGeneralizedLinearModels\n(GLMs).\n1\nWewillalsoshowhowothermodelsintheGLMfamilycanbe\nderivedandappliedtootherandregressionproblems.\n3.1Theexponentialfamily\nToworkourwayuptoGLMs,wewillbeginbyexponentialfamily\ndistributions.Wesaythataclassofdistributionsisintheexponentialfamily\nifitcanbewrittenintheform\np\n(\ny\n;\n\n)=\nb\n(\ny\n)exp(\n\nT\nT\n(\ny\n)\n\na\n(\n\n))(3.1)\nHere,\n\niscalledthe\nnaturalparameter\n(alsocalledthe\ncanonicalparam-\neter\n)ofthedistribution;\nT\n(\ny\n)isthe\ntstatistic\n(forthedistribu-\ntionsweconsider,itwilloftenbethecasethat\nT\n(\ny\n)=\ny\n);and\na\n(\n\n)isthe\nlog\npartitionfunction\n.Thequantity\ne\n\na\n(\n\n)\nessentiallyplaystheroleofanor-\nmalizationconstant,thatmakessurethedistribution\np\n(\ny\n;\n\n)sums/integrates\nover\ny\nto1.\nAchoiceof\nT\n,\na\nand\nb\na\nfamily\n(orset)ofdistributionsthat\nisparameterizedby\n\n;aswevary\n\n,wethengettdistributionswithin\nthisfamily.\n1\nThepresentationofthematerialinthissectiontakesinspirationfromMichaelI.\nJordan,\nLearningingraphicalmodels\n(unpublishedbookdraft),andalsoMcCullaghand\nNelder,\nGeneralizedLinearModels(2nded.)\n.\n26\n"}, {"page_number": 27, "text": "27\nWenowshowthattheBernoulliandtheGaussiandistributionsareex-\namplesofexponentialfamilydistributions.TheBernoullidistributionwith\nmean\n\u02da\n,writtenBernoulli(\n\u02da\n),spadistributionover\ny\n2f\n0\n;\n1\ng\n,sothat\np\n(\ny\n=1;\n\u02da\n)=\n\u02da\n;\np\n(\ny\n=0;\n\u02da\n)=1\n\n\u02da\n.Aswevary\n\u02da\n,weobtainBernoulli\ndistributionswithtmeans.WenowshowthatthisclassofBernoulli\ndistributions,onesobtainedbyvarying\n\u02da\n,isintheexponentialfamily;i.e.,\nthatthereisachoiceof\nT\n,\na\nand\nb\nsothatEquation(3.1)becomesexactly\ntheclassofBernoullidistributions.\nWewritetheBernoullidistributionas:\np\n(\ny\n;\n\u02da\n)=\n\u02da\ny\n(1\n\n\u02da\n)\n1\n\ny\n=exp(\ny\nlog\n\u02da\n+(1\n\ny\n)log(1\n\n\u02da\n))\n=exp\n\nlog\n\n\u02da\n1\n\n\u02da\n\ny\n+log(1\n\n\u02da\n)\n\n:\nThus,thenaturalparameterisgivenby\n\n=log(\n\u02da=\n(1\n\n\u02da\n)).Interestingly,if\nweinvertthisfor\n\nbysolvingfor\n\u02da\nintermsof\n\n,weobtain\n\u02da\n=\n1\n=\n(1+\ne\n\n\n).Thisisthefamiliarsigmoidfunction!Thiswillcomeupagain\nwhenwederivelogisticregressionasaGLM.Tocompletetheformulation\noftheBernoullidistributionasanexponentialfamilydistribution,wealso\nhave\nT\n(\ny\n)=\ny\na\n(\n\n)=\n\nlog(1\n\n\u02da\n)\n=log(1+\ne\n\n)\nb\n(\ny\n)=1\nThisshowsthattheBernoullidistributioncanbewrittenintheformof\nEquation(3.1),usinganappropriatechoiceof\nT\n,\na\nand\nb\n.\nLet'snowmoveontoconsidertheGaussiandistribution.Recallthat,\nwhenderivinglinearregression,thevalueof\n\u02d9\n2\nhadnoonour\nchoiceof\n\nand\nh\n\n(\nx\n).Thus,wecanchooseanarbitraryvaluefor\n\u02d9\n2\nwithout\nchanginganything.Tosimplifythederivationbelow,let'sset\n\u02d9\n2\n=1.\n2\nWe\n2\nIfweleave\n\u02d9\n2\nasavariable,theGaussiandistributioncanalsobeshowntobeinthe\nexponentialfamily,where\n\n2\nR\n2\nisnowa2-dimensionvectorthatdependsonboth\n\nand\n\u02d9\n.ForthepurposesofGLMs,however,the\n\u02d9\n2\nparametercanalsobetreatedbyconsidering\namoregeneraloftheexponentialfamily:\np\n(\ny\n;\n;\u02dd\n)=\nb\n(\na;\u02dd\n)exp((\n\nT\nT\n(\ny\n)\n\na\n(\n\n))\n=c\n(\n\u02dd\n)).Here,\n\u02dd\niscalledthe\ndispersionparameter\n,andfortheGaussian,\nc\n(\n\u02dd\n)=\n\u02d9\n2\n;\nbutgivenourabove,wewon'tneedthemoregeneralitionforthe\nexampleswewillconsiderhere.\n"}, {"page_number": 28, "text": "28\nthenhave:\np\n(\ny\n;\n\n)=\n1\np\n2\n\u02c7\nexp\n\n\n1\n2\n(\ny\n\n\n)\n2\n\n=\n1\np\n2\n\u02c7\nexp\n\n\n1\n2\ny\n2\n\n\nexp\n\n\n\n1\n2\n\n2\n\nThus,weseethattheGaussianisintheexponentialfamily,with\n\n=\n\nT\n(\ny\n)=\ny\na\n(\n\n)=\n\n2\n=\n2\n=\n\n2\n=\n2\nb\n(\ny\n)=(1\n=\np\n2\n\u02c7\n)exp(\n\ny\n2\n=\n2)\n:\nThere'remanyotherdistributionsthataremembersoftheexponen-\ntialfamily:Themultinomial(whichwe'llseelater),thePoisson(formod-\nellingcount-data;alsoseetheproblemset);thegammaandtheexponen-\ntial(formodellingcontinuous,non-negativerandomvariables,suchastime-\nintervals);thebetaandtheDirichlet(fordistributionsoverprobabilities);\nandmanymore.Inthenextsection,wewilldescribeageneral\\recipe\"\nforconstructingmodelsinwhich\ny\n(given\nx\nand\n\n)comesfromanyofthese\ndistributions.\n3.2ConstructingGLMs\nSupposeyouwouldliketobuildamodeltoestimatethenumber\ny\nofcus-\ntomersarrivinginyourstore(ornumberofpage-viewsonyourwebsite)in\nanygivenhour,basedoncertainfeatures\nx\nsuchasstorepromotions,recent\nadvertising,weather,day-of-week,etc.WeknowthatthePoissondistribu-\ntionusuallygivesagoodmodelfornumbersofvisitors.Knowingthis,how\ncanwecomeupwithamodelforourproblem?Fortunately,thePoissonisan\nexponentialfamilydistribution,sowecanapplyaGeneralizedLinearModel\n(GLM).Inthissection,wewillwewilldescribeamethodforconstructing\nGLMmodelsforproblemssuchasthese.\nMoregenerally,consideraclassiorregressionproblemwherewe\nwouldliketopredictthevalueofsomerandomvariable\ny\nasafunctionof\nx\n.ToderiveaGLMforthisproblem,wewillmakethefollowingthree\nassumptionsabouttheconditionaldistributionof\ny\ngiven\nx\nandaboutour\nmodel:\n"}, {"page_number": 29, "text": "29\n1.\ny\nj\nx\n;\n\n\u02d8\nExponentialFamily(\n\n).I.e.,given\nx\nand\n\n,thedistributionof\ny\nfollowssomeexponentialfamilydistribution,withparameter\n\n.\n2.\nGiven\nx\n,ourgoalistopredicttheexpectedvalueof\nT\n(\ny\n)given\nx\n.\nInmostofourexamples,wewillhave\nT\n(\ny\n)=\ny\n,sothismeanswe\nwouldliketheprediction\nh\n(\nx\n)outputbyourlearnedhypothesis\nh\nto\nsatisfy\nh\n(\nx\n)=E[\ny\nj\nx\n].(Notethatthisassumptionisdinthe\nchoicesfor\nh\n\n(\nx\n)forbothlogisticregressionandlinearregression.For\ninstance,inlogisticregression,wehad\nh\n\n(\nx\n)=\np\n(\ny\n=1\nj\nx\n;\n\n)=0\n\np\n(\ny\n=\n0\nj\nx\n;\n\n)+1\n\np\n(\ny\n=1\nj\nx\n;\n\n)=E[\ny\nj\nx\n;\n\n].)\n3.\nThenaturalparameter\n\nandtheinputs\nx\narerelatedlinearly:\n\n=\n\nT\nx\n.\n(Or,if\n\nisvector-valued,then\n\ni\n=\n\nT\ni\nx\n.)\nThethirdoftheseassumptionsmightseemtheleastwellof\ntheabove,anditmightbebetterthoughtofasa\\designchoice\"inour\nrecipefordesigningGLMs,ratherthanasanassumptionperse.These\nthreeassumptions/designchoiceswillallowustoderiveaveryelegantclass\noflearningalgorithms,namelyGLMs,thathavemanydesirableproperties\nsuchaseaseoflearning.Furthermore,theresultingmodelsareoftenvery\neformodellingttypesofdistributionsover\ny\n;forexample,we\nwillshortlyshowthatbothlogisticregressionandordinaryleastsquarescan\nbothbederivedasGLMs.\n3.2.1Ordinaryleastsquares\nToshowthatordinaryleastsquaresisaspecialcaseoftheGLMfamily\nofmodels,considerthesettingwherethetargetvariable\ny\n(alsocalledthe\nresponsevariable\ninGLMterminology)iscontinuous,andwemodelthe\nconditionaldistributionof\ny\ngiven\nx\nasaGaussian\nN\n(\n\u02d9\n2\n).(Here,\n\nmay\ndepend\nx\n.)So,weletthe\nExponentialFamily\n(\n\n)distributionabovebe\ntheGaussiandistribution.Aswesawpreviously,intheformulationofthe\nGaussianasanexponentialfamilydistribution,wehad\n\n=\n\n.So,wehave\nh\n\n(\nx\n)=\nE\n[\ny\nj\nx\n;\n\n]\n=\n\n=\n\n=\n\nT\nx:\nTheequalityfollowsfromAssumption2,above;thesecondequality\nfollowsfromthefactthat\ny\nj\nx\n;\n\n\u02d8N\n(\n\u02d9\n2\n),andsoitsexpectedvalueisgiven\n"}, {"page_number": 30, "text": "30\nby\n\n;thethirdequalityfollowsfromAssumption1(andourearlierderivation\nshowingthat\n\n=\n\nintheformulationoftheGaussianasanexponential\nfamilydistribution);andthelastequalityfollowsfromAssumption3.\n3.2.2Logisticregression\nWenowconsiderlogisticregression.Hereweareinterestedinbinary\ncation,so\ny\n2f\n0\n;\n1\ng\n.Giventhat\ny\nisbinary-valued,itthereforeseemsnatural\ntochoosetheBernoullifamilyofdistributionstomodeltheconditionaldis-\ntributionof\ny\ngiven\nx\n.InourformulationoftheBernoullidistributionas\nanexponentialfamilydistribution,wehad\n\u02da\n=1\n=\n(1+\ne\n\n\n).Furthermore,\nnotethatif\ny\nj\nx\n;\n\n\u02d8\nBernoulli(\n\u02da\n),thenE[\ny\nj\nx\n;\n\n]=\n\u02da\n.So,followingasimilar\nderivationastheoneforordinaryleastsquares,weget:\nh\n\n(\nx\n)=\nE\n[\ny\nj\nx\n;\n\n]\n=\n\u02da\n=1\n=\n(1+\ne\n\n\n)\n=1\n=\n(1+\ne\n\n\nT\nx\n)\nSo,thisgivesushypothesisfunctionsoftheform\nh\n\n(\nx\n)=1\n=\n(1+\ne\n\n\nT\nx\n).If\nyouarepreviouslywonderinghowwecameupwiththeformofthelogistic\nfunction1\n=\n(1+\ne\n\nz\n),thisgivesoneanswer:Onceweassumethat\ny\ncondi-\ntionedon\nx\nisBernoulli,itarisesasaconsequenceoftheofGLMs\nandexponentialfamilydistributions.\nTointroducealittlemoreterminology,thefunction\ng\ngivingthedistri-\nbution'smeanasafunctionofthenaturalparameter(\ng\n(\n\n)=E[\nT\n(\ny\n);\n\n])\niscalledthe\ncanonicalresponsefunction\n.Itsinverse,\ng\n\n1\n,iscalledthe\ncanonicallinkfunction\n.Thus,thecanonicalresponsefunctionforthe\nGaussianfamilyisjusttheidentifyfunction;andthecanonicalresponse\nfunctionfortheBernoulliisthelogisticfunction.\n3\n3.2.3Softmaxregression\nLet'slookatonemoreexampleofaGLM.Consideraproblem\ninwhichtheresponsevariable\ny\ncantakeonanyoneof\nk\nvalues,so\ny\n2\nf\n1\n;\n2\n;:::;k\ng\n.Forexample,ratherthanclassifyingemailintothetwoclasses\n3\nManytextsuse\ng\ntodenotethelinkfunction,and\ng\n\n1\ntodenotetheresponsefunction;\nbutthenotationwe'reusinghere,inheritedfromtheearlymachinelearningliterature,\nwillbemoreconsistentwiththenotationusedintherestoftheclass.\n"}, {"page_number": 31, "text": "31\nspamornot-spam|whichwouldhavebeenabinaryproblem|\nwemightwanttoclassifyitintothreeclasses,suchasspam,personalmail,\nandwork-relatedmail.Theresponsevariableisstilldiscrete,butcannow\ntakeonmorethantwovalues.Wewillthusmodelitasdistributedaccording\ntoamultinomialdistribution.\nLet'sderiveaGLMformodellingthistypeofmultinomialdata.Todo\nso,wewillbeginbyexpressingthemultinomialasanexponentialfamily\ndistribution.\nToparameterizeamultinomialover\nk\npossibleoutcomes,onecoulduse\nk\nparameters\n\u02da\n1\n;:::;\u02da\nk\nspecifyingtheprobabilityofeachoftheoutcomes.\nHowever,theseparameterswouldberedundant,ormoreformally,theywould\nnotbeindependent(sinceknowingany\nk\n\n1ofthe\n\u02da\ni\n'suniquelydetermines\nthelastone,astheymustsatisfy\nP\nk\ni\n=1\n\u02da\ni\n=1).So,wewillinsteadpa-\nrameterizethemultinomialwithonly\nk\n\n1parameters,\n\u02da\n1\n;:::;\u02da\nk\n\n1\n,where\n\u02da\ni\n=\np\n(\ny\n=\ni\n;\n\u02da\n),and\np\n(\ny\n=\nk\n;\n\u02da\n)=1\n\nP\nk\n\n1\ni\n=1\n\u02da\ni\n.Fornotationalconvenience,\nwewillalsolet\n\u02da\nk\n=1\n\nP\nk\n\n1\ni\n=1\n\u02da\ni\n,butweshouldkeepinmindthatthisis\nnotaparameter,andthatitisfullyspby\n\u02da\n1\n;:::;\u02da\nk\n\n1\n.\nToexpressthemultinomialasanexponentialfamilydistribution,wewill\n\nT\n(\ny\n)\n2\nR\nk\n\n1\nasfollows:\nT\n(1)=\n2\n6\n6\n6\n6\n6\n4\n1\n0\n0\n.\n.\n.\n0\n3\n7\n7\n7\n7\n7\n5\n;T\n(2)=\n2\n6\n6\n6\n6\n6\n4\n0\n1\n0\n.\n.\n.\n0\n3\n7\n7\n7\n7\n7\n5\n;T\n(3)=\n2\n6\n6\n6\n6\n6\n4\n0\n0\n1\n.\n.\n.\n0\n3\n7\n7\n7\n7\n7\n5\n;\n\n;T\n(\nk\n\n1)=\n2\n6\n6\n6\n6\n6\n4\n0\n0\n0\n.\n.\n.\n1\n3\n7\n7\n7\n7\n7\n5\n;T\n(\nk\n)=\n2\n6\n6\n6\n6\n6\n4\n0\n0\n0\n.\n.\n.\n0\n3\n7\n7\n7\n7\n7\n5\n;\nUnlikeourpreviousexamples,herewedo\nnot\nhave\nT\n(\ny\n)=\ny\n;also,\nT\n(\ny\n)is\nnowa\nk\n\n1dimensionalvector,ratherthanarealnumber.Wewillwrite\n(\nT\n(\ny\n))\ni\ntodenotethe\ni\n-thelementofthevector\nT\n(\ny\n).\nWeintroduceonemoreveryusefulpieceofnotation.Anindicatorfunc-\ntion1\n\ntakesonavalueof1ifitsargumentistrue,and0otherwise\n(1\nf\nTrue\ng\n=1,1\nf\nFalse\ng\n=0).Forexample,1\nf\n2=3\ng\n=0,and1\nf\n3=\n5\n\n2\ng\n=1.So,wecanalsowritetherelationshipbetween\nT\n(\ny\n)and\ny\nas\n(\nT\n(\ny\n))\ni\n=1\nf\ny\n=\ni\ng\n.(Beforeyoucontinuereading,pleasemakesureyouun-\nderstandwhythisistrue!)Further,wehavethatE[(\nT\n(\ny\n))\ni\n]=\nP\n(\ny\n=\ni\n)=\n\u02da\ni\n.\nWearenowreadytoshowthatthemultinomialisamemberofthe\n"}, {"page_number": 32, "text": "32\nexponentialfamily.Wehave:\np\n(\ny\n;\n\u02da\n)=\n\u02da\n1\nf\ny\n=1\ng\n1\n\u02da\n1\nf\ny\n=2\ng\n2\n\n\u02da\n1\nf\ny\n=\nk\ng\nk\n=\n\u02da\n1\nf\ny\n=1\ng\n1\n\u02da\n1\nf\ny\n=2\ng\n2\n\n\u02da\n1\n\nP\nk\n\n1\ni\n=1\n1\nf\ny\n=\ni\ng\nk\n=\n\u02da\n(\nT\n(\ny\n))\n1\n1\n\u02da\n(\nT\n(\ny\n))\n2\n2\n\n\u02da\n1\n\nP\nk\n\n1\ni\n=1\n(\nT\n(\ny\n))\ni\nk\n=exp((\nT\n(\ny\n))\n1\nlog(\n\u02da\n1\n)+(\nT\n(\ny\n))\n2\nlog(\n\u02da\n2\n)+\n\n+\n\n1\n\nP\nk\n\n1\ni\n=1\n(\nT\n(\ny\n))\ni\n\nlog(\n\u02da\nk\n))\n=exp((\nT\n(\ny\n))\n1\nlog(\n\u02da\n1\n=\u02da\nk\n)+(\nT\n(\ny\n))\n2\nlog(\n\u02da\n2\n=\u02da\nk\n)+\n\n+(\nT\n(\ny\n))\nk\n\n1\nlog(\n\u02da\nk\n\n1\n=\u02da\nk\n)+log(\n\u02da\nk\n))\n=\nb\n(\ny\n)exp(\n\nT\nT\n(\ny\n)\n\na\n(\n\n))\nwhere\n\n=\n2\n6\n6\n6\n4\nlog(\n\u02da\n1\n=\u02da\nk\n)\nlog(\n\u02da\n2\n=\u02da\nk\n)\n.\n.\n.\nlog(\n\u02da\nk\n\n1\n=\u02da\nk\n)\n3\n7\n7\n7\n5\n;\na\n(\n\n)=\n\nlog(\n\u02da\nk\n)\nb\n(\ny\n)=1\n:\nThiscompletesourformulationofthemultinomialasanexponentialfamily\ndistribution.\nThelinkfunctionisgiven(for\ni\n=1\n;:::;k\n)by\n\ni\n=log\n\u02da\ni\n\u02da\nk\n:\nForconvenience,wehavealso\n\nk\n=log(\n\u02da\nk\n=\u02da\nk\n)=0.Toinvertthe\nlinkfunctionandderivetheresponsefunction,wethereforehavethat\ne\n\ni\n=\n\u02da\ni\n\u02da\nk\n\u02da\nk\ne\n\ni\n=\n\u02da\ni\n(3.2)\n\u02da\nk\nk\nX\ni\n=1\ne\n\ni\n=\nk\nX\ni\n=1\n\u02da\ni\n=1\nThisimpliesthat\n\u02da\nk\n=1\n=\nP\nk\ni\n=1\ne\n\ni\n,whichcanbesubstitutedbackintoEqua-\ntion(3.2)togivetheresponsefunction\n\u02da\ni\n=\ne\n\ni\nP\nk\nj\n=1\ne\n\nj\n"}, {"page_number": 33, "text": "33\nThisfunctionmappingfromthe\n\n'stothe\n\u02da\n'siscalledthe\nsoftmax\nfunction.\nTocompleteourmodel,weuseAssumption3,givenearlier,thatthe\n\ni\n's\narelinearlyrelatedtothe\nx\n's.So,have\n\ni\n=\n\nT\ni\nx\n(for\ni\n=1\n;:::;k\n\n1),\nwhere\n\n1\n;:::;\nk\n\n1\n2\nR\nd\n+1\naretheparametersofourmodel.Fornotational\nconvenience,wecanalso\n\nk\n=0,sothat\n\nk\n=\n\nT\nk\nx\n=0,asgiven\npreviously.Hence,ourmodelassumesthattheconditionaldistributionof\ny\ngiven\nx\nisgivenby\np\n(\ny\n=\ni\nj\nx\n;\n\n)=\n\u02da\ni\n=\ne\n\ni\nP\nk\nj\n=1\ne\n\nj\n=\ne\n\nT\ni\nx\nP\nk\nj\n=1\ne\n\nT\nj\nx\n(3.3)\nThismodel,whichappliestoclascationproblemswhere\ny\n2f\n1\n;:::;k\ng\n,is\ncalled\nsoftmaxregression\n.Itisageneralizationoflogisticregression.\nOurhypothesiswilloutput\nh\n\n(\nx\n)=E[\nT\n(\ny\n)\nj\nx\n;\n\n]\n=E\n2\n6\n6\n6\n4\n1\nf\ny\n=1\ng\n1\nf\ny\n=2\ng\n.\n.\n.\n1\nf\ny\n=\nk\n\n1\ng\n\n\n\n\n\n\n\n\n\nx\n;\n\n3\n7\n7\n7\n5\n=\n2\n6\n6\n6\n4\n\u02da\n1\n\u02da\n2\n.\n.\n.\n\u02da\nk\n\n1\n3\n7\n7\n7\n5\n=\n2\n6\n6\n6\n6\n6\n6\n4\nexp(\n\nT\n1\nx\n)\nP\nk\nj\n=1\nexp(\n\nT\nj\nx\n)\nexp(\n\nT\n2\nx\n)\nP\nk\nj\n=1\nexp(\n\nT\nj\nx\n)\n.\n.\n.\nexp(\n\nT\nk\n\n1\nx\n)\nP\nk\nj\n=1\nexp(\n\nT\nj\nx\n)\n3\n7\n7\n7\n7\n7\n7\n5\n:\nInotherwords,ourhypothesiswilloutputtheestimatedprobabilitythat\np\n(\ny\n=\ni\nj\nx\n;\n\n),foreveryvalueof\ni\n=1\n;:::;k\n.(Eventhough\nh\n\n(\nx\n)as\naboveisonly\nk\n\n1dimensional,clearly\np\n(\ny\n=\nk\nj\nx\n;\n\n)canbeobtainedas\n1\n\nP\nk\n\n1\ni\n=1\n\u02da\ni\n.)\n"}, {"page_number": 34, "text": "34\nLastly,let'sdiscussparameterSimilartoouroriginalderivation\nofordinaryleastsquaresandlogisticregression,ifwehaveatrainingsetof\nn\nexamples\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=1\n;:::;n\ng\nandwouldliketolearntheparameters\n\ni\nofthismodel,wewouldbeginbywritingdownthelog-likelihood\n`\n(\n\n)=\nn\nX\ni\n=1\nlog\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\n=\nn\nX\ni\n=1\nlog\nk\nY\nl\n=1\n \ne\n\nT\nl\nx\n(\ni\n)\nP\nk\nj\n=1\ne\n\nT\nj\nx\n(\ni\n)\n!\n1\nf\ny\n(\ni\n)\n=\nl\ng\nToobtainthesecondlineabove,weusedthefor\np\n(\ny\nj\nx\n;\n\n)given\ninEquation(3.3).Wecannowobtainthemaximumlikelihoodestimateof\ntheparametersbymaximizing\n`\n(\n\n)intermsof\n\n,usingamethodsuchas\ngradientascentorNewton'smethod.\n"}, {"page_number": 35, "text": "Chapter4\nGenerativelearningalgorithms\nSofar,we'vemainlybeentalkingaboutlearningalgorithmsthatmodel\np\n(\ny\nj\nx\n;\n\n),theconditionaldistributionof\ny\ngiven\nx\n.Forinstance,logistic\nregressionmodeled\np\n(\ny\nj\nx\n;\n\n)as\nh\n\n(\nx\n)=\ng\n(\n\nT\nx\n)where\ng\nisthesigmoidfunc-\ntion.Inthesenotes,we'lltalkaboutattypeoflearningalgorithm.\nConsideraprobleminwhichwewanttolearntodistinguish\nbetweenelephants(\ny\n=1)anddogs(\ny\n=0),basedonsomefeaturesof\nananimal.Givenatrainingset,analgorithmlikelogisticregressionor\ntheperceptronalgorithm(basically)triestoastraightline|thatis,a\ndecisionboundary|thatseparatestheelephantsanddogs.Then,toclassify\nanewanimalaseitheranelephantoradog,itchecksonwhichsideofthe\ndecisionboundaryitfalls,andmakesitspredictionaccordingly.\nHere'satapproach.First,lookingatelephants,wecanbuilda\nmodelofwhatelephantslooklike.Then,lookingatdogs,wecanbuilda\nseparatemodelofwhatdogslooklike.Finally,toclassifyanewanimal,we\ncanmatchthenewanimalagainsttheelephantmodel,andmatchitagainst\nthedogmodel,toseewhetherthenewanimallooksmoreliketheelephants\normorelikethedogswehadseeninthetrainingset.\nAlgorithmsthattrytolearn\np\n(\ny\nj\nx\n)directly(suchaslogisticregression),\noralgorithmsthattrytolearnmappingsdirectlyfromthespaceofinputs\nX\ntothelabels\nf\n0\n;\n1\ng\n,(suchastheperceptronalgorithm)arecalled\ndiscrim-\ninative\nlearningalgorithms.Here,we'lltalkaboutalgorithmsthatinstead\ntrytomodel\np\n(\nx\nj\ny\n)(and\np\n(\ny\n)).Thesealgorithmsarecalled\ngenerative\nlearningalgorithms.Forinstance,if\ny\nindicateswhetheranexampleisa\ndog(0)oranelephant(1),then\np\n(\nx\nj\ny\n=0)modelsthedistributionofdogs'\nfeatures,and\np\n(\nx\nj\ny\n=1)modelsthedistributionofelephants'features.\nAftermodeling\np\n(\ny\n)(calledthe\nclasspriors\n)and\np\n(\nx\nj\ny\n),ouralgorithm\n35\n"}, {"page_number": 36, "text": "36\ncanthenuseBayesruletoderivetheposteriordistributionon\ny\ngiven\nx\n:\np\n(\ny\nj\nx\n)=\np\n(\nx\nj\ny\n)\np\n(\ny\n)\np\n(\nx\n)\n:\nHere,thedenominatorisgivenby\np\n(\nx\n)=\np\n(\nx\nj\ny\n=1)\np\n(\ny\n=1)+\np\n(\nx\nj\ny\n=\n0)\np\n(\ny\n=0)(youshouldbeabletoverifythatthisistruefromthestandard\npropertiesofprobabilities),andthuscanalsobeexpressedintermsofthe\nquantities\np\n(\nx\nj\ny\n)and\np\n(\ny\n)thatwe'velearned.Actually,ifwerecalculating\np\n(\ny\nj\nx\n)inordertomakeaprediction,thenwedon'tactuallyneedtocalculate\nthedenominator,since\nargmax\ny\np\n(\ny\nj\nx\n)=argmax\ny\np\n(\nx\nj\ny\n)\np\n(\ny\n)\np\n(\nx\n)\n=argmax\ny\np\n(\nx\nj\ny\n)\np\n(\ny\n)\n:\n4.1Gaussiandiscriminantanalysis\nThegenerativelearningalgorithmthatwe'lllookatisGaussiandiscrim-\ninantanalysis(GDA).Inthismodel,we'llassumethat\np\n(\nx\nj\ny\n)isdistributed\naccordingtoamultivariatenormaldistribution.Let'stalkaboutthe\npropertiesofmultivariatenormaldistributionsbeforemovingontotheGDA\nmodelitself.\n4.1.1Themultivariatenormaldistribution\nThemultivariatenormaldistributionin\nd\n-dimensions,alsocalledthemulti-\nvariateGaussiandistribution,isparameterizedbya\nmeanvector\n\n2\nR\nd\nanda\ncovariancematrix\n\n2\nR\nd\n\nd\n,where\n\n0issymmetricandpositive\nAlsowritten\\\nN\n(\n\nitsdensityisgivenby:\np\n(\nx\n;\n\n=\n1\n(2\n\u02c7\n)\nd=\n2\nj\n\nj\n1\n=\n2\nexp\n\n\n1\n2\n(\nx\n\n\n)\nT\n\n\n1\n(\nx\n\n\n)\n\n:\nIntheequationabove,\\\nj\n\nj\n\"denotesthedeterminantofthematrix\nForarandomvariable\nX\ndistributed\nN\n(\n\nthemeanis(unsurpris-\ningly)givenby\n\n:\nE[\nX\n]=\nZ\nx\nxp\n(\nx\n;\n\n\ndx\n=\n\nThe\ncovariance\nofavector-valuedrandomvariable\nZ\nisasCov(\nZ\n)=\nE[(\nZ\n\nE[\nZ\n])(\nZ\n\nE[\nZ\n])\nT\n].Thisgeneralizesthenotionofthevarianceofa\n"}, {"page_number": 37, "text": "37\nreal-valuedrandomvariable.ThecovariancecanalsobeasCov(\nZ\n)=\nE[\nZZ\nT\n]\n\n(E[\nZ\n])(E[\nZ\n])\nT\n.(Youshouldbeabletoprovetoyourselfthatthese\ntwoareequivalent.)If\nX\n\u02d8N\n(\n\nthen\nCov(\nX\n)=\n:\nHerearesomeexamplesofwhatthedensityofaGaussiandistribution\nlookslike:\nTheleft-mostureshowsaGaussianwithmeanzero(thatis,the2x1\nzero-vector)andcovariancematrix=\nI\n(the2x2identitymatrix).AGaus-\nsianwithzeromeanandidentitycovarianceisalsocalledthe\nstandardnor-\nmaldistribution\n.ThemiddleshowsthedensityofaGaussianwith\nzeromeanand=0\n:\n6\nI\n;andintherightmostshowsonewith,=2\nI\n.\nWeseethatasbecomeslarger,theGaussianbecomesmore\\spread-out,\"\nandasitbecomessmaller,thedistributionbecomesmore\\compressed.\"\nLet'slookatsomemoreexamples.\nTheaboveshowGaussianswithmean0,andwithcovariance\nmatricesrespectively\n=\n\n10\n01\n\n;=\n\n10.5\n0.51\n\n;=\n\n10.8\n0.81\n\n:\nTheleftmostshowsthefamiliarstandardnormaldistribution,andwe\nseethatasweincreasetheentryinthedensitybecomesmore\n"}, {"page_number": 38, "text": "38\n\\compressed\"towardsthe45\n\nline(givenby\nx\n1\n=\nx\n2\n).Wecanseethismore\nclearlywhenwelookatthecontoursofthesamethreedensities:\nHere'sonelastsetofexamplesgeneratedbyvarying\nTheplotsaboveused,respectively,\n=\n\n1-0.5\n-0.51\n\n;=\n\n1-0.8\n-0.81\n\n;=\n\n30.8\n0.81\n\n:\nFromtheleftmostandmiddleweseethatbydecreasingthe\ndiagonalelementsofthecovariancematrix,thedensitynowbecomes\\com-\npressed\"again,butintheoppositedirection.Lastly,aswevarythepa-\nrameters,moregenerallythecontourswillformellipses(therightmost\nshowinganexample).\nAsourlastsetofexamples,=\nI\n,byvarying\n\n,wecanalsomove\nthemeanofthedensityaround.\n"}, {"page_number": 39, "text": "39\nTheaboveweregeneratedusing=\nI\n,andrespectively\n\n=\n\n1\n0\n\n;\n\n=\n\n-0.5\n0\n\n;\n\n=\n\n-1\n-1.5\n\n:\n4.1.2TheGaussiandiscriminantanalysismodel\nWhenwehaveaprobleminwhichtheinputfeatures\nx\nare\ncontinuous-valuedrandomvariables,wecanthenusetheGaussianDiscrim-\ninantAnalysis(GDA)model,whichmodels\np\n(\nx\nj\ny\n)usingamultivariatenor-\nmaldistribution.Themodelis:\ny\n\u02d8\nBernoulli(\n\u02da\n)\nx\nj\ny\n=0\n\u02d8N\n(\n\n0\n;\n\nx\nj\ny\n=1\n\u02d8N\n(\n\n1\n;\n\nWritingoutthedistributions,thisis:\np\n(\ny\n)=\n\u02da\ny\n(1\n\n\u02da\n)\n1\n\ny\np\n(\nx\nj\ny\n=0)=\n1\n(2\n\u02c7\n)\nd=\n2\nj\n\nj\n1\n=\n2\nexp\n\n\n1\n2\n(\nx\n\n\n0\n)\nT\n\n\n1\n(\nx\n\n\n0\n)\n\np\n(\nx\nj\ny\n=1)=\n1\n(2\n\u02c7\n)\nd=\n2\nj\n\nj\n1\n=\n2\nexp\n\n\n1\n2\n(\nx\n\n\n1\n)\nT\n\n\n1\n(\nx\n\n\n1\n)\n\nHere,theparametersofourmodelare\n\u02da\n,\n\n0\nand\n\n1\n.(Notethatwhile\nthere'retwotmeanvectors\n\n0\nand\n\n1\n,thismodelisusuallyapplied\nusingonlyonecovariancematrixThelog-likelihoodofthedataisgiven\nby\n`\n(\n\u02da;\n0\n;\n1\n;\n=log\nn\nY\ni\n=1\np\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n;\n\u02da;\n0\n;\n1\n;\n\n=log\nn\nY\ni\n=1\np\n(\nx\n(\ni\n)\nj\ny\n(\ni\n)\n;\n\n0\n;\n1\n;\n\np\n(\ny\n(\ni\n)\n;\n\u02da\n)\n:\n"}, {"page_number": 40, "text": "40\nBymaximizing\n`\nwithrespecttotheparameters,wethemaximumlike-\nlihoodestimateoftheparameters(seeproblemset1)tobe:\n\u02da\n=\n1\nn\nn\nX\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\n\n0\n=\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\nx\n(\ni\n)\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\n\n1\n=\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\nx\n(\ni\n)\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\n=\n1\nn\nn\nX\ni\n=1\n(\nx\n(\ni\n)\n\n\ny\n(\ni\n)\n)(\nx\n(\ni\n)\n\n\ny\n(\ni\n)\n)\nT\n:\nPictorially,whatthealgorithmisdoingcanbeseeninasfollows:\nShowninthearethetrainingset,aswellasthecontoursofthe\ntwoGaussiandistributionsthathavebeentothedataineachofthe\ntwoclasses.NotethatthetwoGaussianshavecontoursthatarethesame\nshapeandorientation,sincetheyshareacovariancematrixbuttheyhave\ntmeans\n\n0\nand\n\n1\n.Alsoshownintheisthestraightline\ngivingthedecisionboundaryatwhich\np\n(\ny\n=1\nj\nx\n)=0\n:\n5.Ononesideof\ntheboundary,we'llpredict\ny\n=1tobethemostlikelyoutcome,andonthe\notherside,we'llpredict\ny\n=0.\n"}, {"page_number": 41, "text": "41\n4.1.3Discussion:GDAandlogisticregression\nTheGDAmodelhasaninterestingrelationshiptologisticregression.Ifwe\nviewthequantity\np\n(\ny\n=1\nj\nx\n;\n\u02da;\n0\n;\n1\n;\nasafunctionof\nx\n,we'llthatit\ncanbeexpressedintheform\np\n(\ny\n=1\nj\nx\n;\n\u02da;\n\n;\n0\n;\n1\n)=\n1\n1+exp(\n\n\nT\nx\n)\n;\nwhere\n\nissomeappropriatefunctionof\n\u02da;\n\n;\n0\n;\n1\n.\n1\nThisisexactlytheform\nthatlogisticregression|adiscriminativealgorithm|usedtomodel\np\n(\ny\n=\n1\nj\nx\n).\nWhenwouldwepreferonemodeloveranother?GDAandlogisticregres-\nsionwill,ingeneral,givetdecisionboundarieswhentrainedonthe\nsamedataset.Whichisbetter?\nWejustarguedthatif\np\n(\nx\nj\ny\n)ismultivariategaussian(withshared\nthen\np\n(\ny\nj\nx\n)necessarilyfollowsalogisticfunction.Theconverse,however,\nisnottrue;i.e.,\np\n(\ny\nj\nx\n)beingalogisticfunctiondoesnotimply\np\n(\nx\nj\ny\n)is\nmultivariategaussian.ThisshowsthatGDAmakes\nstronger\nmodelingas-\nsumptionsaboutthedatathandoeslogisticregression.Itturnsoutthat\nwhenthesemodelingassumptionsarecorrect,thenGDAwillbetter\ntothedata,andisabettermodel.Sp,when\np\n(\nx\nj\ny\n)isindeedgaus-\nsian(withsharedthenGDAis\nasymptoticallyt\n.Informally,\nthismeansthatinthelimitofverylargetrainingsets(large\nn\n),thereisno\nalgorithmthatisstrictlybetterthanGDA(intermsof,say,howaccurately\ntheyestimate\np\n(\ny\nj\nx\n)).Inparticular,itcanbeshownthatinthissetting,\nGDAwillbeabetteralgorithmthanlogisticregression;andmoregenerally,\nevenforsmalltrainingsetsizes,wewouldgenerallyexpectGDAtobetter.\nIncontrast,bymakingtlyweakerassumptions,logisticregres-\nsionisalsomore\nrobust\nandlesssensitivetoincorrectmodelingassumptions.\nTherearemanytsetsofassumptionsthatwouldleadto\np\n(\ny\nj\nx\n)taking\ntheformofalogisticfunction.Forexample,if\nx\nj\ny\n=0\n\u02d8\nPoisson(\n\n0\n),and\nx\nj\ny\n=1\n\u02d8\nPoisson(\n\n1\n),then\np\n(\ny\nj\nx\n)willbelogistic.Logisticregressionwill\nalsoworkwellonPoissondatalikethis.ButifweweretouseGDAonsuch\ndata|andGaussiandistributionstosuchnon-Gaussiandata|thenthe\nresultswillbelesspredictable,andGDAmay(ormaynot)dowell.\nTosummarize:GDAmakesstrongermodelingassumptions,andismore\ndatat(i.e.,requireslesstrainingdatatolearn\\well\")whenthemod-\nelingassumptionsarecorrectoratleastapproximatelycorrect.Logistic\n1\nThisusestheconventionofthe\nx\n(\ni\n)\n'sontheright-hand-sidetobe(\nd\n+1)-\ndimensionalvectorsbyaddingtheextracoordinate\nx\n(\ni\n)\n0\n=1;seeproblemset1.\n"}, {"page_number": 42, "text": "42\nregressionmakesweakerassumptions,andissigtlymorerobustto\ndeviationsfrommodelingassumptions.Sp,whenthedataisin-\ndeednon-Gaussian,theninthelimitoflargedatasets,logisticregressionwill\nalmostalwaysdobetterthanGDA.Forthisreason,inpracticelogisticre-\ngressionisusedmoreoftenthanGDA.(Somerelatedconsiderationsabout\ndiscriminativevs.generativemodelsalsoapplyfortheNaiveBayesalgo-\nrithmthatwediscussnext,buttheNaiveBayesalgorithmisstillconsidered\naverygood,andiscertainlyalsoaverypopular,algorithm.)\n4.2Naivebayes\nInGDA,thefeaturevectors\nx\nwerecontinuous,real-valuedvectors.Let's\nnowtalkaboutatlearningalgorithminwhichthe\nx\nj\n'sarediscrete-\nvalued.\nForourmotivatingexample,considerbuildinganemailspamlterusing\nmachinelearning.Here,wewishtoclassifymessagesaccordingtowhether\ntheyareunsolicitedcommercial(spam)email,ornon-spamemail.After\nlearningtodothis,wecanthenhaveourmailreaderautomatically\noutthespammessagesandperhapsplacetheminaseparatemailfolder.\nClassifyingemailsisoneexampleofabroadersetofproblemscalled\ntext\n\n.\nLet'ssaywehaveatrainingset(asetofemailslabeledasspamornon-\nspam).We'llbeginourconstructionofourspamlterbyspecifyingthe\nfeatures\nx\nj\nusedtorepresentanemail.\nWewillrepresentanemailviaafeaturevectorwhoselengthisequalto\nthenumberofwordsinthedictionary.Sp,ifanemailcontainsthe\nj\n-thwordofthedictionary,thenwewillset\nx\nj\n=1;otherwise,welet\nx\nj\n=0.\nForinstance,thevector\nx\n=\n2\n6\n6\n6\n6\n6\n6\n6\n6\n6\n4\n1\n0\n0\n.\n.\n.\n1\n.\n.\n.\n0\n3\n7\n7\n7\n7\n7\n7\n7\n7\n7\n5\na\naardvark\naardwolf\n.\n.\n.\nbuy\n.\n.\n.\nzygmurgy\nisusedtorepresentanemailthatcontainsthewords\\a\"and\\buy,\"butnot\n"}, {"page_number": 43, "text": "43\n\\aardvark,\"\\aardwolf\"or\\zygmurgy.\"\n2\nThesetofwordsencodedintothe\nfeaturevectoriscalledthe\nvocabulary\n,sothedimensionof\nx\nisequalto\nthesizeofthevocabulary.\nHavingchosenourfeaturevector,wenowwanttobuildagenerative\nmodel.So,wehavetomodel\np\n(\nx\nj\ny\n).Butifwehave,say,avocabularyof\n50000words,then\nx\n2f\n0\n;\n1\ng\n50000\n(\nx\nisa50000-dimensionalvectorof0'sand\n1's),andifweweretomodel\nx\nexplicitlywithamultinomialdistributionover\nthe2\n50000\npossibleoutcomes,thenwe'dendupwitha(2\n50000\n\n1)-dimensional\nparametervector.Thisisclearlytoomanyparameters.\nTomodel\np\n(\nx\nj\ny\n),wewillthereforemakeaverystrongassumption.Wewill\nassumethatthe\nx\ni\n'sareconditionallyindependentgiven\ny\n.Thisassumption\niscalledthe\nNaiveBayes(NB)assumption\n,andtheresultingalgorithmis\ncalledthe\nNaiveBayes\n.Forinstance,if\ny\n=1meansspamemail;\n\\buy\"isword2087and\\price\"isword39831;thenweareassumingthatif\nItellyou\ny\n=1(thataparticularpieceofemailisspam),thenknowledge\nof\nx\n2087\n(knowledgeofwhether\\buy\"appearsinthemessage)willhaveno\nonyourbeliefsaboutthevalueof\nx\n39831\n(whether\\price\"appears).\nMoreformally,thiscanbewritten\np\n(\nx\n2087\nj\ny\n)=\np\n(\nx\n2087\nj\ny;x\n39831\n).(Notethat\nthisis\nnot\nthesameassayingthat\nx\n2087\nand\nx\n39831\nareindependent,which\nwouldhavebeenwritten\\\np\n(\nx\n2087\n)=\np\n(\nx\n2087\nj\nx\n39831\n)\";rather,weareonly\nassumingthat\nx\n2087\nand\nx\n39831\nareconditionallyindependent\ngiven\ny\n.)\nWenowhave:\np\n(\nx\n1\n;:::;x\n50000\nj\ny\n)\n=\np\n(\nx\n1\nj\ny\n)\np\n(\nx\n2\nj\ny;x\n1\n)\np\n(\nx\n3\nj\ny;x\n1\n;x\n2\n)\n\np\n(\nx\n50000\nj\ny;x\n1\n;:::;x\n49999\n)\n=\np\n(\nx\n1\nj\ny\n)\np\n(\nx\n2\nj\ny\n)\np\n(\nx\n3\nj\ny\n)\n\np\n(\nx\n50000\nj\ny\n)\n=\nd\nY\nj\n=1\np\n(\nx\nj\nj\ny\n)\nTheequalitysimplyfollowsfromtheusualpropertiesofprobabilities,\nandthesecondequalityusedtheNBassumption.Wenotethateventhough\n2\nActually,ratherthanlookingthroughanEnglishdictionaryforthelistofallEnglish\nwords,inpracticeitismorecommontolookthroughourtrainingsetandencodeinour\nfeaturevectoronlythewordsthatoccuratleastoncethere.Apartfromreducingthe\nnumberofwordsmodeledandhencereducingourcomputationalandspacerequirements,\nthisalsohastheadvantageofallowingustomodel/includeasafeaturemanywords\nthatmayappearinyouremail(suchas\\cs229\")butthatyouwon'tinadictionary.\nSometimes(asinthehomework),wealsoexcludetheveryhighfrequencywords(which\nwillbewordslike\\the,\"\\of,\"\\and\";thesehighfrequency,\\contentfree\"wordsarecalled\nstopwords\n)sincetheyoccurinsomanydocumentsanddolittletoindicatewhetheran\nemailisspamornon-spam.\n"}, {"page_number": 44, "text": "44\ntheNaiveBayesassumptionisanextremelystrongassumptions,theresulting\nalgorithmworkswellonmanyproblems.\nOurmodelisparameterizedby\n\u02da\nj\nj\ny\n=1\n=\np\n(\nx\nj\n=1\nj\ny\n=1),\n\u02da\nj\nj\ny\n=0\n=\np\n(\nx\nj\n=\n1\nj\ny\n=0),and\n\u02da\ny\n=\np\n(\ny\n=1).Asusual,givenatrainingset\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=\n1\n;:::;n\ng\n,wecanwritedownthejointlikelihoodofthedata:\nL\n(\n\u02da\ny\n;\u02da\nj\nj\ny\n=0\n;\u02da\nj\nj\ny\n=1\n)=\nn\nY\ni\n=1\np\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n)\n:\nMaximizingthiswithrespectto\n\u02da\ny\n;\u02da\nj\nj\ny\n=0\nand\n\u02da\nj\nj\ny\n=1\ngivesthemaximum\nlikelihoodestimates:\n\u02da\nj\nj\ny\n=1\n=\nP\nn\ni\n=1\n1\nf\nx\n(\ni\n)\nj\n=1\n^\ny\n(\ni\n)\n=1\ng\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\n\u02da\nj\nj\ny\n=0\n=\nP\nn\ni\n=1\n1\nf\nx\n(\ni\n)\nj\n=1\n^\ny\n(\ni\n)\n=0\ng\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\n\u02da\ny\n=\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\nn\nIntheequationsabove,the\\\n^\n\"symbolmeans\\and.\"Theparametershave\naverynaturalinterpretation.Forinstance,\n\u02da\nj\nj\ny\n=1\nisjustthefractionofthe\nspam(\ny\n=1)emailsinwhichword\nj\ndoesappear.\nHavingalltheseparameters,tomakeapredictiononanewexample\nwithfeatures\nx\n,wethensimplycalculate\np\n(\ny\n=1\nj\nx\n)=\np\n(\nx\nj\ny\n=1)\np\n(\ny\n=1)\np\n(\nx\n)\n=\n\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n=1)\n\np\n(\ny\n=1)\n\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n=1)\n\np\n(\ny\n=1)+\n\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n=0)\n\np\n(\ny\n=0)\n;\nandpickwhicheverclasshasthehigherposteriorprobability.\nLastly,wenotethatwhilewehavedevelopedtheNaiveBayesalgorithm\nmainlyforthecaseofproblemswherethefeatures\nx\nj\narebinary-valued,the\ngeneralizationtowhere\nx\nj\ncantakevaluesin\nf\n1\n;\n2\n;:::;k\nj\ng\nisstraightforward.\nHere,wewouldsimplymodel\np\n(\nx\nj\nj\ny\n)asmultinomialratherthanasBernoulli.\nIndeed,evenifsomeoriginalinputattribute(say,thelivingareaofahouse,\nasinourearlierexample)werecontinuousvalued,itisquitecommonto\ndiscretize\nit|thatis,turnitintoasmallsetofdiscretevalues|andapply\nNaiveBayes.Forinstance,ifweusesomefeature\nx\nj\ntorepresentlivingarea,\nwemightdiscretizethecontinuousvaluesasfollows:\n"}, {"page_number": 45, "text": "45\nLivingarea(sq.feet)\n<\n400\n400-800\n800-1200\n1200-1600\n>\n1600\nx\ni\n1\n2\n3\n4\n5\nThus,forahousewithlivingarea890squarefeet,wewouldsetthevalue\nofthecorrespondingfeature\nx\nj\nto3.WecanthenapplytheNaiveBayes\nalgorithm,andmodel\np\n(\nx\nj\nj\ny\n)withamultinomialdistribution,asdescribed\npreviously.Whentheoriginal,continuous-valuedattributesarenotwell-\nmodeledbyamultivariatenormaldistribution,discretizingthefeaturesand\nusingNaiveBayes(insteadofGDA)willoftenresultinabetter\n4.2.1Laplacesmoothing\nTheNaiveBayesalgorithmaswehavedescribeditwillworkfairlywell\nformanyproblems,butthereisasimplechangethatmakesitworkmuch\nbetter,especiallyfortextLet'sdiscussaproblemwith\nthealgorithminitscurrentform,andthentalkabouthowwecanxit.\nConsiderspam/email,andlet'ssupposethat,weareinthe\nyearof20xx,aftercompletingCS229andhavingdoneexcellentworkonthe\nproject,youdecidearoundMay20xxtosubmitworkyoudidtotheNeurIPS\nconferenceforpublication.\n3\nBecauseyouendupdiscussingtheconference\ninyouremails,youalsostartgettingmessageswiththeword\\neurips\"\ninit.ButthisisyourNeurIPSpaper,anduntilthistime,youhad\nnotpreviouslyseenanyemailscontainingtheword\\neurips\";inparticular\n\\neurips\"didnoteverappearinyourtrainingsetofspam/non-spamemails.\nAssumingthat\\neurips\"wasthe35000thwordinthedictionary,yourNaive\nBayesspamthereforehadpickeditsmaximumlikelihoodestimatesof\ntheparameters\n\u02da\n35000\nj\ny\ntobe\n\u02da\n35000\nj\ny\n=1\n=\nP\nn\ni\n=1\n1\nf\nx\n(\ni\n)\n35000\n=1\n^\ny\n(\ni\n)\n=1\ng\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\n=0\n\u02da\n35000\nj\ny\n=0\n=\nP\nn\ni\n=1\n1\nf\nx\n(\ni\n)\n35000\n=1\n^\ny\n(\ni\n)\n=0\ng\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\n=0\nI.e.,becauseithasneverseen\\neurips\"beforeineitherspamornon-spam\ntrainingexamples,itthinkstheprobabilityofseeingitineithertypeofemail\niszero.Hence,whentryingtodecideifoneofthesemessagescontaining\n3\nNeurIPSisoneofthetopmachinelearningconferences.Thedeadlineforsubmitting\napaperistypicallyinMay-June.\n"}, {"page_number": 46, "text": "46\n\\neurips\"isspam,itcalculatestheclassposteriorprobabilities,andobtains\np\n(\ny\n=1\nj\nx\n)=\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n=1)\np\n(\ny\n=1)\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n=1)\np\n(\ny\n=1)+\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n=0)\np\n(\ny\n=0)\n=\n0\n0\n:\nThisisbecauseeachoftheterms\\\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n)\"includesaterm\np\n(\nx\n35000\nj\ny\n)=\n0thatismultipliedintoit.Hence,ouralgorithmobtains0\n=\n0,anddoesn't\nknowhowtomakeaprediction.\nStatingtheproblemmorebroadly,itisstatisticallyabadideatoesti-\nmatetheprobabilityofsomeeventtobezerojustbecauseyouhaven'tseen\nitbeforeinyourtrainingset.Taketheproblemofestimatingthemean\nofamultinomialrandomvariable\nz\ntakingvaluesin\nf\n1\n;:::;k\ng\n.Wecanpa-\nrameterizeourmultinomialwith\n\u02da\nj\n=\np\n(\nz\n=\nj\n).Givenasetof\nn\nindependent\nobservations\nf\nz\n(1)\n;:::;z\n(\nn\n)\ng\n,themaximumlikelihoodestimatesaregivenby\n\u02da\nj\n=\nP\nn\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\nn\n:\nAswesawpreviously,ifweweretousethesemaximumlikelihoodestimates,\nthensomeofthe\n\u02da\nj\n'smightendupaszero,whichwasaproblem.Toavoid\nthis,wecanuse\nLaplacesmoothing\n,whichreplacestheaboveestimate\nwith\n\u02da\nj\n=\n1+\nP\nn\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\nk\n+\nn\n:\nHere,we'veadded1tothenumerator,and\nk\ntothedenominator.Notethat\nP\nk\nj\n=1\n\u02da\nj\n=1stillholds(checkthisyourself!),whichisadesirableproperty\nsincethe\n\u02da\nj\n'sareestimatesforprobabilitiesthatweknowmustsumto1.\nAlso,\n\u02da\nj\n6\n=0forallvaluesof\nj\n,solvingourproblemofprobabilitiesbeing\nestimatedaszero.Undercertain(arguablyquitestrong)conditions,itcan\nbeshownthattheLaplacesmoothingactuallygivestheoptimalestimator\nofthe\n\u02da\nj\n's.\nReturningtoourNaiveBayeswithLaplacesmoothing,we\nthereforeobtainthefollowingestimatesoftheparameters:\n\u02da\nj\nj\ny\n=1\n=\n1+\nP\nn\ni\n=1\n1\nf\nx\n(\ni\n)\nj\n=1\n^\ny\n(\ni\n)\n=1\ng\n2+\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\n\u02da\nj\nj\ny\n=0\n=\n1+\nP\nn\ni\n=1\n1\nf\nx\n(\ni\n)\nj\n=1\n^\ny\n(\ni\n)\n=0\ng\n2+\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\n"}, {"page_number": 47, "text": "47\n(Inpractice,itusuallydoesn'tmattermuchwhetherweapplyLaplacesmooth-\ningto\n\u02da\ny\nornot,sincewewilltypicallyhaveafairfractioneachofspamand\nnon-spammessages,so\n\u02da\ny\nwillbeareasonableestimateof\np\n(\ny\n=1)andwill\nbequitefarfrom0anyway.)\n4.2.2Eventmodelsfortext\nTocloseourdiscussionofgenerativelearningalgorithms,let'stalkabout\nonemoremodelthatisspfortextWhileNaiveBayes\naswe'vepresenteditwillworkwellformanyproblems,fortext\nthereisarelatedmodelthatdoesevenbetter.\nInthespcontextoftextNaiveBayesaspresenteduses\nthewhat'scalledthe\nBernoullieventmodel\n(orsometimes\nmulti-variate\nBernoullieventmodel\n).Inthismodel,weassumedthatthewayanemail\nisgeneratedisthatitisrandomlydetermined(accordingtotheclass\npriors\np\n(\ny\n))whetheraspammerornon-spammerwillsendyouyournext\nmessage.Then,thepersonsendingtheemailrunsthroughthedictionary,\ndecidingwhethertoincludeeachword\nj\ninthatemailindependentlyand\naccordingtotheprobabilities\np\n(\nx\nj\n=1\nj\ny\n)=\n\u02da\nj\nj\ny\n.Thus,theprobabilityofa\nmessagewasgivenby\np\n(\ny\n)\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n).\nHere'satmodel,calledthe\nMultinomialeventmodel\n.To\ndescribethismodel,wewilluseatnotationandsetoffeaturesfor\nrepresentingemails.Welet\nx\nj\ndenotetheidentityofthe\nj\n-thwordinthe\nemail.Thus,\nx\nj\nisnowanintegertakingvaluesin\nf\n1\n;:::;\nj\nV\njg\n,where\nj\nV\nj\nisthesizeofourvocabulary(dictionary).Anemailof\nd\nwordsisnowrep-\nresentedbyavector(\nx\n1\n;x\n2\n;:::;x\nd\n)oflength\nd\n;notethat\nd\ncanvaryfor\ntdocuments.Forinstance,ifanemailstartswith\\ANeurIPS...,\"\nthen\nx\n1\n=1(\\a\"isthewordinthedictionary),and\nx\n2\n=35000(if\n\\neurips\"isthe35000thwordinthedictionary).\nInthemultinomialeventmodel,weassumethatthewayanemailis\ngeneratedisviaarandomprocessinwhichspam/non-spamisdeter-\nmined(accordingto\np\n(\ny\n))asbefore.Then,thesenderoftheemailwritesthe\nemailbygenerating\nx\n1\nfromsomemultinomialdistributionoverwords\n(\np\n(\nx\n1\nj\ny\n)).Next,thesecondword\nx\n2\nischosenindependentlyof\nx\n1\nbutfrom\nthesamemultinomialdistribution,andsimilarlyfor\nx\n3\n,\nx\n4\n,andsoon,until\nall\nd\nwordsoftheemailhavebeengenerated.Thus,theoverallprobabilityof\namessageisgivenby\np\n(\ny\n)\nQ\nd\nj\n=1\np\n(\nx\nj\nj\ny\n).Notethatthisformulalookslikethe\nonewehadearlierfortheprobabilityofamessageundertheBernoullievent\nmodel,butthatthetermsintheformulanowmeanverytthings.In\nparticular\nx\nj\nj\ny\nisnowamultinomial,ratherthanaBernoullidistribution.\n"}, {"page_number": 48, "text": "48\nTheparametersforournewmodelare\n\u02da\ny\n=\np\n(\ny\n)asbefore,\n\u02da\nk\nj\ny\n=1\n=\np\n(\nx\nj\n=\nk\nj\ny\n=1)(forany\nj\n)and\n\u02da\nk\nj\ny\n=0\n=\np\n(\nx\nj\n=\nk\nj\ny\n=0).Notethatwehave\nassumedthat\np\n(\nx\nj\nj\ny\n)isthesameforallvaluesof\nj\n(i.e.,thatthedistribution\naccordingtowhichawordisgenerateddoesnotdependonitsposition\nj\nwithintheemail).\nIfwearegivenatrainingset\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=1\n;:::;n\ng\nwhere\nx\n(\ni\n)\n=\n(\nx\n(\ni\n)\n1\n;x\n(\ni\n)\n2\n;:::;x\n(\ni\n)\nd\ni\n)(here,\nd\ni\nisthenumberofwordsinthe\ni\n-trainingexample),\nthelikelihoodofthedataisgivenby\nL\n(\n\u02da\ny\n;\u02da\nk\nj\ny\n=0\n;\u02da\nk\nj\ny\n=1\n)=\nn\nY\ni\n=1\np\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n)\n=\nn\nY\ni\n=1\n \nd\ni\nY\nj\n=1\np\n(\nx\n(\ni\n)\nj\nj\ny\n;\n\u02da\nk\nj\ny\n=0\n;\u02da\nk\nj\ny\n=1\n)\n!\np\n(\ny\n(\ni\n)\n;\n\u02da\ny\n)\n:\nMaximizingthisyieldsthemaximumlikelihoodestimatesoftheparameters:\n\u02da\nk\nj\ny\n=1\n=\nP\nn\ni\n=1\nP\nd\ni\nj\n=1\n1\nf\nx\n(\ni\n)\nj\n=\nk\n^\ny\n(\ni\n)\n=1\ng\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\nd\ni\n\u02da\nk\nj\ny\n=0\n=\nP\nn\ni\n=1\nP\nd\ni\nj\n=1\n1\nf\nx\n(\ni\n)\nj\n=\nk\n^\ny\n(\ni\n)\n=0\ng\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\nd\ni\n\u02da\ny\n=\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\nn\n:\nIfweweretoapplyLaplacesmoothing(whichisneededinpracticeforgood\nperformance)whenestimating\n\u02da\nk\nj\ny\n=0\nand\n\u02da\nk\nj\ny\n=1\n,weadd1tothenumerators\nand\nj\nV\nj\ntothedenominators,andobtain:\n\u02da\nk\nj\ny\n=1\n=\n1+\nP\nn\ni\n=1\nP\nd\ni\nj\n=1\n1\nf\nx\n(\ni\n)\nj\n=\nk\n^\ny\n(\ni\n)\n=1\ng\nj\nV\nj\n+\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=1\ng\nd\ni\n\u02da\nk\nj\ny\n=0\n=\n1+\nP\nn\ni\n=1\nP\nd\ni\nj\n=1\n1\nf\nx\n(\ni\n)\nj\n=\nk\n^\ny\n(\ni\n)\n=0\ng\nj\nV\nj\n+\nP\nn\ni\n=1\n1\nf\ny\n(\ni\n)\n=0\ng\nd\ni\n:\nWhilenotnecessarilytheverybestalgorithm,theNaiveBayes\noftenworkssurprisinglywell.Itisoftenalsoaverygoodthing\ntotry,\"givenitssimplicityandeaseofimplementation.\n"}, {"page_number": 49, "text": "Chapter5\nKernelmethods\n5.1Featuremaps\nRecallthatinourdiscussionaboutlinearregression,weconsideredtheprob-\nlemofpredictingthepriceofahouse(denotedby\ny\n)fromthelivingareaof\nthehouse(denotedby\nx\n),andwealinearfunctionof\nx\ntothetraining\ndata.Whatiftheprice\ny\ncanbemoreaccuratelyrepresentedasa\nnon-linear\nfunctionof\nx\n?Inthiscase,weneedamoreexpressivefamilyofmodelsthan\nlinearmodels.\nWestartbyconsideringcubicfunctions\ny\n=\n\n3\nx\n3\n+\n\n2\nx\n2\n+\n\n1\nx\n+\n\n0\n.\nItturnsoutthatwecanviewthecubicfunctionasalinearfunctionover\ntheatsetoffeaturevariablesbelow).Concretely,letthe\nfunction\n\u02da\n:\nR\n!\nR\n4\nbedas\n\u02da\n(\nx\n)=\n2\n6\n6\n4\n1\nx\nx\n2\nx\n3\n3\n7\n7\n5\n2\nR\n4\n:\n(5.1)\nLet\n\n2\nR\n4\nbethevectorcontaining\n\n0\n;\n1\n;\n2\n;\n3\nasentries.Thenwecan\nrewritethecubicfunctionin\nx\nas:\n\n3\nx\n3\n+\n\n2\nx\n2\n+\n\n1\nx\n+\n\n0\n=\n\nT\n\u02da\n(\nx\n)\nThus,acubicfunctionofthevariable\nx\ncanbeviewedasalinearfunction\noverthevariables\n\u02da\n(\nx\n).Todistinguishbetweenthesetwosetsofvariables,\ninthecontextofkernelmethods,wewillcallthe\\original\"inputvaluethe\ninput\nattributes\nofaproblem(inthiscase,\nx\n,thelivingarea).Whenthe\n49\n"}, {"page_number": 50, "text": "50\noriginalinputismappedtosomenewsetofquantities\n\u02da\n(\nx\n),wewillcallthose\nnewquantitiesthe\nfeatures\nvariables.(Unfortunately,tauthorsuse\nttermstodescribethesetwothingsintcontexts.)Wewill\ncall\n\u02da\na\nfeaturemap\n,whichmapstheattributestothefeatures.\n5.2LMS(leastmeansquares)withfeatures\nWewillderivethegradientdescentalgorithmforthemodel\n\nT\n\u02da\n(\nx\n).\nFirstrecallthatforordinaryleastsquareproblemwherewewereto\n\nT\nx\n,\nthebatchgradientdescentupdateis(seethelecturenoteforitsderiva-\ntion):\n\n:=\n\n+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n)\n\nx\n(\ni\n)\n:=\n\n+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\n\nT\nx\n(\ni\n)\n\nx\n(\ni\n)\n:\n(5.2)\nLet\n\u02da\n:\nR\nd\n!\nR\np\nbeafeaturemapthatmapsattribute\nx\n(in\nR\nd\n)tothe\nfeatures\n\u02da\n(\nx\n)in\nR\np\n.(Inthemotivatingexampleintheprevioussubsection,\nwehave\nd\n=1and\np\n=4.)Nowourgoalistothefunction\n\nT\n\u02da\n(\nx\n),with\n\nbeingavectorin\nR\np\ninsteadof\nR\nd\n.Wecanreplacealltheoccurrencesof\nx\n(\ni\n)\ninthealgorithmaboveby\n\u02da\n(\nx\n(\ni\n)\n)toobtainthenewupdate:\n\n:=\n\n+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\n\nT\n\u02da\n(\nx\n(\ni\n)\n)\n\n\u02da\n(\nx\n(\ni\n)\n)(5.3)\nSimilarly,thecorrespondingstochasticgradientdescentupdateruleis\n\n:=\n\n+\n\n\ny\n(\ni\n)\n\n\nT\n\u02da\n(\nx\n(\ni\n)\n)\n\n\u02da\n(\nx\n(\ni\n)\n)(5.4)\n5.3LMSwiththekerneltrick\nThegradientdescentupdate,orstochasticgradientupdateabovebecomes\ncomputationallyexpensivewhenthefeatures\n\u02da\n(\nx\n)ishigh-dimensional.For\nexample,considerthedirectextensionofthefeaturemapinequation(5.1)\ntohigh-dimensionalinput\nx\n:suppose\nx\n2\nR\nd\n,andlet\n\u02da\n(\nx\n)bethevectorthat\n"}, {"page_number": 51, "text": "51\ncontainsallthemonomialsof\nx\nwithdegree\n\n3\n\u02da\n(\nx\n)=\n2\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n4\n1\nx\n1\nx\n2\n.\n.\n.\nx\n2\n1\nx\n1\nx\n2\nx\n1\nx\n3\n.\n.\n.\nx\n2\nx\n1\n.\n.\n.\nx\n3\n1\nx\n2\n1\nx\n2\n.\n.\n.\n3\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n5\n:\n(5.5)\nThedimensionofthefeatures\n\u02da\n(\nx\n)isontheorderof\nd\n3\n.\n1\nThisisapro-\nhibitivelylongvectorforcomputationalpurpose|when\nd\n=1000,each\nupdaterequiresatleastcomputingandstoringa1000\n3\n=10\n9\ndimensional\nvector,whichis10\n6\ntimesslowerthantheupdateruleforforordinaryleast\nsquaresupdates(5.2).\nItmayappearatthatsuch\nd\n3\nruntimeperupdateandmemoryusage\nareinevitable,becausethevector\n\nitselfisofdimension\np\n\u02c7\nd\n3\n,andwemay\nneedtoupdateeveryentryof\n\nandstoreit.However,wewillintroducethe\nkerneltrickwithwhichwewillnotneedtostore\n\nexplicitly,andtheruntime\ncanbetlyimproved.\nForsimplicity,weassumetheinitializethevalue\n\n=0,andwefocus\nontheiterativeupdate(5.3).Themainobservationisthatatanytime,\n\ncanberepresentedasalinearcombinationofthevectors\n\u02da\n(\nx\n(1)\n)\n;:::;\u02da\n(\nx\n(\nn\n)\n).\nIndeed,wecanshowthisinductivelyasfollows.Atinitialization,\n\n=0=\nP\nn\ni\n=1\n0\n\n\u02da\n(\nx\n(\ni\n)\n).Assumeatsomepoint,\n\ncanberepresentedas\n\n=\nn\nX\ni\n=1\n\ni\n\u02da\n(\nx\n(\ni\n)\n)(5.6)\n1\nHere,forsimplicity,weincludeallthemonomialswithrepetitions(sothat,e.g.,\nx\n1\nx\n2\nx\n3\nand\nx\n2\nx\n3\nx\n1\nbothappearin\n\u02da\n(\nx\n)).Therefore,therearetotally1+\nd\n+\nd\n2\n+\nd\n3\nentriesin\n\u02da\n(\nx\n).\n"}, {"page_number": 52, "text": "52\nforsome\n\n1\n;:::;\nn\n2\nR\n.Thenweclaimthatinthenextround,\n\nisstilla\nlinearcombinationof\n\u02da\n(\nx\n(1)\n)\n;:::;\u02da\n(\nx\n(\nn\n)\n)because\n\n:=\n\n+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\n\nT\n\u02da\n(\nx\n(\ni\n)\n)\n\n\u02da\n(\nx\n(\ni\n)\n)\n=\nn\nX\ni\n=1\n\ni\n\u02da\n(\nx\n(\ni\n)\n)+\n\nn\nX\ni\n=1\n\ny\n(\ni\n)\n\n\nT\n\u02da\n(\nx\n(\ni\n)\n)\n\n\u02da\n(\nx\n(\ni\n)\n)\n=\nn\nX\ni\n=1\n(\n\ni\n+\n\n\ny\n(\ni\n)\n\n\nT\n\u02da\n(\nx\n(\ni\n)\n)\n\n)\n|\n{z\n}\nnew\n\ni\n\u02da\n(\nx\n(\ni\n)\n)(5.7)\nYoumayrealizethatourgeneralstrategyistoimplicitlyrepresentthe\np\n-\ndimensionalvector\n\nbyasetofcots\n\n1\n;:::;\nn\n.Towardsdoingthis,\nwederivetheupdateruleofthecots\n\n1\n;:::;\nn\n.Usingtheequation\nabove,weseethatthenew\n\ni\ndependsontheoldonevia\n\ni\n:=\n\ni\n+\n\n\ny\n(\ni\n)\n\n\nT\n\u02da\n(\nx\n(\ni\n)\n)\n\n(5.8)\nHerewestillhavetheold\n\nontheRHSoftheequation.Replacing\n\nby\n\n=\nP\nn\nj\n=1\n\nj\n\u02da\n(\nx\n(\nj\n)\n)gives\n8\ni\n2f\n1\n;:::;n\ng\n;\ni\n:=\n\ni\n+\n\n \ny\n(\ni\n)\n\nn\nX\nj\n=1\n\nj\n\u02da\n(\nx\n(\nj\n)\n)\nT\n\u02da\n(\nx\n(\ni\n)\n)\n!\nWeoftenrewrite\n\u02da\n(\nx\n(\nj\n)\n)\nT\n\u02da\n(\nx\n(\ni\n)\n)as\nh\n\u02da\n(\nx\n(\nj\n)\n)\n;\u02da\n(\nx\n(\ni\n)\n)\ni\ntoemphasizethatit'sthe\ninnerproductofthetwofeaturevectors.Viewing\n\ni\n'sasthenewrepresenta-\ntionof\n\n,wehavesuccessfullytranslatedthebatchgradientdescentalgorithm\nintoanalgorithmthatupdatesthevalueof\n\niteratively.Itmayappearthat\nateveryiteration,westillneedtocomputethevaluesof\nh\n\u02da\n(\nx\n(\nj\n)\n)\n;\u02da\n(\nx\n(\ni\n)\n)\ni\nfor\nallpairsof\ni;j\n,eachofwhichmaytakeroughly\nO\n(\np\n)operation.However,\ntwoimportantpropertiescometorescue:\n1.\nWecanpre-computethepairwiseinnerproducts\nh\n\u02da\n(\nx\n(\nj\n)\n)\n;\u02da\n(\nx\n(\ni\n)\n)\ni\nforall\npairsof\ni;j\nbeforetheloopstarts.\n2.\nForthefeaturemap\n\u02da\nin(5.5)(ormanyotherinterestingfea-\nturemaps),computing\nh\n\u02da\n(\nx\n(\nj\n)\n)\n;\u02da\n(\nx\n(\ni\n)\n)\ni\ncanbetanddoesnot\n"}, {"page_number": 53, "text": "53\nnecessarilyrequirecomputing\n\u02da\n(\nx\n(\ni\n)\n)explicitly.Thisisbecause:\nh\n\u02da\n(\nx\n)\n;\u02da\n(\nz\n)\ni\n=1+\nd\nX\ni\n=1\nx\ni\nz\ni\n+\nX\ni;j\n2f\n1\n;:::;d\ng\nx\ni\nx\nj\nz\ni\nz\nj\n+\nX\ni;j;k\n2f\n1\n;:::;d\ng\nx\ni\nx\nj\nx\nk\nz\ni\nz\nj\nz\nk\n=1+\nd\nX\ni\n=1\nx\ni\nz\ni\n+\n \nd\nX\ni\n=1\nx\ni\nz\ni\n!\n2\n+\n \nd\nX\ni\n=1\nx\ni\nz\ni\n!\n3\n=1+\nh\nx;z\ni\n+\nh\nx;z\ni\n2\n+\nh\nx;z\ni\n3\n(5.9)\nTherefore,tocompute\nh\n\u02da\n(\nx\n)\n;\u02da\n(\nz\n)\ni\n,wecancompute\nh\nx;z\ni\nwith\nO\n(\nd\n)timeandthentakeanotherconstantnumberofoperationstocom-\npute1+\nh\nx;z\ni\n+\nh\nx;z\ni\n2\n+\nh\nx;z\ni\n3\n.\nAsyouwillsee,theinnerproductsbetweenthefeatures\nh\n\u02da\n(\nx\n)\n;\u02da\n(\nz\n)\ni\nare\nessentialhere.Wethe\nKernel\ncorrespondingtothefeaturemap\n\u02da\nas\nafunctionthatmaps\nXX!\nR\nsatisfying:\n2\nK\n(\nx;z\n)\n,\nh\n\u02da\n(\nx\n)\n;\u02da\n(\nz\n)\ni\n(5.10)\nTowrapupthediscussion,wewritethedownthealgorithmas\nfollows:\n1.\nComputeallthevalues\nK\n(\nx\n(\ni\n)\n;x\n(\nj\n)\n)\n,\nh\n\u02da\n(\nx\n(\ni\n)\n)\n;\u02da\n(\nx\n(\nj\n)\n)\ni\nusingequa-\ntion(5.9)forall\ni;j\n2f\n1\n;:::;n\ng\n.Set\n\n:=0.\n2.\nLoop:\n8\ni\n2f\n1\n;:::;n\ng\n;\ni\n:=\n\ni\n+\n\n \ny\n(\ni\n)\n\nn\nX\nj\n=1\n\nj\nK\n(\nx\n(\ni\n)\n;x\n(\nj\n)\n)\n!\n(5.11)\nOrinvectornotation,letting\nK\nbethe\nn\n\nn\nmatrixwith\nK\nij\n=\nK\n(\nx\n(\ni\n)\n;x\n(\nj\n)\n),wehave\n\n:=\n\n+\n\n(\n~y\n\nK\n)\nWiththealgorithmabove,wecanupdatetherepresentation\n\nofthe\nvector\n\ntlywith\nO\n(\nn\n)timeperupdate.Finally,weneedtoshowthat\n2\nRecallthat\nX\nisthespaceoftheinput\nx\n.Inourrunningexample,\nX\n=\nR\nd\n"}, {"page_number": 54, "text": "54\ntheknowledgeoftherepresentation\n\ntocomputetheprediction\n\nT\n\u02da\n(\nx\n).Indeed,wehave\n\nT\n\u02da\n(\nx\n)=\nn\nX\ni\n=1\n\ni\n\u02da\n(\nx\n(\ni\n)\n)\nT\n\u02da\n(\nx\n)=\nn\nX\ni\n=1\n\ni\nK\n(\nx\n(\ni\n)\n;x\n)(5.12)\nYoumayrealizethatfundamentallyallweneedtoknowaboutthefeature\nmap\n\u02da\n(\n\n)isencapsulatedinthecorrespondingkernelfunction\nK\n(\n\n;\n\n).We\nwillexpandonthisinthenextsection.\n5.4Propertiesofkernels\nInthelastsubsection,westartedwithanexplicitlyfeaturemap\n\u02da\n,\nwhichinducesthekernelfunction\nK\n(\nx;z\n)\n,\nh\n\u02da\n(\nx\n)\n;\u02da\n(\nz\n)\ni\n.Thenwesawthat\nthekernelfunctionissointrinsicsothataslongasthekernelfunctionis\nthewholetrainingalgorithmcanbewrittenentirelyinthelanguage\nofthekernelwithoutreferringtothefeaturemap\n\u02da\n,socanthepredictionof\natestexample\nx\n(equation(5.12).)\nTherefore,itwouldbetemptedtootherkernelfunction\nK\n(\n\n;\n\n)and\nrunthealgorithm(5.11).Notethatthealgorithm(5.11)doesnotneedto\nexplicitlyaccessthefeaturemap\n\u02da\n,andthereforeweonlyneedtoensurethe\nexistenceofthefeaturemap\n\u02da\n,butdonotnecessarilyneedtobeableto\nexplicitlywrite\n\u02da\ndown.\nWhatkindsoffunctions\nK\n(\n\n;\n\n)cancorrespondtosomefeaturemap\n\u02da\n?In\notherwords,canwetellifthereissomefeaturemapping\n\u02da\nsothat\nK\n(\nx;z\n)=\n\u02da\n(\nx\n)\nT\n\u02da\n(\nz\n)forall\nx\n,\nz\n?\nIfwecananswerthisquestionbygivingaprecisecharacterizationofvalid\nkernelfunctions,thenwecancompletelychangetheinterfaceofselecting\nfeaturemaps\n\u02da\ntotheinterfaceofselectingkernelfunction\nK\n.Concretely,\nwecanpickafunction\nK\n,verifythatitthecharacterization(so\nthatthereexistsafeaturemap\n\u02da\nthat\nK\ncorrespondsto),andthenwecan\nrunupdaterule(5.11).Thebhereisthatwedon'thavetobeable\ntocompute\n\u02da\norwriteitdownanalytically,andweonlyneedtoknowits\nexistence.Wewillanswerthisquestionattheendofthissubsectionafter\nwegothroughseveralconcreteexamplesofkernels.\nSuppose\nx;z\n2\nR\nd\n,andlet'sconsiderthefunction\nK\n(\n\n;\n\n)as:\nK\n(\nx;z\n)=(\nx\nT\nz\n)\n2\n:\n"}, {"page_number": 55, "text": "55\nWecanalsowritethisas\nK\n(\nx;z\n)=\n \nd\nX\ni\n=1\nx\ni\nz\ni\n! \nd\nX\nj\n=1\nx\nj\nz\nj\n!\n=\nd\nX\ni\n=1\nd\nX\nj\n=1\nx\ni\nx\nj\nz\ni\nz\nj\n=\nd\nX\ni;j\n=1\n(\nx\ni\nx\nj\n)(\nz\ni\nz\nj\n)\nThus,weseethat\nK\n(\nx;z\n)=\nh\n\u02da\n(\nx\n)\n;\u02da\n(\nz\n)\ni\nisthekernelfunctionthatcorre-\nspondstothethefeaturemapping\n\u02da\ngiven(shownhereforthecaseof\nd\n=3)\nby\n\u02da\n(\nx\n)=\n2\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n4\nx\n1\nx\n1\nx\n1\nx\n2\nx\n1\nx\n3\nx\n2\nx\n1\nx\n2\nx\n2\nx\n2\nx\n3\nx\n3\nx\n1\nx\n3\nx\n2\nx\n3\nx\n3\n3\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n5\n:\nRevisitingthecomputationalperspectiveofkernel,notethatwhereas\ncalculatingthehigh-dimensional\n\u02da\n(\nx\n)requires\nO\n(\nd\n2\n)time,\nK\n(\nx;z\n)\ntakesonly\nO\n(\nd\n)time|linearinthedimensionoftheinputattributes.\nForanotherrelatedexample,alsoconsider\nK\n(\n\n;\n\n)by\nK\n(\nx;z\n)=(\nx\nT\nz\n+\nc\n)\n2\n=\nd\nX\ni;j\n=1\n(\nx\ni\nx\nj\n)(\nz\ni\nz\nj\n)+\nd\nX\ni\n=1\n(\np\n2\ncx\ni\n)(\np\n2\ncz\ni\n)+\nc\n2\n:\n(Checkthisyourself.)Thisfunction\nK\nisakernelfunctionthatcorresponds\n"}, {"page_number": 56, "text": "56\ntothefeaturemapping(againshownfor\nd\n=3)\n\u02da\n(\nx\n)=\n2\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n4\nx\n1\nx\n1\nx\n1\nx\n2\nx\n1\nx\n3\nx\n2\nx\n1\nx\n2\nx\n2\nx\n2\nx\n3\nx\n3\nx\n1\nx\n3\nx\n2\nx\n3\nx\n3\np\n2\ncx\n1\np\n2\ncx\n2\np\n2\ncx\n3\nc\n3\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n5\n;\nandtheparameter\nc\ncontrolstherelativeweightingbetweenthe\nx\ni\n\norder)andthe\nx\ni\nx\nj\n(secondorder)terms.\nMorebroadly,thekernel\nK\n(\nx;z\n)=(\nx\nT\nz\n+\nc\n)\nk\ncorrespondstoafeature\nmappingtoan\n\nd\n+\nk\nk\n\nfeaturespace,correspondingofallmonomialsofthe\nform\nx\ni\n1\nx\ni\n2\n:::x\ni\nk\nthatareuptoorder\nk\n.However,despiteworkinginthis\nO\n(\nd\nk\n)-dimensionalspace,computing\nK\n(\nx;z\n)stilltakesonly\nO\n(\nd\n)time,and\nhenceweneverneedtoexplicitlyrepresentfeaturevectorsinthisveryhigh\ndimensionalfeaturespace.\nKernelsassimilaritymetrics.\nNow,let'stalkaboutaslightlyt\nviewofkernels.Intuitively,(andtherearethingswrongwiththisintuition,\nbutnevermind),if\n\u02da\n(\nx\n)and\n\u02da\n(\nz\n)areclosetogether,thenwemightexpect\nK\n(\nx;z\n)=\n\u02da\n(\nx\n)\nT\n\u02da\n(\nz\n)tobelarge.Conversely,if\n\u02da\n(\nx\n)and\n\u02da\n(\nz\n)arefarapart|\nsaynearlyorthogonaltoeachother|then\nK\n(\nx;z\n)=\n\u02da\n(\nx\n)\nT\n\u02da\n(\nz\n)willbesmall.\nSo,wecanthinkof\nK\n(\nx;z\n)assomemeasurementofhowsimilarare\n\u02da\n(\nx\n)\nand\n\u02da\n(\nz\n),orofhowsimilarare\nx\nand\nz\n.\nGiventhisintuition,supposethatforsomelearningproblemthatyou're\nworkingon,you'vecomeupwithsomefunction\nK\n(\nx;z\n)thatyouthinkmight\nbeareasonablemeasureofhowsimilar\nx\nand\nz\nare.Forinstance,perhaps\nyouchose\nK\n(\nx;z\n)=exp\n\n\njj\nx\n\nz\njj\n2\n2\n\u02d9\n2\n\n:\nThisisareasonablemeasureof\nx\nand\nz\n'ssimilarity,andiscloseto1when\nx\nand\nz\nareclose,andnear0when\nx\nand\nz\narefarapart.Doesthereexist\n"}, {"page_number": 57, "text": "57\nafeaturemap\n\u02da\nsuchthatthekernel\nK\nabove\nK\n(\nx;z\n)=\n\u02da\n(\nx\n)\nT\n\u02da\n(\nz\n)?Inthisparticularexample,theanswerisyes.Thiskerneliscalled\nthe\nGaussiankernel\n,andcorrespondstoandimensionalfeature\nmapping\n\u02da\n.Wewillgiveaprecisecharacterizationaboutwhatproperties\nafunction\nK\nneedstosatisfysothatitcanbeavalidkernelfunctionthat\ncorrespondstosomefeaturemap\n\u02da\n.\nNecessaryconditionsforvalidkernels.\nSupposefornowthat\nK\nis\nindeedavalidkernelcorrespondingtosomefeaturemapping\n\u02da\n,andwewill\nseewhatpropertiesitNow,considersomesetof\nn\npoints\n(notnecessarilythetrainingset)\nf\nx\n(1)\n;:::;x\n(\nn\n)\ng\n,andletasquare,\nn\n-by-\nn\nmatrix\nK\nbesothatits(\ni;j\n)-entryisgivenby\nK\nij\n=\nK\n(\nx\n(\ni\n)\n;x\n(\nj\n)\n).\nThismatrixiscalledthe\nkernelmatrix\n.Notethatwe'veoverloadedthe\nnotationandused\nK\ntodenoteboththekernelfunction\nK\n(\nx;z\n)andthe\nkernelmatrix\nK\n,duetotheirobviouscloserelationship.\nNow,if\nK\nisavalidkernel,then\nK\nij\n=\nK\n(\nx\n(\ni\n)\n;x\n(\nj\n)\n)=\n\u02da\n(\nx\n(\ni\n)\n)\nT\n\u02da\n(\nx\n(\nj\n)\n)=\n\u02da\n(\nx\n(\nj\n)\n)\nT\n\u02da\n(\nx\n(\ni\n)\n)=\nK\n(\nx\n(\nj\n)\n;x\n(\ni\n)\n)=\nK\nji\n,andhence\nK\nmustbesymmetric.More-\nover,letting\n\u02da\nk\n(\nx\n)denotethe\nk\n-thcoordinateofthevector\n\u02da\n(\nx\n),wethat\nforanyvector\nz\n,wehave\nz\nT\nKz\n=\nX\ni\nX\nj\nz\ni\nK\nij\nz\nj\n=\nX\ni\nX\nj\nz\ni\n\u02da\n(\nx\n(\ni\n)\n)\nT\n\u02da\n(\nx\n(\nj\n)\n)\nz\nj\n=\nX\ni\nX\nj\nz\ni\nX\nk\n\u02da\nk\n(\nx\n(\ni\n)\n)\n\u02da\nk\n(\nx\n(\nj\n)\n)\nz\nj\n=\nX\nk\nX\ni\nX\nj\nz\ni\n\u02da\nk\n(\nx\n(\ni\n)\n)\n\u02da\nk\n(\nx\n(\nj\n)\n)\nz\nj\n=\nX\nk\n \nX\ni\nz\ni\n\u02da\nk\n(\nx\n(\ni\n)\n)\n!\n2\n\n0\n:\nThesecond-to-laststepusesthefactthat\nP\ni;j\na\ni\na\nj\n=(\nP\ni\na\ni\n)\n2\nfor\na\ni\n=\nz\ni\n\u02da\nk\n(\nx\n(\ni\n)\n).Since\nz\nwasarbitrary,thisshowsthat\nK\nispositive\n(\nK\n\n0).\nHence,we'veshownthatif\nK\nisavalidkernel(i.e.,ifitcorrespondsto\nsomefeaturemapping\n\u02da\n),thenthecorrespondingkernelmatrix\nK\n2\nR\nn\n\nn\nissymmetricpositive\n"}, {"page_number": 58, "text": "58\ntconditionsforvalidkernels.\nMoregenerally,thecondition\naboveturnsouttobenotonlyanecessary,butalsoat,condition\nfor\nK\ntobeavalidkernel(alsocalledaMercerkernel).Thefollowingresult\nisduetoMercer.\n3\nTheorem(Mercer).\nLet\nK\n:\nR\nd\n\nR\nd\n7!\nR\nbegiven.Thenfor\nK\ntobeavalid(Mercer)kernel,itisnecessaryandtthatforany\nf\nx\n(1)\n;:::;x\n(\nn\n)\ng\n,(\nn<\n1\n),thecorrespondingkernelmatrixissymmetricpos-\nitive\nGivenafunction\nK\n,apartfromtryingtoafeaturemapping\n\u02da\nthat\ncorrespondstoit,thistheoremthereforegivesanotherwayoftestingifitis\navalidkernel.You'llalsohaveachancetoplaywiththeseideasmorein\nproblemset2.\nInclass,wealsotalkedaboutacoupleofotherexamplesofker-\nnels.Forinstance,considerthedigitrecognitionproblem,inwhichgiven\nanimage(16x16pixels)ofahandwrittendigit(0-9),wehavetoout\nwhichdigititwas.Usingeitherasimplepolynomialkernel\nK\n(\nx;z\n)=(\nx\nT\nz\n)\nk\northeGaussiankernel,SVMswereabletoobtainextremelygoodperfor-\nmanceonthisproblem.Thiswasparticularlysurprisingsincetheinput\nattributes\nx\nwerejust256-dimensionalvectorsoftheimagepixelintensity\nvalues,andthesystemhadnopriorknowledgeaboutvision,orevenabout\nwhichpixelsareadjacenttowhichotherones.Anotherexamplethatwe\ntalkedaboutinlecturewasthatiftheobjects\nx\nthatwearetrying\ntoclassifyarestrings(say,\nx\nisalistofaminoacids,whichstrungtogether\nformaprotein),thenitseemshardtoconstructareasonable,\\small\"setof\nfeaturesformostlearningalgorithms,especiallyiftstringshavedif-\nferentlengths.However,considerletting\n\u02da\n(\nx\n)beafeaturevectorthatcounts\nthenumberofoccurrencesofeachlength-\nk\nsubstringin\nx\n.Ifwe'reconsid-\neringstringsofEnglishletters,thenthereare26\nk\nsuchstrings.Hence,\n\u02da\n(\nx\n)\nisa26\nk\ndimensionalvector;evenformoderatevaluesof\nk\n,thisisprobably\ntoobigforustotlyworkwith.(e.g.,26\n4\n\u02c7\n460000.)However,using\n(dynamicprogramming-ish)stringmatchingalgorithms,itispossibletoef-\ntlycompute\nK\n(\nx;z\n)=\n\u02da\n(\nx\n)\nT\n\u02da\n(\nz\n),sothatwecannowimplicitlywork\ninthis26\nk\n-dimensionalfeaturespace,butwithouteverexplicitlycomputing\nfeaturevectorsinthisspace.\n3\nManytextspresentMercer'stheoreminaslightlymorecomplicatedforminvolving\nL\n2\nfunctions,butwhentheinputattributestakevaluesin\nR\nd\n,theversiongivenhereis\nequivalent.\n"}, {"page_number": 59, "text": "59\nApplicationofkernelmethods:\nWe'veseentheapplicationofkernels\ntolinearregression.Inthenextpart,wewillintroducethesupportvector\nmachinestowhichkernelscanbedirectlyapplied.dwelltoomuchlongeron\nithere.Infact,theideaofkernelshastlybroaderapplicabilitythan\nlinearregressionandSVMs.Sp,ifyouhaveanylearningalgorithm\nthatyoucanwriteintermsofonlyinnerproducts\nh\nx;z\ni\nbetweeninput\nattributevectors,thenbyreplacingthiswith\nK\n(\nx;z\n)where\nK\nisakernel,\nyoucan\\magically\"allowyouralgorithmtoworktlyinthehigh\ndimensionalfeaturespacecorrespondingto\nK\n.Forinstance,thiskerneltrick\ncanbeappliedwiththeperceptrontoderiveakernelperceptronalgorithm.\nManyofthealgorithmsthatwe'llseelaterinthisclasswillalsobeamenable\ntothismethod,whichhascometobeknownasthe\\kerneltrick.\"\n"}, {"page_number": 60, "text": "Chapter6\nSupportvectormachines\nThissetofnotespresentstheSupportVectorMachine(SVM)learningal-\ngorithm.SVMsareamongthebest(andmanybelieveareindeedthebest)\n\"supervisedlearningalgorithms.TotelltheSVMstory,we'll\nneedtotalkaboutmarginsandtheideaofseparatingdatawithalarge\n\\gap.\"Next,we'lltalkabouttheoptimalmarginwhichwilllead\nusintoadigressiononLagrangeduality.We'llalsoseekernels,whichgive\nawaytoapplySVMstlyinveryhighdimensional(suchas\ndimensional)featurespaces,and,we'llclosethestorywiththe\nSMOalgorithm,whichgivesantimplementationofSVMs.\n6.1Margins:intuition\nWe'llstartourstoryonSVMsbytalkingaboutmargins.Thissectionwill\ngivetheintuitionsaboutmarginsandabouttheofourpredic-\ntions;theseideaswillbemadeformalinSection6.3.\nConsiderlogisticregression,wheretheprobability\np\n(\ny\n=1\nj\nx\n;\n\n)ismod-\neledby\nh\n\n(\nx\n)=\ng\n(\n\nT\nx\n).Wethenpredict\\1\"onaninput\nx\nifandonlyif\nh\n\n(\nx\n)\n\n0\n:\n5,orequivalently,ifandonlyif\n\nT\nx\n\n0.Considerapositive\ntrainingexample(\ny\n=1).Thelarger\n\nT\nx\nis,thelargeralsois\nh\n\n(\nx\n)=\np\n(\ny\n=\n1\nj\nx\n;\n\n),andthusalsothehigherourdegreeofthatthelabelis1.\nThus,informallywecanthinkofourpredictionasbeingverytthat\ny\n=1if\n\nT\nx\n\u02db\n0.Similarly,wethinkoflogisticregressionastly\npredicting\ny\n=0,if\n\nT\nx\n\u02dd\n0.Givenatrainingset,againinformallyitseems\nthatwe'dhavefoundagoodtothetrainingdataifwecan\n\nsothat\n\nT\nx\n(\ni\n)\n\u02db\n0whenever\ny\n(\ni\n)\n=1,and\n\nT\nx\n(\ni\n)\n\u02dd\n0whenever\ny\n(\ni\n)\n=0,sincethis\nwouldaveryt(andcorrect)setofforallthe\n60\n"}, {"page_number": 61, "text": "61\ntrainingexamples.Thisseemstobeanicegoaltoaimfor,andwe'llsoon\nformalizethisideausingthenotionoffunctionalmargins.\nForarenttypeofintuition,considerthefollowingure,inwhichx's\nrepresentpositivetrainingexamples,o'sdenotenegativetrainingexamples,\nadecisionboundary(thisisthelinegivenbytheequation\n\nT\nx\n=0,and\nisalsocalledthe\nseparatinghyperplane\n)isalsoshown,andthreepoints\nhavealsobeenlabeledA,BandC.\nNoticethatthepointAisveryfarfromthedecisionboundary.Ifweare\naskedtomakeapredictionforthevalueof\ny\natA,itseemsweshouldbe\nquitetthat\ny\n=1there.Conversely,thepointCisverycloseto\nthedecisionboundary,andwhileit'sonthesideofthedecisionboundary\nonwhichwewouldpredict\ny\n=1,itseemslikelythatjustasmallchangeto\nthedecisionboundarycouldeasilyhavecausedoutpredictiontobe\ny\n=0.\nHence,we'remuchmoretaboutourpredictionatAthanatC.The\npointBliesin-betweenthesetwocases,andmorebroadly,weseethatif\napointisfarfromtheseparatinghyperplane,thenwemaybetly\nmoretinourpredictions.Again,informallywethinkitwouldbe\nniceif,givenatrainingset,wemanagetoadecisionboundarythat\nallowsustomakeallcorrectandt(meaningfarfromthedecision\nboundary)predictionsonthetrainingexamples.We'llformalizethislater\nusingthenotionofgeometricmargins.\n"}, {"page_number": 62, "text": "62\n6.2Notation(optionreading)\nTomakeourdiscussionofSVMseasier,we'llneedtointroduceanew\nnotationfortalkingaboutWewillbeconsideringalinear\nforabinaryproblemwithlabels\ny\nandfeatures\nx\n.\nFromnow,we'lluse\ny\n2\n1\n;\n1\ng\n(insteadof\nf\n0\n;\n1\ng\n)todenotetheclasslabels.\nAlso,ratherthanparameterizingourlinearwiththevector\n\n,we\nwilluseparameters\nw;b\n,andwriteouras\nh\nw;b\n(\nx\n)=\ng\n(\nw\nT\nx\n+\nb\n)\n:\nHere,\ng\n(\nz\n)=1if\nz\n\n0,and\ng\n(\nz\n)=\n\n1otherwise.This\\\nw;b\n\"notation\nallowsustoexplicitlytreattheinterceptterm\nb\nseparatelyfromtheother\nparameters.(Wealsodroptheconventionwehadpreviouslyofletting\nx\n0\n=1\nbeanextracoordinateintheinputfeaturevector.)Thus,\nb\ntakestheroleof\nwhatwaspreviously\n\n0\n,and\nw\ntakestheroleof[\n\n1\n:::\nd\n]\nT\n.\nNotealsothat,fromourof\ng\nabove,ourwilldirectly\npredicteither1or\n\n1(cf.theperceptronalgorithm),withoutgoing\nthroughtheintermediatestepofestimating\np\n(\ny\n=1)(whichiswhatlogistic\nregressiondoes).\n6.3Functionalandgeometricmargins(op-\ntionreading)\nLet'sformalizethenotionsofthefunctionalandgeometricmargins.Givena\ntrainingexample(\nx\n(\ni\n)\n;y\n(\ni\n)\n),wethe\nfunctionalmargin\nof(\nw;b\n)with\nrespecttothetrainingexampleas\n^\n\n(\ni\n)\n=\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n:\nNotethatif\ny\n(\ni\n)\n=1,thenforthefunctionalmargintobelarge(i.e.,for\nourpredictiontobetandcorrect),weneed\nw\nT\nx\n(\ni\n)\n+\nb\ntobealarge\npositivenumber.Conversely,if\ny\n(\ni\n)\n=\n\n1,thenforthefunctionalmargin\ntobelarge,weneed\nw\nT\nx\n(\ni\n)\n+\nb\ntobealargenegativenumber.Moreover,if\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n>\n0,thenourpredictiononthisexampleiscorrect.(Check\nthisyourself.)Hence,alargefunctionalmarginrepresentsatanda\ncorrectprediction.\nForalinearwiththechoiceof\ng\ngivenabove(takingvaluesin\n\n1\n;\n1\ng\n),there'sonepropertyofthefunctionalmarginthatmakesitnota\nverygoodmeasureofhowever.Givenourchoiceof\ng\n,wenotethat\n"}, {"page_number": 63, "text": "63\nifwereplace\nw\nwith2\nw\nand\nb\nwith2\nb\n,thensince\ng\n(\nw\nT\nx\n+\nb\n)=\ng\n(2\nw\nT\nx\n+2\nb\n),\nthiswouldnotchange\nh\nw;b\n(\nx\n)atall.I.e.,\ng\n,andhencealso\nh\nw;b\n(\nx\n),depends\nonlyonthesign,butnotonthemagnitude,of\nw\nT\nx\n+\nb\n.However,replacing\n(\nw;b\n)with(2\nw;\n2\nb\n)alsoresultsinmultiplyingourfunctionalmarginbya\nfactorof2.Thus,itseemsthatbyexploitingourfreedomtoscale\nw\nand\nb\n,\nwecanmakethefunctionalmarginarbitrarilylargewithoutreallychanging\nanythingmeaningful.Intuitively,itmightthereforemakesensetoimpose\nsomesortofnormalizationconditionsuchasthat\njj\nw\njj\n2\n=1;i.e.,wemight\nreplace(\nw;b\n)with(\nw=\njj\nw\njj\n2\n;b=\njj\nw\njj\n2\n),andinsteadconsiderthefunctional\nmarginof(\nw=\njj\nw\njj\n2\n;b=\njj\nw\njj\n2\n).We'llcomebacktothislater.\nGivenatrainingset\nS\n=\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=1\n;:::;n\ng\n,wealsothe\nfunctionmarginof(\nw;b\n)withrespectto\nS\nasthesmallestofthefunctional\nmarginsoftheindividualtrainingexamples.Denotedby^\n\n,thiscantherefore\nbewritten:\n^\n\n=min\ni\n=1\n;:::;n\n^\n\n(\ni\n)\n:\nNext,let'stalkabout\ngeometricmargins\n.Considerthepicturebelow:\nThedecisionboundarycorrespondingto(\nw;b\n)isshown,alongwiththe\nvector\nw\n.Notethat\nw\nisorthogonal(at90\n\n)totheseparatinghyperplane.\n(Youshouldconvinceyourselfthatthismustbethecase.)Considerthe\npointatA,whichrepresentstheinput\nx\n(\ni\n)\nofsometrainingexamplewith\nlabel\ny\n(\ni\n)\n=1.Itsdistancetothedecisionboundary,\n\n(\ni\n)\n,isgivenbytheline\nsegmentAB.\nHowcanwethevalueof\n\n(\ni\n)\n?Well,\nw=\njj\nw\njj\nisaunit-lengthvector\npointinginthesamedirectionas\nw\n.Since\nA\nrepresents\nx\n(\ni\n)\n,wetherefore\n"}, {"page_number": 64, "text": "64\nthatthepoint\nB\nisgivenby\nx\n(\ni\n)\n\n\n(\ni\n)\n\nw=\njj\nw\njj\n.Butthispointlieson\nthedecisionboundary,andallpoints\nx\nonthedecisionboundarysatisfythe\nequation\nw\nT\nx\n+\nb\n=0.Hence,\nw\nT\n\nx\n(\ni\n)\n\n\n(\ni\n)\nw\njj\nw\njj\n\n+\nb\n=0\n:\nSolvingfor\n\n(\ni\n)\nyields\n\n(\ni\n)\n=\nw\nT\nx\n(\ni\n)\n+\nb\njj\nw\njj\n=\n\nw\njj\nw\njj\n\nT\nx\n(\ni\n)\n+\nb\njj\nw\njj\n:\nThiswasworkedoutforthecaseofapositivetrainingexampleatAinthe\nwherebeingonthe\\positive\"sideofthedecisionboundaryisgood.\nMoregenerally,wethegeometricmarginof(\nw;b\n)withrespecttoa\ntrainingexample(\nx\n(\ni\n)\n;y\n(\ni\n)\n)tobe\n\n(\ni\n)\n=\ny\n(\ni\n)\n \n\nw\njj\nw\njj\n\nT\nx\n(\ni\n)\n+\nb\njj\nw\njj\n!\n:\nNotethatif\njj\nw\njj\n=1,thenthefunctionalmarginequalsthegeometric\nmargin|thisthusgivesusawayofrelatingthesetwoerentnotionsof\nmargin.Also,thegeometricmarginisinvarianttorescalingoftheparame-\nters;i.e.,ifwereplace\nw\nwith2\nw\nand\nb\nwith2\nb\n,thenthegeometricmargin\ndoesnotchange.Thiswillinfactcomeinhandylater.Sp,because\nofthisinvariancetothescalingoftheparameters,whentryingto\nw\nand\nb\ntotrainingdata,wecanimposeanarbitraryscalingconstrainton\nw\nwithout\nchanginganythingimportant;forinstance,wecandemandthat\njj\nw\njj\n=1,or\nj\nw\n1\nj\n=5,or\nj\nw\n1\n+\nb\nj\n+\nj\nw\n2\nj\n=2,andanyofthesecanbesimplyby\nrescaling\nw\nand\nb\n.\nFinally,givenatrainingset\nS\n=\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=1\n;:::;n\ng\n,wealso\nthegeometricmarginof(\nw;b\n)withrespectto\nS\ntobethesmallestofthe\ngeometricmarginsontheindividualtrainingexamples:\n\n=min\ni\n=1\n;:::;n\n\n(\ni\n)\n:\n6.4Theoptimalmargin(optionread-\ning)\nGivenatrainingset,itseemsfromourpreviousdiscussionthatanatural\ndesideratumistotrytoadecisionboundarythatmaximizesthe(ge-\nometric)margin,sincethiswouldaverytsetofpredictions\n"}, {"page_number": 65, "text": "65\nonthetrainingsetandagoodtothetrainingdata.Sp,this\nwillresultinarthatseparatesthepositiveandthenegativetraining\nexampleswitha\\gap\"(geometricmargin).\nFornow,wewillassumethatwearegivenatrainingsetthatislinearly\nseparable;i.e.,thatitispossibletoseparatethepositiveandnegativeex-\namplesusingsomeseparatinghyperplane.Howwillwetheonethat\nachievesthemaximumgeometricmargin?Wecanposethefollowingopti-\nmizationproblem:\nmax\n;w;b\n\ns.t.\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n;i\n=1\n;:::;n\njj\nw\njj\n=1\n:\nI.e.,wewanttomaximize\n\n,subjecttoeachtrainingexamplehavingfunc-\ntionalmarginatleast\n\n.The\njj\nw\njj\n=1constraintmoreoverensuresthatthe\nfunctionalmarginequalstothegeometricmargin,sowearealsoguaranteed\nthatallthegeometricmarginsareatleast\n\n.Thus,solvingthisproblemwill\nresultin(\nw;b\n)withthelargestpossiblegeometricmarginwithrespecttothe\ntrainingset.\nIfwecouldsolvetheoptimizationproblemabove,we'dbedone.Butthe\n\\\njj\nw\njj\n=1\"constraintisanasty(non-convex)one,andthisproblemcertainly\nisn'tinanyformatthatwecanplugintostandardoptimizationsoftwareto\nsolve.So,let'strytransformingtheproblemintoanicerone.Consider:\nmax\n^\n;w;b\n^\n\njj\nw\njj\ns.t.\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n^\n;i\n=1\n;:::;n\nHere,we'regoingtomaximize^\n=\njj\nw\njj\n,subjecttothefunctionalmarginsall\nbeingatleast^\n\n.Sincethegeometricandfunctionalmarginsarerelatedby\n\n=^\n=\njj\nw\nj\n,thiswillgiveustheanswerwewant.Moreover,we'vegottenrid\noftheconstraint\njj\nw\njj\n=1thatwedidn'tlike.Thedownsideisthatwenow\nhaveanasty(again,non-convex)objective\n^\n\njj\nw\njj\nfunction;and,westilldon't\nhaveanysoftwarethatcansolvethisformofanoptimization\nproblem.\nLet'skeepgoing.Recallourearlierdiscussionthatwecanaddanarbi-\ntraryscalingconstrainton\nw\nand\nb\nwithoutchanginganything.Thisisthe\nkeyideawe'llusenow.Wewillintroducethescalingconstraintthatthe\nfunctionalmarginof\nw;b\nwithrespecttothetrainingsetmustbe1:\n^\n\n=1\n:\n"}, {"page_number": 66, "text": "66\nSincemultiplying\nw\nand\nb\nbysomeconstantresultsinthefunctionalmargin\nbeingmultipliedbythatsameconstant,thisisindeedascalingconstraint,\nandcanbebyrescaling\nw;b\n.Pluggingthisintoourproblemabove,\nandnotingthatmaximizing^\n=\njj\nw\njj\n=1\n=\njj\nw\njj\nisthesamethingasminimizing\njj\nw\njj\n2\n,wenowhavethefollowingoptimizationproblem:\nmin\nw;b\n1\n2\njj\nw\njj\n2\ns.t.\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n1\n;i\n=1\n;:::;n\nWe'venowtransformedtheproblemintoaformthatcanbetly\nsolved.Theaboveisanoptimizationproblemwithaconvexquadraticob-\njectiveandonlylinearconstraints.Itssolutiongivesusthe\noptimalmar-\ngin\n.Thisoptimizationproblemcanbesolvedusingcommercial\nquadraticprogramming(QP)code.\n1\nWhilewecouldcalltheproblemsolvedhere,whatwewillinsteaddois\nmakeadigressiontotalkaboutLagrangeduality.Thiswillleadustoour\noptimizationproblem'sdualform,whichwillplayakeyroleinallowingusto\nusekernelstogetoptimalmargintoworktlyinveryhigh\ndimensionalspaces.Thedualformwillalsoallowustoderiveant\nalgorithmforsolvingtheaboveoptimizationproblemthatwilltypicallydo\nmuchbetterthangenericQPsoftware.\n6.5Lagrangeduality(optionalreading)\nLet'stemporarilyputasideSVMsandmaximummarginandtalk\naboutsolvingconstrainedoptimizationproblems.\nConsideraproblemofthefollowingform:\nmin\nw\nf\n(\nw\n)\ns.t.\nh\ni\n(\nw\n)=0\n;i\n=1\n;:::;l:\nSomeofyoumayrecallhowthemethodofLagrangemultiplierscanbeused\ntosolveit.(Don'tworryifyouhaven'tseenitbefore.)Inthismethod,we\nthe\nLagrangian\ntobe\nL\n(\nw;\n)=\nf\n(\nw\n)+\nl\nX\ni\n=1\n\ni\nh\ni\n(\nw\n)\n1\nYoumaybefamiliarwithlinearprogramming,whichsolvesoptimizationproblems\nthathavelinearobjectivesandlinearconstraints.QPsoftwareisalsowidelyavailable,\nwhichallowsconvexquadraticobjectivesandlinearconstraints.\n"}, {"page_number": 67, "text": "67\nHere,the\n\ni\n'sarecalledthe\nLagrangemultipliers\n.Wewouldthen\nandset\nL\n'spartialderivativestozero:\n@\nL\n@w\ni\n=0;\n@\nL\n@\ni\n=0\n;\nandsolvefor\nw\nand\n\n.\nInthissection,wewillgeneralizethistoconstrainedoptimizationprob-\nlemsinwhichwemayhaveinequalityaswellasequalityconstraints.Dueto\ntimeconstraints,wewon'treallybeabletodothetheoryofLagrangeduality\njusticeinthisclass,\n2\nbutwewillgivethemainideasandresults,whichwe\nwillthenapplytoouroptimalmarginoptimizationproblem.\nConsiderthefollowing,whichwe'llcallthe\nprimal\noptimizationproblem:\nmin\nw\nf\n(\nw\n)\ns.t.\ng\ni\n(\nw\n)\n\n0\n;i\n=1\n;:::;k\nh\ni\n(\nw\n)=0\n;i\n=1\n;:::;l:\nTosolveit,westartbythe\ngeneralizedLagrangian\nL\n(\nw;;\n)=\nf\n(\nw\n)+\nk\nX\ni\n=1\n\ni\ng\ni\n(\nw\n)+\nl\nX\ni\n=1\n\ni\nh\ni\n(\nw\n)\n:\nHere,the\n\ni\n'sand\n\ni\n'saretheLagrangemultipliers.Considerthequantity\n\nP\n(\nw\n)=max\n\n:\n\ni\n\n0\nL\n(\nw;;\n)\n:\nHere,the\\\nP\n\"subscriptstandsfor\\primal.\"Letsome\nw\nbegiven.If\nw\nviolatesanyoftheprimalconstraints(i.e.,ifeither\ng\ni\n(\nw\n)\n>\n0or\nh\ni\n(\nw\n)\n6\n=0\nforsome\ni\n),thenyoushouldbeabletoverifythat\n\nP\n(\nw\n)=max\n\n:\n\ni\n\n0\nf\n(\nw\n)+\nk\nX\ni\n=1\n\ni\ng\ni\n(\nw\n)+\nl\nX\ni\n=1\n\ni\nh\ni\n(\nw\n)\n(6.1)\n=\n1\n:\n(6.2)\nConversely,iftheconstraintsareindeedforaparticularvalueof\nw\n,\nthen\n\nP\n(\nw\n)=\nf\n(\nw\n).Hence,\n\nP\n(\nw\n)=\n\u02c6\nf\n(\nw\n)if\nw\nprimalconstraints\n1\notherwise\n:\n2\nReadersinterestedinlearningmoreaboutthistopicareencouragedtoread,e.g.,R.\nT.Rockarfeller(1970),ConvexAnalysis,PrincetonUniversityPress.\n"}, {"page_number": 68, "text": "68\nThus,\n\nP\ntakesthesamevalueastheobjectiveinourproblemforallval-\nuesof\nw\nthattheprimalconstraints,andispositiveyifthe\nconstraintsareviolated.Hence,ifweconsidertheminimizationproblem\nmin\nw\n\nP\n(\nw\n)=min\nw\nmax\n\n:\n\ni\n\n0\nL\n(\nw;;\n)\n;\nweseethatitisthesameproblem(i.e.,andhasthesamesolutionsas)our\noriginal,primalproblem.Forlateruse,wealsotheoptimalvalueof\ntheobjectivetobe\np\n\n=min\nw\n\nP\n(\nw\n);wecallthisthe\nvalue\noftheprimal\nproblem.\nNow,let'slookataslightlytproblem.Wee\n\nD\n(\n;\n)=min\nw\nL\n(\nw;;\n)\n:\nHere,the\\\nD\n\"subscriptstandsfor\\dual.\"Notealsothatwhereasinthe\nof\n\nP\nwewereoptimizing(maximizing)withrespectto\n;\n,here\nweareminimizingwithrespectto\nw\n.\nWecannowposethe\ndual\noptimizationproblem:\nmax\n\n:\n\ni\n\n0\n\nD\n(\n;\n)=max\n\n:\n\ni\n\n0\nmin\nw\nL\n(\nw;;\n)\n:\nThisisexactlythesameasourprimalproblemshownabove,exceptthatthe\norderofthe\\max\"andthe\\min\"arenowexchanged.Wealsothe\noptimalvalueofthedualproblem'sobjectivetobe\nd\n\n=max\n\n:\n\ni\n\n0\n\nD\n(\nw\n).\nHowaretheprimalandthedualproblemsrelated?Itcaneasilybeshown\nthat\nd\n\n=max\n\n:\n\ni\n\n0\nmin\nw\nL\n(\nw;;\n)\n\nmin\nw\nmax\n\n:\n\ni\n\n0\nL\n(\nw;;\n)=\np\n\n:\n(Youshouldconvinceyourselfofthis;thisfollowsfromthe\\maxmin\"ofa\nfunctionalwaysbeinglessthanorequaltothe\\minmax.\")However,under\ncertainconditions,wewillhave\nd\n\n=\np\n\n;\nsothatwecansolvethedualprobleminlieuoftheprimalproblem.Let's\nseewhattheseconditionsare.\nSuppose\nf\nandthe\ng\ni\n'sareconvex,\n3\nandthe\nh\ni\n'sare\n4\nSuppose\nfurtherthattheconstraints\ng\ni\nare(strictly)feasible;thismeansthatthere\nexistssome\nw\nsothat\ng\ni\n(\nw\n)\n<\n0forall\ni\n.\n3\nWhen\nf\nhasaHessian,thenitisconvexifandonlyiftheHessianispositivesemi-\nForinstance,\nf\n(\nw\n)=\nw\nT\nw\nisconvex;similarly,alllinear(andfunctions\narealsoconvex.(Afunction\nf\ncanalsobeconvexwithoutbeingtiable,butwe\nwon'tneedthosemoregeneralofconvexityhere.)\n4\nI.e.,thereexists\na\ni\n,\nb\ni\n,sothat\nh\ni\n(\nw\n)=\na\nT\ni\nw\n+\nb\ni\n.\\meansthesamethingas\nlinear,exceptthatwealsoallowtheextrainterceptterm\nb\ni\n.\n"}, {"page_number": 69, "text": "69\nUnderouraboveassumptions,theremustexist\nw\n\n;\n\n;\n\nsothat\nw\n\nisthe\nsolutiontotheprimalproblem,\n\n\n;\n\narethesolutiontothedualproblem,\nandmoreover\np\n\n=\nd\n\n=\nL\n(\nw\n\n;\n\n;\n\n).Moreover,\nw\n\n;\n\nand\n\n\nsatisfythe\nKarush-Kuhn-Tucker(KKT)conditions\n,whichareasfollows:\n@\n@w\ni\nL\n(\nw\n\n;\n\n;\n\n)=0\n;i\n=1\n;:::;d\n(6.3)\n@\n@\ni\nL\n(\nw\n\n;\n\n;\n\n)=0\n;i\n=1\n;:::;l\n(6.4)\n\n\ni\ng\ni\n(\nw\n\n)=0\n;i\n=1\n;:::;k\n(6.5)\ng\ni\n(\nw\n\n)\n\n0\n;i\n=1\n;:::;k\n(6.6)\n\n\n\n0\n;i\n=1\n;:::;k\n(6.7)\nMoreover,ifsome\nw\n\n;\n\n;\n\nsatisfytheKKTconditions,thenitisalsoasolutiontot\nheprimalanddual\nproblems.\nWedrawattentiontoEquation(6.5),whichiscalledtheKKT\ndual\ncomplementarity\ncondition.Sp,itimpliesthatif\n\n\ni\n>\n0,then\ng\ni\n(\nw\n\n)=0.(I.e.,the\\\ng\ni\n(\nw\n)\n\n0\"constraintis\nactive\n,meaningitholdswith\nequalityratherthanwithinequality.)Lateron,thiswillbekeyforshowing\nthattheSVMhasonlyasmallnumberof\\supportvectors\";theKKTdual\ncomplementarityconditionwillalsogiveusourconvergencetestwhenwe\ntalkabouttheSMOalgorithm.\n6.6Optimalmarginthedualform\n(optionreading)\nNote:\nTheequivalenceofoptimizationproblem\n(6.8)\nandtheoptimization\nproblem\n(6.12)\n,andtherelationshipbetweentheprimaryanddualvariables\ninequation\n(6.10)\narethemostimportanttakehomemessagesofthissection.\nPreviously,weposedthefollowing(primal)optimizationproblemfor\ningtheoptimalmargin\nmin\nw;b\n1\n2\njj\nw\njj\n2\n(6.8)\ns.t.\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n1\n;i\n=1\n;:::;n\nWecanwritetheconstraintsas\ng\ni\n(\nw\n)=\n\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)+1\n\n0\n:\n"}, {"page_number": 70, "text": "70\nWehaveonesuchconstraintforeachtrainingexample.Notethatfromthe\nKKTdualcomplementaritycondition,wewillhave\n\ni\n>\n0onlyforthetrain-\ningexamplesthathavefunctionalmarginexactlyequaltoone(i.e.,theones\ncorrespondingtoconstraintsthatholdwithequality,\ng\ni\n(\nw\n)=0).Consider\nthebelow,inwhichamaximummarginseparatinghyperplaneisshown\nbythesolidline.\nThepointswiththesmallestmarginsareexactlytheonesclosesttothe\ndecisionboundary;here,thesearethethreepoints(onenegativeandtwopos-\nitiveexamples)thatlieonthedashedlinesparalleltothedecisionboundary.\nThus,onlythreeofthe\n\ni\n's|namely,theonescorrespondingtothesethree\ntrainingexamples|willbenon-zeroattheoptimalsolutiontoouroptimiza-\ntionproblem.Thesethreepointsarecalledthe\nsupportvectors\ninthis\nproblem.Thefactthatthenumberofsupportvectorscanbemuchsmaller\nthanthesizethetrainingsetwillbeusefullater.\nLet'smoveon.Lookingahead,aswedevelopthedualformoftheprob-\nlem,onekeyideatowatchoutforisthatwe'lltrytowriteouralgorithm\nintermsofonlytheinnerproduct\nh\nx\n(\ni\n)\n;x\n(\nj\n)\ni\n(thinkofthisas(\nx\n(\ni\n)\n)\nT\nx\n(\nj\n)\n)\nbetweenpointsintheinputfeaturespace.Thefactthatwecanexpressour\nalgorithmintermsoftheseinnerproductswillbekeywhenweapplythe\nkerneltrick.\nWhenweconstructtheLagrangianforouroptimizationproblemwehave:\nL\n(\nw;b;\n)=\n1\n2\njj\nw\njj\n2\n\nn\nX\ni\n=1\n\ni\n\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n1\n\n:\n(6.9)\nNotethatthere'reonly\\\n\ni\n\"butno\\\n\ni\n\"Lagrangemultipliers,sincethe\nproblemhasonlyinequalityconstraints.\n"}, {"page_number": 71, "text": "71\nLet'sthedualformoftheproblem.Todoso,weneedto\nminimize\nL\n(\nw;b;\n)withrespectto\nw\nand\nb\n(for\n\n),toget\n\nD\n,which\nwe'lldobysettingthederivativesof\nL\nwithrespectto\nw\nand\nb\ntozero.We\nhave:\nr\nw\nL\n(\nw;b;\n)=\nw\n\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\nx\n(\ni\n)\n=0\nThisimpliesthat\nw\n=\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\nx\n(\ni\n)\n:\n(6.10)\nAsforthederivativewithrespectto\nb\n,weobtain\n@\n@b\nL\n(\nw;b;\n)=\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\n=0\n:\n(6.11)\nIfwetaketheof\nw\ninEquation(6.10)andplugthatbackinto\ntheLagrangian(Equation6.9),andsimplify,weget\nL\n(\nw;b;\n)=\nn\nX\ni\n=1\n\ni\n\n1\n2\nn\nX\ni;j\n=1\ny\n(\ni\n)\ny\n(\nj\n)\n\ni\n\nj\n(\nx\n(\ni\n)\n)\nT\nx\n(\nj\n)\n\nb\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\n:\nButfromEquation(6.11),thelasttermmustbezero,soweobtain\nL\n(\nw;b;\n)=\nn\nX\ni\n=1\n\ni\n\n1\n2\nn\nX\ni;j\n=1\ny\n(\ni\n)\ny\n(\nj\n)\n\ni\n\nj\n(\nx\n(\ni\n)\n)\nT\nx\n(\nj\n)\n:\nRecallthatwegottotheequationabovebyminimizing\nL\nwithrespectto\nw\nand\nb\n.Puttingthistogetherwiththeconstraints\n\ni\n\n0(thatwealways\nhad)andtheconstraint(6.11),weobtainthefollowingdualoptimization\nproblem:\nmax\n\nW\n(\n\n)=\nn\nX\ni\n=1\n\ni\n\n1\n2\nn\nX\ni;j\n=1\ny\n(\ni\n)\ny\n(\nj\n)\n\ni\n\nj\nh\nx\n(\ni\n)\n;x\n(\nj\n)\ni\n:\n(6.12)\ns.t.\n\ni\n\n0\n;i\n=1\n;:::;n\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\n=0\n;\nYoushouldalsobeabletoverifythattheconditionsrequiredfor\np\n\n=\nd\n\nandtheKKTconditions(Equations6.3{6.7)toholdareindeedin\n"}, {"page_number": 72, "text": "72\nouroptimizationproblem.Hence,wecansolvethedualinlieuofsolving\ntheprimalproblem.Sp,inthedualproblemabove,wehavea\nmaximizationprobleminwhichtheparametersarethe\n\ni\n's.We'lltalklater\naboutthespalgorithmthatwe'regoingtousetosolvethedualproblem,\nbutifweareindeedabletosolveit(i.e.,the\n\n'sthatmaximize\nW\n(\n\n)\nsubjecttotheconstraints),thenwecanuseEquation(6.10)togobackand\ntheoptimal\nw\n'sasafunctionofthe\n\n's.Havingfound\nw\n\n,byconsidering\ntheprimalproblem,itisalsostraightforwardtondtheoptimalvaluefor\ntheinterceptterm\nb\nas\nb\n\n=\n\nmax\ni\n:\ny\n(\ni\n)\n=\n\n1\nw\n\nT\nx\n(\ni\n)\n+min\ni\n:\ny\n(\ni\n)\n=1\nw\n\nT\nx\n(\ni\n)\n2\n:\n(6.13)\n(Checkforyourselfthatthisiscorrect.)\nBeforemovingon,let'salsotakeamorecarefullookatEquation(6.10),\nwhichgivestheoptimalvalueof\nw\nintermsof(theoptimalvalueof)\n\n.\nSupposewe'veourmodel'sparameterstoatrainingset,andnowwishto\nmakeapredictionatanewpointinput\nx\n.Wewouldthencalculate\nw\nT\nx\n+\nb\n,\nandpredict\ny\n=1ifandonlyifthisquantityisbiggerthanzero.But\nusing(6.10),thisquantitycanalsobewritten:\nw\nT\nx\n+\nb\n=\n \nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\nx\n(\ni\n)\n!\nT\nx\n+\nb\n(6.14)\n=\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\nh\nx\n(\ni\n)\n;x\ni\n+\nb:\n(6.15)\nHence,ifwe'vefoundthe\n\ni\n's,inordertomakeaprediction,wehaveto\ncalculateaquantitythatdependsonlyontheinnerproductbetween\nx\nand\nthepointsinthetrainingset.Moreover,wesawearlierthatthe\n\ni\n'swillall\nbezeroexceptforthesupportvectors.Thus,manyofthetermsinthesum\nabovewillbezero,andwereallyneedtoonlytheinnerproductsbetween\nx\nandthesupportvectors(ofwhichthereisoftenonlyasmallnumber)in\nordercalculate(6.15)andmakeourprediction.\nByexaminingthedualformoftheoptimizationproblem,wegainedsig-\ntinsightintothestructureoftheproblem,andwerealsoabletowrite\ntheentirealgorithmintermsofonlyinnerproductsbetweeninputfeature\nvectors.Inthenextsection,wewillexploitthispropertytoapplytheker-\nnelstoourproblem.Theresultingalgorithm,\nsupportvector\nmachines\n,willbeabletotlylearninveryhighdimensionalspaces.\n"}, {"page_number": 73, "text": "73\n6.7Regularizationandthenon-separablecase\n(optionalreading)\nThederivationoftheSVMaspresentedsofarassumedthatthedatais\nlinearlyseparable.Whilemappingdatatoahighdimensionalfeaturespace\nvia\n\u02da\ndoesgenerallyincreasethelikelihoodthatthedataisseparable,we\ncan'tguaranteethatitalwayswillbeso.Also,insomecasesitisnotclear\nthataseparatinghyperplaneisexactlywhatwe'dwanttodo,since\nthatmightbesusceptibletooutliers.Forinstance,theleftbelow\nshowsanoptimalmarginandwhenasingleoutlierisaddedinthe\nupper-leftregion(rightitcausesthedecisionboundarytomakea\ndramaticswing,andtheresultinghasamuchsmallermargin.\nTomakethealgorithmworkfornon-linearlyseparabledatasetsaswell\nasbelesssensitivetooutliers,wereformulateouroptimization(using\n`\n1\nregularization\n)asfollows:\nmin\n;w;b\n1\n2\njj\nw\njj\n2\n+\nC\nn\nX\ni\n=1\n\u02d8\ni\ns.t.\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n1\n\n\u02d8\ni\n;i\n=1\n;:::;n\n\u02d8\ni\n\n0\n;i\n=1\n;:::;n:\nThus,examplesarenowpermittedtohave(functional)marginlessthan1,\nandifanexamplehasfunctionalmargin1\n\n\u02d8\ni\n(with\n\u02d8>\n0),wewouldpay\nacostoftheobjectivefunctionbeingincreasedby\nC\u02d8\ni\n.Theparameter\nC\ncontrolstherelativeweightingbetweenthetwingoalsofmakingthe\njj\nw\njj\n2\nsmall(whichwesawearliermakesthemarginlarge)andofensuringthat\nmostexampleshavefunctionalmarginatleast1.\n"}, {"page_number": 74, "text": "74\nAsbefore,wecanformtheLagrangian:\nL\n(\nw;b;\u02d8;;r\n)=\n1\n2\nw\nT\nw\n+\nC\nn\nX\ni\n=1\n\u02d8\ni\n\nn\nX\ni\n=1\n\ni\n\ny\n(\ni\n)\n(\nx\nT\nw\n+\nb\n)\n\n1+\n\u02d8\ni\n\n\nn\nX\ni\n=1\nr\ni\n\u02d8\ni\n:\nHere,the\n\ni\n'sand\nr\ni\n'sareourLagrangemultipliers(constrainedtobe\n\n0).\nWewon'tgothroughthederivationofthedualagainindetail,butafter\nsettingthederivativeswithrespectto\nw\nand\nb\ntozeroasbefore,substituting\nthembackin,andsimplifying,weobtainthefollowingdualformofthe\nproblem:\nmax\n\nW\n(\n\n)=\nn\nX\ni\n=1\n\ni\n\n1\n2\nn\nX\ni;j\n=1\ny\n(\ni\n)\ny\n(\nj\n)\n\ni\n\nj\nh\nx\n(\ni\n)\n;x\n(\nj\n)\ni\ns.t.0\n\n\ni\n\nC;i\n=1\n;:::;n\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\n=0\n;\nAsbefore,wealsohavethat\nw\ncanbeexpressedintermsofthe\n\ni\n'sas\ngiveninEquation(6.10),sothataftersolvingthedualproblem,wecancon-\ntinuetouseEquation(6.15)tomakeourpredictions.Notethat,somewhat\nsurprisingly,inadding\n`\n1\nregularization,theonlychangetothedualprob-\nlemisthatwhatwasoriginallyaconstraintthat0\n\n\ni\nhasnowbecome\n0\n\n\ni\n\nC\n.Thecalculationfor\nb\n\nalsohastobemo(Equation6.13is\nnolongervalid);seethecommentsinthenextsection/Platt'spaper.\nAlso,theKKTdual-complementarityconditions(whichinthenextsec-\ntionwillbeusefulfortestingfortheconvergenceoftheSMOalgorithm)\nare:\n\ni\n=0\n)\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n1\n(6.16)\n\ni\n=\nC\n)\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)\n\n1\n(6.17)\n0\n<\ni\n<C\n)\ny\n(\ni\n)\n(\nw\nT\nx\n(\ni\n)\n+\nb\n)=1\n:\n(6.18)\nNow,allthatremainsistogiveanalgorithmforactuallysolvingthedual\nproblem,whichwewilldointhenextsection.\n6.8TheSMOalgorithm(optionalreading)\nTheSMO(sequentialminimaloptimization)algorithm,duetoJohnPlatt,\ngivesantwayofsolvingthedualproblemarisingfromthederivation\n"}, {"page_number": 75, "text": "75\noftheSVM.PartlytomotivatetheSMOalgorithm,andpartlybecauseit's\ninterestinginitsownright,let'stakeanotherdigressiontotalkabout\nthecoordinateascentalgorithm.\n6.8.1Coordinateascent\nConsidertryingtosolvetheunconstrainedoptimizationproblem\nmax\n\nW\n(\n\n1\n;\n2\n;:::;\nn\n)\n:\nHere,wethinkof\nW\nasjustsomefunctionoftheparameters\n\ni\n's,andfornow\nignoreanyrelationshipbetweenthisproblemandSVMs.We'vealreadyseen\ntwooptimizationalgorithms,gradientascentandNewton'smethod.The\nnewalgorithmwe'regoingtoconsiderhereiscalled\ncoordinateascent\n:\nLoopuntilconvergence:\nf\nFor\ni\n=1\n;:::;n\n,\nf\n\ni\n:=argmax\n^\n\ni\nW\n(\n\n1\n;:::;\ni\n\n1\n;\n^\n\ni\n;\ni\n+1\n;:::;\nn\n).\ng\ng\nThus,intheinnermostloopofthisalgorithm,wewillholdallthevariables\nexceptforsome\n\ni\nandreoptimize\nW\nwithrespecttojusttheparameter\n\ni\n.Intheversionofthismethodpresentedhere,theinner-loopreoptimizes\nthevariablesinorder\n\n1\n;\n2\n;:::;\nn\n;\n1\n;\n2\n;:::\n.(Amoresophisticatedversion\nmightchooseotherorderings;forinstance,wemaychoosethenextvariable\ntoupdateaccordingtowhichoneweexpecttoallowustomakethelargest\nincreasein\nW\n(\n\n).)\nWhenthefunction\nW\nhappenstobeofsuchaformthatthe\\argmax\"\nintheinnerloopcanbeperformedtly,thencoordinateascentcanbe\nafairlytalgorithm.Here'sapictureofcoordinateascentinaction:\n"}, {"page_number": 76, "text": "76\nTheellipsesinthearethecontoursofaquadraticfunctionthat\nwewanttooptimize.Coordinateascentwasinitializedat(2\n;\n\n2),andalso\nplottedintheisthepaththatittookonitswaytotheglobalmaximum.\nNoticethatoneachstep,coordinateascenttakesastepthat'sparalleltoone\noftheaxes,sinceonlyonevariableisbeingoptimizedatatime.\n6.8.2SMO\nWeclosethediscussionofSVMsbysketchingthederivationoftheSMO\nalgorithm.\nHere'sthe(dual)optimizationproblemthatwewanttosolve:\nmax\n\nW\n(\n\n)=\nn\nX\ni\n=1\n\ni\n\n1\n2\nn\nX\ni;j\n=1\ny\n(\ni\n)\ny\n(\nj\n)\n\ni\n\nj\nh\nx\n(\ni\n)\n;x\n(\nj\n)\ni\n:\n(6.19)\ns.t.0\n\n\ni\n\nC;i\n=1\n;:::;n\n(6.20)\nn\nX\ni\n=1\n\ni\ny\n(\ni\n)\n=0\n:\n(6.21)\nLet'ssaywehavesetof\n\ni\n'sthatsatisfytheconstraints(6.20-6.21).Now,\nsupposewewanttohold\n\n2\n;:::;\nn\nandtakeacoordinateascentstep\nandreoptimizetheobjectivewithrespectto\n\n1\n.Canwemakeanyprogress?\nTheanswerisno,becausetheconstraint(6.21)ensuresthat\n\n1\ny\n(1)\n=\n\nn\nX\ni\n=2\n\ni\ny\n(\ni\n)\n:\n"}, {"page_number": 77, "text": "77\nOr,bymultiplyingbothsidesby\ny\n(1)\n,weequivalentlyhave\n\n1\n=\n\ny\n(1)\nn\nX\ni\n=2\n\ni\ny\n(\ni\n)\n:\n(Thisstepusedthefactthat\ny\n(1)\n2\n1\n;\n1\ng\n,andhence(\ny\n(1)\n)\n2\n=1.)Hence,\n\n1\nisexactlydeterminedbytheother\n\ni\n's,andifweweretohold\n\n2\n;:::;\nn\nthenwecan'tmakeanychangeto\n\n1\nwithoutviolatingthecon-\nstraint(6.21)intheoptimizationproblem.\nThus,ifwewanttoupdatesomesubjectofthe\n\ni\n's,wemustupdateat\nleasttwoofthemsimultaneouslyinordertokeepsatisfyingtheconstraints.\nThismotivatestheSMOalgorithm,whichsimplydoesthefollowing:\nRepeattillconvergence\nf\n1.\nSelectsomepair\n\ni\nand\n\nj\ntoupdatenext(usingaheuristicthat\ntriestopickthetwothatwillallowustomakethebiggestprogress\ntowardstheglobalmaximum).\n2.\nReoptimize\nW\n(\n\n)withrespectto\n\ni\nand\n\nj\n,whileholdingallthe\nother\n\nk\n's(\nk\n6\n=\ni;j\n)\ng\nTotestforconvergenceofthisalgorithm,wecancheckwhethertheKKT\nconditions(Equations6.16-6.18)aretowithinsome\nt\nol\n.Here,\nt\nol\nis\ntheconvergencetoleranceparameter,andistypicallysettoaround0.01to\n0.001.(Seethepaperandpseudocodefordetails.)\nThekeyreasonthatSMOisantalgorithmisthattheupdateto\n\ni\n,\n\nj\ncanbecomputedverytly.Let'snowsketchthemain\nideasforderivingthetupdate.\nLet'ssaywecurrentlyhavesomesettingofthe\n\ni\n'sthatsatisfythecon-\nstraints(6.20-6.21),andsupposewe'vedecidedtohold\n\n3\n;:::;\nn\nand\nwanttoreoptimize\nW\n(\n\n1\n;\n2\n;:::;\nn\n)withrespectto\n\n1\nand\n\n2\n(subjectto\ntheconstraints).From(6.21),werequirethat\n\n1\ny\n(1)\n+\n\n2\ny\n(2)\n=\n\nn\nX\ni\n=3\n\ni\ny\n(\ni\n)\n:\nSincetherighthandsideis(aswe've\n\n3\n;:::\nn\n),wecanjustlet\nitbedenotedbysomeconstant\n\n:\n\n1\ny\n(1)\n+\n\n2\ny\n(2)\n=\n:\n(6.22)\nWecanthuspicturetheconstraintson\n\n1\nand\n\n2\nasfollows:\n"}, {"page_number": 78, "text": "78\nFromtheconstraints(6.20),weknowthat\n\n1\nand\n\n2\nmustliewithinthebox\n[0\n;C\n]\n\n[0\n;C\n]shown.Alsoplottedistheline\n\n1\ny\n(1)\n+\n\n2\ny\n(2)\n=\n\n,onwhichwe\nknow\n\n1\nand\n\n2\nmustlie.Notealsothat,fromtheseconstraints,weknow\nL\n\n\n2\n\nH\n;otherwise,(\n\n1\n;\n2\n)can'tsimultaneouslysatisfyboththebox\nandthestraightlineconstraint.Inthisexample,\nL\n=0.Butdependingon\nwhattheline\n\n1\ny\n(1)\n+\n\n2\ny\n(2)\n=\n\nlookslike,thiswon'talwaysnecessarilybe\nthecase;butmoregenerally,therewillbesomelower-bound\nL\nandsome\nupper-bound\nH\nonthepermissiblevaluesfor\n\n2\nthatwillensurethat\n\n1\n,\n\n2\nliewithinthebox[0\n;C\n]\n\n[0\n;C\n].\nUsingEquation(6.22),wecanalsowrite\n\n1\nasafunctionof\n\n2\n:\n\n1\n=(\n\n\n\n2\ny\n(2)\n)\ny\n(1)\n:\n(Checkthisderivationyourself;weagainusedthefactthat\ny\n(1)\n2\n1\n;\n1\ng\nso\nthat(\ny\n(1)\n)\n2\n=1.)Hence,theobjective\nW\n(\n\n)canbewritten\nW\n(\n\n1\n;\n2\n;:::;\nn\n)=\nW\n((\n\n\n\n2\ny\n(2)\n)\ny\n(1)\n;\n2\n;:::;\nn\n)\n:\nTreating\n\n3\n;:::;\nn\nasconstants,youshouldbeabletoverifythatthisis\njustsomequadraticfunctionin\n\n2\n.I.e.,thiscanalsobeexpressedinthe\nform\n\n2\n2\n+\n\n2\n+\nc\nforsomeappropriate\na\n,\nb\n,and\nc\n.Ifweignorethe\\box\"\nconstraints(6.20)(or,equivalently,that\nL\n\n\n2\n\nH\n),thenwecaneasily\nmaximizethisquadraticfunctionbysettingitsderivativetozeroandsolving.\nWe'lllet\n\nn\new;unclipped\n2\ndenotetheresultingvalueof\n\n2\n.Youshouldalsobe\nabletoconvinceyourselfthatifwehadinsteadwantedtomaximize\nW\nwith\nrespectto\n\n2\nbutsubjecttotheboxconstraint,thenwecantheresulting\nvalueoptimalsimplybytaking\n\nn\new;unclipped\n2\nand\\clipping\"ittolieinthe\n"}, {"page_number": 79, "text": "79\n[\nL;H\n]interval,toget\n\nn\new\n2\n=\n8\n<\n:\nH\nif\n\nn\new;unclipped\n2\n>H\n\nn\new;unclipped\n2\nif\nL\n\n\nn\new;unclipped\n2\n\nH\nL\nif\n\nn\new;unclipped\n2\n<L\nFinally,havingfoundthe\n\nn\new\n2\n,wecanuseEquation(6.22)togobackand\ntheoptimalvalueof\n\nn\new\n1\n.\nThere'reacouplemoredetailsthatarequiteeasybutthatwe'llleaveyou\ntoreadaboutyourselfinPlatt'spaper:Oneisthechoiceoftheheuristics\nusedtoselectthenext\n\ni\n,\n\nj\ntoupdate;theotherishowtoupdate\nb\nasthe\nSMOalgorithmisrun.\n"}, {"page_number": 80, "text": "PartII\nDeeplearning\n80\n"}, {"page_number": 81, "text": "Chapter7\nDeeplearning\nWenowbeginourstudyofdeeplearning.Inthissetofnotes,wegivean\noverviewofneuralnetworks,discussvectorizationanddiscusstrainingneural\nnetworkswithbackpropagation.\n7.1Supervisedlearningwithnon-linearmod-\nels\nInthesupervisedlearningsetting(predicting\ny\nfromtheinput\nx\n),suppose\nourmodel/hypothesisis\nh\n\n(\nx\n).Inthepastlectures,wehaveconsideredthe\ncaseswhen\nh\n\n(\nx\n)=\n\n>\nx\n(inlinearregressionorlogisticregression)or\nh\n\n(\nx\n)=\n\n>\n\u02da\n(\nx\n)(where\n\u02da\n(\nx\n)isthefeaturemap).Acommonalityofthesetwomodels\nisthattheyarelinearintheparameters\n\n.Nextwewillconsiderlearning\ngeneralfamilyofmodelsthatare\nnon-linearinboth\ntheparameters\n\nandtheinputs\nx\n.Themostcommonnon-linearmodelsareneuralnetworks,\nwhichwewillstaringfromthenextsection.Forthissection,it\ntothink\nh\n\n(\nx\n)asanabstractnon-linearmodel.\n1\nSuppose\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n)\ng\nn\ni\n=1\narethetrainingexamples.Forsimplicity,westart\nwiththecasewhere\ny\n(\ni\n)\n2\nR\nand\nh\n\n(\nx\n)\n2\nR\n.\nCost/lossfunction.\nWetheleastsquarecostfunctionforthe\ni\n-th\nexample(\nx\n(\ni\n)\n;y\n(\ni\n)\n)as\nJ\n(\ni\n)\n(\n\n)=\n1\n2\n(\nh\n\n(\nx\n(\ni\n)\n)\n\ny\n(\ni\n)\n)\n2\n(7.1)\n1\nIfaconcreteexampleishelpful,perhapsthinkaboutthemodel\nh\n\n(\nx\n)=\n\n2\n1\nx\n2\n1\n+\n\n2\n2\nx\n2\n2\n+\n\n+\n\n2\nd\nx\n2\nd\ninthissubsection,eventhoughit'snotaneuralnetwork.\n81\n"}, {"page_number": 82, "text": "82\nandthemean-squarecostfunctionforthedatasetas\nJ\n(\n\n)=\n1\nn\nn\nX\ni\n=1\nJ\n(\ni\n)\n(\n\n)(7.2)\nwhichissameasinlinearregressionexceptthatweintroduceaconstant\n1\n=n\ninfrontofthecostfunctiontobeconsistentwiththeconvention.Note\nthatmultiplyingthecostfunctionwithascalarwillnotchangethelocal\nminimaorglobalminimaofthecostfunction.Alsonotethattheunderlying\nparameterizationfor\nh\n\n(\nx\n)iserentfromthecaseoflinearregression,\neventhoughtheformofthecostfunctionisthesamemean-squaredloss.\nThroughoutthenotes,weusethewords\\loss\"and\\cost\"interchangeably.\nOptimizers(SGD).\nCommonly,peopleusegradientdescent(GD),stochas-\nticgradient(SGD),ortheirvariantstooptimizethelossfunction\nJ\n(\n\n).GD's\nupdaterulecanbewrittenas\n2\n\n:=\n\n\n\nr\n\nJ\n(\n\n)(7.3)\nwhere\n>\n0isoftenreferredtoasthelearningrateorstepsize.Next,we\nintroduceaversionoftheSGD(Algorithm1),whichislightlytfrom\nthatinthelecturenotes.\nAlgorithm1\nStochasticGradientDescent\n1:\nHyperparameter:learningrate\n\n,numberoftotaliteration\nn\niter\n.\n2:\nInitialize\n\nrandomly.\n3:\nfor\ni\n=1to\nn\niter\ndo\n4:\nSample\nj\nuniformlyfrom\nf\n1\n;:::;n\ng\n,andupdate\n\nby\n\n:=\n\n\n\nr\n\nJ\n(\nj\n)\n(\n\n)(7.4)\nOftentimescomputingthegradientof\nB\nexamplessimultaneouslyforthe\nparameter\n\ncanbefasterthancomputing\nB\ngradientsseparatelydueto\nhardwareparallelization.Therefore,amini-batchversionofSGDismost\n2\nRecallthat,asinthepreviouslecturenotes,weusethenotation\\\na\n:=\nb\n\"to\ndenoteanoperation(inacomputerprogram)inwhichwe\nset\nthevalueofavariable\na\nto\nbeequaltothevalueof\nb\n.Inotherwords,thisoperationoverwrites\na\nwiththevalueof\nb\n.Incontrast,wewillwrite\\\na\n=\nb\n\"whenweareassertingastatementoffact,thatthe\nvalueof\na\nisequaltothevalueof\nb\n.\n"}, {"page_number": 83, "text": "83\ncommonlyusedindeeplearning,asshowninAlgorithm2.Therearealso\nothervariantsoftheSGDormini-batchSGDwithslightlyentsampling\nschemes.\nAlgorithm2\nMini-batchStochasticGradientDescent\n1:\nHyperparameters:learningrate\n\n,batchsize\nB\n,#iterations\nn\niter\n.\n2:\nInitialize\n\nrandomly\n3:\nfor\ni\n=1to\nn\niter\ndo\n4:\nSample\nB\nexamples\nj\n1\n;:::;j\nB\n(withoutreplacement)uniformlyfrom\nf\n1\n;:::;n\ng\n,andupdate\n\nby\n\n:=\n\n\n\nB\nB\nX\nk\n=1\nr\n\nJ\n(\nj\nk\n)\n(\n\n)(7.5)\nWiththesegenericalgorithms,atypicaldeeplearningmodelislearned\nwiththefollowingsteps.1.aneuralnetworkparametrization\nh\n\n(\nx\n),\nwhichwewillintroduceinSection7.2,and2.writethebackpropagation\nalgorithmtocomputethegradientofthelossfunction\nJ\n(\nj\n)\n(\n\n)tly,\nwhichwillbecoveredinSection7.3,and3.runSGDormini-batchSGD(or\nothergradient-basedoptimizers)withthelossfunction\nJ\n(\n\n).\n7.2Neuralnetworks\nNeuralnetworksrefertobroadtypeofnon-linearmodels/parametrizations\nh\n\n(\nx\n)thatinvolvecombinationsofmatrixmultiplicationsandotherentry-\nwisenon-linearoperations.Wewillstartsmallandslowlybuildupaneural\nnetwork,stepbystep.\nANeuralNetworkwithaSingleNeuron.\nRecallthehousingprice\npredictionproblemfrombefore:giventhesizeofthehouse,wewantto\npredicttheprice.Wewilluseitasarunningexampleinthissubsection.\nPreviously,weastraightlinetothegraphofsizevs.housingprice.\nNow,insteadofastraightline,wewishtopreventnegativehousing\npricesbysettingtheabsoluteminimumpriceaszero.Thisproducesa\\kink\"\ninthegraphasshowninFigure7.1.Howdowerepresentsuchafunction\nwithasinglekinkas\nh\n\n(\nx\n)withunknownparameter?(Afterdoingso,we\ncaninvokethemachineryinSection7.1.)\n"}, {"page_number": 84, "text": "84\nWeaparameterizedfunction\nh\n\n(\nx\n)withinput\nx\n,parameterizedby\n\n,whichoutputsthepriceofthehouse\ny\n.Formally,\nh\n\n:\nx\n!\ny\n.Perhaps\noneofthesimplestparametrizationwouldbe\nh\n\n(\nx\n)=max(\nwx\n+\nb;\n0)\n;\nwhere\n\n=(\nw;b\n)\n2\nR\n2\n(7.6)\nHere\nh\n\n(\nx\n)returnsasinglevalue:(\nwx\n+\nb\n)orzero,whicheverisgreater.In\nthecontextofneuralnetworks,thefunctionmax\nf\nt;\n0\ng\niscalledaReLU(pro-\nnounced\\ray-lu\"),orlinearunit,andoftendenotedbyReLU(\nt\n)\n,\nmax\nf\nt;\n0\ng\n.\nGenerally,aone-dimensionalnon-linearfunctionthatmaps\nR\nto\nR\nsuchas\nReLUisoftenreferredtoasan\nactivationfunction\n.Themodel\nh\n\n(\nx\n)issaid\ntohaveasingleneuronpartlybecauseithasasinglenon-linearactivation\nfunction.(Wewilldiscussmoreaboutwhyanon-linearactivationiscalled\nneuron.)\nWhentheinput\nx\n2\nR\nd\nhasmultipledimensions,aneuralnetworkwith\nasingleneuroncanbewrittenas\nh\n\n(\nx\n)=ReLU(\nw\n>\nx\n+\nb\n)\n;\nwhere\nw\n2\nR\nd\n,\nb\n2\nR\n,and\n\n=(\nw;b\n)(7.7)\nTheterm\nb\nisoftenreferredtoasthe\\bias\",andthevector\nw\nisreferred\ntoastheweightvector.Suchaneuralnetworkhas1layer.(Wewill\nwhatmultiplelayersmeaninthesequel.)\nStackingNeurons.\nAmorecomplexneuralnetworkmaytakethesingle\nneurondescribedaboveand\\stack\"themtogethersuchthatoneneuron\npassesitsoutputasinputintothenextneuron,resultinginamorecomplex\nfunction.\nLetusnowdeepenthehousingpredictionexample.Inadditiontothesize\nofthehouse,supposethatyouknowthenumberofbedrooms,thezipcode\nandthewealthoftheneighborhood.Buildingneuralnetworksisanalogous\ntoLegobricks:youtakeindividualbricksandstackthemtogethertobuild\ncomplexstructures.Thesameappliestoneuralnetworks:wetakeindividual\nneuronsandstackthemtogethertocreatecomplexneuralnetworks.\nGiventhesefeatures(size,numberofbedrooms,zipcode,andwealth),\nwemightthendecidethatthepriceofthehousedependsonthemaximum\nfamilysizeitcanaccommodate.Supposethefamilysizeisafunctionofthe\nsizeofthehouseandnumberofbedrooms(seeFigure7.2).Thezipcode\nmayprovideadditionalinformationsuchashowwalkabletheneighborhood\nis(i.e.,canyouwalktothegrocerystoreordoyouneedtodriveeverywhere).\nCombiningthezipcodewiththewealthoftheneighborhoodmaypredict\n"}, {"page_number": 85, "text": "85\nFigure7.1:Housingpriceswitha\\kink\"inthegraph.\nthequalityofthelocalelementaryschool.Giventhesethreederivedfeatures\n(familysize,walkable,schoolquality),wemayconcludethatthepriceofthe\nhomeultimatelydependsonthesethreefeatures.\nFigure7.2:Diagramofasmallneuralnetworkforpredictinghousingprices.\nFormally,theinputtoaneuralnetworkisasetofinputfeatures\nx\n1\n;x\n2\n;x\n3\n;x\n4\n.Wedenotetheintermediatevariablesfor\\familysize\",\\walk-\nable\",and\\schoolquality\"by\na\n1\n;a\n2\n;a\n3\n(these\na\ni\n'sareoftenreferredtoas\n\\hiddenunits\"or\\hiddenneurons\").Werepresenteachofthe\na\ni\n'sasaneu-\nralnetworkwithasingleneuronwithasubsetof\nx\n1\n;:::;x\n4\nasinputs.Then\nasinFigure7.1,wewillhavetheparameterization:\na\n1\n=ReLU(\n\n1\nx\n1\n+\n\n2\nx\n2\n+\n\n3\n)\na\n2\n=ReLU(\n\n4\nx\n3\n+\n\n5\n)\na\n3\n=ReLU(\n\n6\nx\n3\n+\n\n7\nx\n4\n+\n\n8\n)\n"}, {"page_number": 86, "text": "86\nwhere(\n\n1\n;\n\n;\n8\n)areparameters.Nowwerepresenttheoutput\nh\n\n(\nx\n)\nasanotherlinearfunctionwith\na\n1\n;a\n2\n;a\n3\nasinputs,andweget\n3\nh\n\n(\nx\n)=\n\n9\na\n1\n+\n\n10\na\n2\n+\n\n11\na\n3\n+\n\n12\n(7.8)\nwhere\n\ncontainsalltheparameters(\n\n1\n;\n\n;\n12\n).\nNowwerepresenttheoutputasaquitecomplexfunctionof\nx\nwithpa-\nrameters\n\n.Thenyoucanusethisparametrization\nh\n\nwiththemachineryof\nSection7.1tolearntheparameters\n\n.\nInspirationfromBiologicalNeuralNetworks.\nAsthenamesuggests,\nneuralnetworkswereinspiredbybiologicalneuralnetworks.The\nhiddenunits\na\n1\n;:::;a\nm\ncorrespondtotheneuronsinabiologicalneuralnet-\nwork,andtheparameters\n\ni\n'scorrespondtothesynapses.However,it's\nunclearhowsimilarthemoderndeepneuralnetworksaretothebi-\nologicalones.Forexample,perhapsnotmanyneuroscientiststhinkbiological\nneuralnetworkscouldhave1000layers,whilesomemodernneural\nnetworksdo(wewillelaboratemoreonthenotionoflayers.)Moreover,it's\nanopenquestionwhetherhumanbrainsupdatetheirneuralnetworksina\nwaysimilartothewaythatcomputerscientistslearnneuralnet-\nworks(usingbackpropagation,whichwewillintroduceinthenextsection.).\nTwo-layerFully-ConnectedNeuralNetworks.\nWeconstructedthe\nneuralnetworkinequation(7.8)usingatamountofpriorknowl-\nedge/beliefabouthowthe\\familysize\",\\walkable\",and\\schoolquality\"are\ndeterminedbytheinputs.Weimplicitlyassumedthatweknowthefamily\nsizeisanimportantquantitytolookatandthatitcanbedeterminedby\nonlythe\\size\"and\\#bedrooms\".Suchapriorknowledgemightnotbe\navailableforotherapplications.Itwouldbemoreandgeneraltohave\nagenericparameterization.Asimplewaywouldbetowritetheintermediate\nvariable\na\n1\nasafunctionofall\nx\n1\n;:::;x\n4\n:\na\n1\n=ReLU(\nw\n>\n1\nx\n+\nb\n1\n)\n;\nwhere\nw\n1\n2\nR\n4\nand\nb\n1\n2\nR\n(7.9)\na\n2\n=ReLU(\nw\n>\n2\nx\n+\nb\n2\n)\n;\nwhere\nw\n2\n2\nR\n4\nand\nb\n2\n2\nR\na\n3\n=ReLU(\nw\n>\n3\nx\n+\nb\n3\n)\n;\nwhere\nw\n3\n2\nR\n4\nand\nb\n3\n2\nR\nWestill\nh\n\n(\nx\n)usingequation(7.8)with\na\n1\n;a\n2\n;a\n3\nbeingd\nasabove.Thuswehaveaso-called\nfully-connectedneuralnetwork\nas\n3\nTypically,formulti-layerneuralnetwork,attheend,neartheoutput,wedon'tapply\nReLU,especiallywhentheoutputisnotnecessarilyapositivenumber.\n"}, {"page_number": 87, "text": "87\nFigure7.3:Diagramofatwo-layerfullyconnectedneuralnetwork.Each\nedgefromnode\nx\ni\ntonode\na\nj\nindicatesthat\na\nj\ndependson\nx\ni\n.Theedgefrom\nx\ni\nto\na\nj\nisassociatedwiththeweight(\nw\n[1]\nj\n)\ni\nwhichdenotesthe\ni\n-thcoordinate\nofthevector\nw\n[1]\nj\n.Theactivation\na\nj\ncanbecomputedbytakingtheReLUof\ntheweightedsumof\nx\ni\n'swiththeweightsbeingtheweightsassociatedwith\ntheincomingedges,thatis,\na\nj\n=ReLU(\nP\nd\ni\n=1\n(\nw\n[1]\nj\n)\ni\nx\ni\n)\n:\nvisualizedinthedependencygraphinFigure7.3becausealltheintermediate\nvariables\na\ni\n'sdependonalltheinputs\nx\ni\n's.\nForfullgenerality,atwo-layerfully-connectedneuralnetworkwith\nm\nhiddenunitsand\nd\ndimensionalinput\nx\n2\nR\nd\nisas\n8\nj\n2\n[1\n;:::;m\n]\n;z\nj\n=\nw\n[1]\nj\n>\nx\n+\nb\n[1]\nj\nwhere\nw\n[1]\nj\n2\nR\nd\n;b\n[1]\nj\n2\nR\n(7.10)\na\nj\n=ReLU(\nz\nj\n)\n;\na\n=[\na\n1\n;:::;a\nm\n]\n>\n2\nR\nm\nh\n\n(\nx\n)=\nw\n[2]\n>\na\n+\nb\n[2]\nwhere\nw\n[2]\n2\nR\nm\n;b\n[2]\n2\nR\n;\n(7.11)\nNotethatbydefaultthevectorsin\nR\nd\nareviewedascolumnvectors,and\ninparticular\na\nisacolumnvectorwithcomponents\na\n1\n;a\n2\n;:::;a\nm\n.Theindices\n[1]\nand\n[2]\nareusedtodistinguishtwosetsofparameters:the\nw\n[1]\nj\n's(eachof\nwhichisavectorin\nR\nd\n)and\nw\n[2]\n(whichisavectorin\nR\nm\n).Wewillhave\nmoreoftheselater.\nVectorization.\nBeforeweintroduceneuralnetworkswithmorelayersand\nmorecomplexstructures,wewillsimplifytheexpressionsforneuralnetworks\nwithmorematrixandvectornotations.Anotherimportantmotivationof\n"}, {"page_number": 88, "text": "88\nvectorizationisthespeedperspectiveintheimplementation.Inorderto\nimplementaneuralnetworktly,onemustbecarefulwhenusingfor\nloops.Themostnaturalwaytoimplementequation(7.10)incodeisperhaps\ntouseaforloop.Inpractice,thedimensionalitiesoftheinputsandhidden\nunitsarehigh.Asaresult,codewillrunveryslowlyifyouuseforloops.\nLeveragingtheparallelisminGPUsis/wascrucialfortheprogressofdeep\nlearning.\nThisgaveriseto\nvectorization\n.Insteadofusingforloops,vectorization\ntakesadvantageofmatrixalgebraandhighlyoptimizednumericallinear\nalgebrapackages(e.g.,BLAS)tomakeneuralnetworkcomputationsrun\nquickly.Beforethedeeplearningera,aforloopmayhavebeent\nonsmallerdatasets,butmoderndeepnetworksandstate-of-the-artdatasets\nwillbeinfeasibletorunwithforloops.\nWevectorizethetwo-layerfully-connectedneuralnetworkasbelow.We\naweightmatrix\nW\n[1]\nin\nR\nm\n\nd\nastheconcatenationofallthevectors\nw\n[1]\nj\n'sinthefollowingway:\nW\n[1]\n=\n2\n6\n6\n6\n6\n4\n|\nw\n[1]\n1\n>\n|\n|\nw\n[1]\n2\n>\n|\n.\n.\n.\n|\nw\n[1]\nm\n>\n|\n3\n7\n7\n7\n7\n5\n2\nR\nm\n\nd\n(7.12)\nNowbytheofmatrixvectormultiplication,wecanwrite\nz\n=\n[\nz\n1\n;:::;z\nm\n]\n>\n2\nR\nm\nas\n2\n6\n6\n6\n4\nz\n1\n.\n.\n.\n.\n.\n.\nz\nm\n3\n7\n7\n7\n5\n|\n{z\n}\nz\n2\nR\nm\n\n1\n=\n2\n6\n6\n6\n6\n4\n|\nw\n[1]\n1\n>\n|\n|\nw\n[1]\n2\n>\n|\n.\n.\n.\n|\nw\n[1]\nm\n>\n|\n3\n7\n7\n7\n7\n5\n|\n{z\n}\nW\n[1]\n2\nR\nm\n\nd\n2\n6\n6\n6\n4\nx\n1\nx\n2\n.\n.\n.\nx\nd\n3\n7\n7\n7\n5\n|\n{z\n}\nx\n2\nR\nd\n\n1\n+\n2\n6\n6\n6\n4\nb\n[1]\n1\nb\n[1]\n2\n.\n.\n.\nb\n[1]\nm\n3\n7\n7\n7\n5\n|\n{z\n}\nb\n[1]\n2\nR\nm\n\n1\n(7.13)\nOrsuccinctly,\nz\n=\nW\n[1]\nx\n+\nb\n[1]\n(7.14)\nWeremarkagainthatavectorin\nR\nd\ninthisnotes,followingtheconventions\npreviouslyestablished,isautomaticallyviewedasacolumnvector,andcan\nalsobeviewedasa\nd\n\n1dimensionalmatrix.(Notethatthisisent\nfromnumpywhereavectorisviewedasarowvectorinbroadcasting.)\n"}, {"page_number": 89, "text": "89\nComputingtheactivations\na\n2\nR\nm\nfrom\nz\n2\nR\nm\ninvolvesanelement-\nwisenon-linearapplicationoftheReLUfunction,whichcanbecomputedin\nparalleltly.OverloadingReLUforelement-wiseapplicationofReLU\n(meaning,foravector\nt\n2\nR\nd\n,ReLU(\nt\n)isavectorsuchthatReLU(\nt\n)\ni\n=\nReLU(\nt\ni\n)),wehave\na\n=ReLU(\nz\n)(7.15)\n\nW\n[2]\n=[\nw\n[2]\n>\n]\n2\nR\n1\n\nm\nsimilarly.Then,themodelinequa-\ntion(7.11)canbesummarizedas\na\n=ReLU(\nW\n[1]\nx\n+\nb\n[1]\n)\nh\n\n(\nx\n)=\nW\n[2]\na\n+\nb\n[2]\n(7.16)\nHere\n\nconsistsof\nW\n[1]\n;W\n[2]\n(oftenreferredtoastheweightmatrices)and\nb\n[1]\n;b\n[2]\n(referredtoasthebiases).Thecollectionof\nW\n[1]\n;b\n[1]\nisreferredtoas\nthelayer,and\nW\n[2]\n;b\n[2]\nthesecondlayer.Theactivation\na\nisreferredtoas\nthehiddenlayer.Atwo-layerneuralnetworkisalsocalledone-hidden-layer\nneuralnetwork.\nMulti-layerfully-connectedneuralnetworks.\nWiththissuccinctno-\ntations,wecanstackmorelayerstogetadeeperfully-connectedneu-\nralnetwork.Let\nr\nbethenumberoflayers(weightmatrices).Let\nW\n[1]\n;:::;W\n[\nr\n]\n;b\n[1]\n;:::;b\n[\nr\n]\nbetheweightmatricesandbiasesofallthelayers.\nThenamulti-layerneuralnetworkcanbewrittenas\na\n[1]\n=ReLU(\nW\n[1]\nx\n+\nb\n[1]\n)\na\n[2]\n=ReLU(\nW\n[2]\na\n[1]\n+\nb\n[2]\n)\n\na\n[\nr\n\n1]\n=ReLU(\nW\n[\nr\n\n1]\na\n[\nr\n\n2]\n+\nb\n[\nr\n\n1]\n)\nh\n\n(\nx\n)=\nW\n[\nr\n]\na\n[\nr\n\n1]\n+\nb\n[\nr\n]\n(7.17)\nWenotethattheweightmatricesandbiasesneedtohavecompatible\ndimensionsfortheequationsabovetomakesense.If\na\n[\nk\n]\nhasdimension\nm\nk\n,\nthentheweightmatrix\nW\n[\nk\n]\nshouldbeofdimension\nm\nk\n\nm\nk\n\n1\n,andthebias\nb\n[\nk\n]\n2\nR\nm\nk\n.Moreover,\nW\n[1]\n2\nR\nm\n1\n\nd\nand\nW\n[\nr\n]\n2\nR\n1\n\nm\nr\n\n1\n.\nThetotalnumberofneuronsinthenetworkis\nm\n1\n+\n\n+\nm\nr\n,andthe\ntotalnumberofparametersinthisnetworkis(\nd\n+1)\nm\n1\n+(\nm\n1\n+1)\nm\n2\n+\n\n+\n(\nm\nr\n\n1\n+1)\nm\nr\n.\n"}, {"page_number": 90, "text": "90\nSometimesfornotationalconsistencywealsowrite\na\n[0]\n=\nx\n,and\na\n[\nr\n]\n=\nh\n\n(\nx\n).Thenwehavesimplerecursionthat\na\n[\nk\n]\n=ReLU(\nW\n[\nk\n]\na\n[\nk\n\n1]\n+\nb\n[\nk\n]\n)\n;\n8\nk\n=1\n;:::;r\n\n1(7.18)\nNotethatthiswouldhavebetruefor\nk\n=\nr\niftherewereanadditional\nReLUinequation(7.17),butoftenpeopleliketomakethelastlayerlinear\n(akawithoutaReLU)sothatnegativeoutputsarepossibleandit'seasier\ntointerpretthelastlayerasalinearmodel.(Moreontheinterpretabilityat\nthe\\connectiontokernelmethod\"paragraphofthissection.)\nOtheractivationfunctions.\nTheactivationfunctionReLUcanbere-\nplacedbymanyothernon-linearfunction\n\u02d9\n(\n\n)thatmaps\nR\nto\nR\nsuchas\n\u02d9\n(\nz\n)=\n1\n1+\ne\n\nz\n(sigmoid)(7.19)\n\u02d9\n(\nz\n)=\ne\nz\n\ne\n\nz\ne\nz\n+\ne\n\nz\n(tanh)(7.20)\nWhydowenotusetheidentityfunctionfor\n\u02d9\n(\nz\n)\n?\nThatis,why\nnotuse\n\u02d9\n(\nz\n)=\nz\n?Assumeforsakeofargumentthat\nb\n[1]\nand\nb\n[2]\narezeros.\nSuppose\n\u02d9\n(\nz\n)=\nz\n,thenfortwo-layerneuralnetwork,wehavethat\nh\n\n(\nx\n)=\nW\n[2]\na\n[1]\n(7.21)\n=\nW\n[2]\n\u02d9\n(\nz\n[1]\n)by(7.22)\n=\nW\n[2]\nz\n[1]\nsince\n\u02d9\n(\nz\n)=\nz\n(7.23)\n=\nW\n[2]\nW\n[1]\nx\nfromEquation(7.13)(7.24)\n=\n~\nWx\nwhere\n~\nW\n=\nW\n[2]\nW\n[1]\n(7.25)\nNoticehow\nW\n[2]\nW\n[1]\ncollapsedinto\n~\nW\n.\nThisisbecauseapplyingalinearfunctiontoanotherlinearfunctionwill\nresultinalinearfunctionovertheoriginalinput(i.e.,youcanconstructa\n~\nW\nsuchthat\n~\nWx\n=\nW\n[2]\nW\n[1]\nx\n).Thislosesmuchoftherepresentationalpower\noftheneuralnetworkasoftentimestheoutputwearetryingtopredict\nhasanon-linearrelationshipwiththeinputs.Withoutnon-linearactivation\nfunctions,theneuralnetworkwillsimplyperformlinearregression.\nConnectiontotheKernelMethod.\nInthepreviouslectures,wecovered\ntheconceptoffeaturemaps.Recallthatthemainmotivationforfeature\n"}, {"page_number": 91, "text": "91\nmapsistorepresentfunctionsthatarenon-linearintheinput\nx\nby\n\n>\n\u02da\n(\nx\n),\nwhere\n\naretheparametersand\n\u02da\n(\nx\n),thefeaturemap,isahandcrafted\nfunctionnon-linearintherawinput\nx\n.Theperformanceofthelearning\nalgorithmscantlydependsonthechoiceofthefeaturemap\n\u02da\n(\nx\n).\nOftentimespeopleusedomainknowledgetodesignthefeaturemap\n\u02da\n(\nx\n)that\nsuitstheparticularapplications.Theprocessofchoosingthefeaturemaps\nisoftenreferredtoas\nfeatureengineering\n.\nWecanviewdeeplearningasawaytoautomaticallylearntheright\nfeaturemap(sometimesalsoreferredtoas\\therepresentation\")asfollows.\nSupposewedenoteby\n\nthecollectionoftheparametersinafully-connected\nneuralnetworks(equation(7.17))exceptthoseinthelastlayer.Thenwe\ncanabstractright\na\n[\nr\n\n1]\nasafunctionoftheinput\nx\nandtheparametersin\n\n:\na\n[\nr\n\n1]\n=\n\u02da\n\n(\nx\n).Nowwecanwritethemodelas\nh\n\n(\nx\n)=\nW\n[\nr\n]\n\u02da\n\n(\nx\n)+\nb\n[\nr\n]\n(7.26)\nWhen\n\nisthen\n\u02da\n\n(\n\n)canviewedasafeaturemap,andtherefore\nh\n\n(\nx\n)\nisjustalinearmodeloverthefeatures\n\u02da\n\n(\nx\n).However,wewilltrainthe\nneuralnetworks,boththeparametersin\n\nandtheparameters\nW\n[\nr\n]\n;b\n[\nr\n]\nare\noptimized,andthereforewearenotlearningalinearmodelinthefeature\nspace,butalsolearningagoodfeaturemap\n\u02da\n\n(\n\n)itselfsothatit'spossi-\nbletopredictaccuratelywithalinearmodelontopofthefeaturemap.\nTherefore,deeplearningtendstodependlessonthedomainknowledgeof\ntheparticularapplicationsandrequiresoftenlessfeatureengineering.The\npenultimatelayer\na\n[\nr\n]\nisoften(informally)referredtoasthelearnedfeatures\norrepresentationsinthecontextofdeeplearning.\nIntheexampleofhousepriceprediction,afully-connectedneuralnetwork\ndoesnotneedustospecifytheintermediatequantitysuch\\familysize\",and\nmayautomaticallydiscoversomeusefulfeaturesinthelastpenultimatelayer\n(theactivation\na\n[\nr\n\n1]\n),andusethemtolinearlypredictthehousingprice.\nOftenthefeaturemap/representationobtainedfromonedatasets(thatis,\nthefunction\n\u02da\n\n(\n\n)canbealsousefulforotherdatasets,whichindicatesthey\ncontainessentialinformationaboutthedata.However,oftentimes,theneural\nnetworkwilldiscovercomplexfeatureswhichareveryusefulforpredicting\ntheoutputbutmaybeforahumantounderstandorinterpret.This\niswhysomepeoplerefertoneuralnetworksasa\nblackbox\n,asitcanbe\ntounderstandthefeaturesithasdiscovered.\n"}, {"page_number": 92, "text": "92\n7.3Backpropagation\nInthissection,weintroducebackpropgationortiation,which\ncomputesthegradientoftheloss\nr\nJ\n(\nj\n)\n(\n\n)tly.Wewillstartwithan\ninformaltheoremthatstatesthataslongasareal-valuedfunction\nf\ncanbe\ntlycomputed/evaluatedbyaerentiablenetworkorcircuit,thenits\ngradientcanbetlycomputedinasimilartime.Wewillthenshow\nhowtodothisconcretelyforfully-connectedneuralnetworks.\nBecausetheformalityofthegeneraltheoremisnotthemainfocushere,\nwewillintroducethetermswithinformalnitions.Byatiable\ncircuitoratiablenetwork,wemeanacompositionofasequenceof\ntiablearithmeticoperations(additions,subtraction,multiplication,\ndivisions,etc)andelementarytiablefunctions(ReLU,exp,log,sin,\ncos,etc.).Letthesizeofthecircuitbethetotalnumberofsuchoperations\nandelementaryfunctions.Weassumethateachoftheoperationsandfunc-\ntions,andtheirderivativesorpartialderivativesecanbecomputedin\nO\n(1)\ntimeinthecomputer.\nTheorem7.3.1\n:\n[backpropagationorentiation,informallystated]\nSupposeaentiablecircuitofsize\nN\ncomputesareal-valuedfunction\nf\n:\nR\n`\n!\nR\n.Then,thegradient\nr\nf\ncanbecomputedintime\nO\n(\nN\n)\n,bya\ncircuitofsize\nO\n(\nN\n)\n.\n4\nWenotethatthelossfunction\nJ\n(\nj\n)\n(\n\n)for\nj\n-thexamplecanbeindeed\ncomputedbyasequenceofoperationsandfunctionsinvolvingadditions,\nsubtraction,multiplications,andnon-linearactivations.Thusthetheorem\nsuggeststhatweshouldbeabletocomputethe\nr\nJ\n(\nj\n)\n(\n\n)inasimilartime\ntothatforcomputing\nJ\n(\nj\n)\n(\n\n)itself.Thisdoesnotonlyapplytothefully-\nconnectedneuralnetworkintroducedintheSection7.2,butalsomanyother\ntypesofneuralnetworks.\nIntherestofthesection,wewillshowcasehowtocomputethegradientof\nthelosstlyforfully-connectedneuralnetworksusingbackpropagation.\nEventhoughtiationorbackpropagationisimplementedinall\nthedeeplearningpackagessuchaswandpytorch,understandingit\nisveryhelpfulforgaininginsightsintotheworkingofdeeplearning.\n4\nWenoteiftheoutputofthefunction\nf\ndoesnotdependonsomeoftheinputco-\nordinates,thenwesetbydefaultthegradientw.r.tthatcoordinatetozero.Settingto\nzerodoesnotcounttowardsthetotalruntimehereinouraccountingscheme.Thisiswhy\nwhen\nN\n\n`\n,wecancomputethegradientin\nO\n(\nN\n)time,whichmightbepotentiallyeven\nlessthan\n`\n.\n"}, {"page_number": 93, "text": "93\n7.3.1Preliminary:chainrule\nWerecallthechainruleincalculus.Supposethevariable\nJ\ndependson\nthevariables\n\n1\n;:::;\np\nviatheintermediatevariable\ng\n1\n;:::;g\nk\n:\ng\nj\n=\ng\nj\n(\n\n1\n;:::;\np\n)\n;\n8\nj\n2f\n1\n;\n\n;k\ng\n(7.27)\nJ\n=\nJ\n(\ng\n1\n;:::;g\nk\n)(7.28)\nHereweoverloadthemeaningof\ng\nj\n's:theydenoteboththeintermediate\nvariablesbutalsothefunctionsusedtocomputetheintermediatevariables.\nThen,bythechainrule,wehavethat\n8\ni\n,\n@J\n@\ni\n=\nk\nX\nj\n=1\n@J\n@g\nj\n@g\nj\n@\ni\n(7.29)\nFortheeaseofinvokingthechainruleinthefollowingsubsectionsinvarious\nways,wewillcall\nJ\ntheoutputvariable,\ng\n1\n;:::;g\nk\nintermediatevariables,\nand\n\n1\n;:::;\np\ntheinputvariableinthechainrule.\n7.3.2One-neuronneuralnetworks\nSimplifyingnotations:\nIntherestofthesection,wewillconsidera\ngenericinput\nx\nandcomputethegradientof\nh\n\n(\nx\n)w.r.t\n\n.Forsimplicity,\nweuse\no\nasashorthandfor\nh\n\n(\nx\n)(\no\nstandsfor\noutput\n).Forsimplicity,with\nslightabuseofnotation,weuse\nJ\n=\n1\n2\n(\ny\n\no\n)\n2\ntodenotethelossfunction.\n(Notethatthisoverridestheof\nJ\nasthetotallossinSection7.1.)\nOurgoalistocomputethederivativeof\nJ\nw.r.ttheparameter\n\n.\nWeconsidertheneuralnetworkwithoneneuroninequa-\ntion(7.7).Recallthatwecomputethelossfunctionviathefollowingse-\nquentialsteps:\nz\n=\nw\n>\nx\n+\nb\n(7.30)\no\n=ReLU(\nz\n)(7.31)\nJ\n=\n1\n2\n(\ny\n\no\n)\n2\n(7.32)\nBythechainrulewith\nJ\nastheoutputvariable,\no\nastheintermediatevariable,\nand\nw\ni\ntheinputvariable,wehavethat\n@J\n@w\ni\n=\n@J\n@o\n\n@o\n@w\ni\n(7.33)\n"}, {"page_number": 94, "text": "94\nInvokingthechainrulewith\no\nastheoutputvariable,\nz\nastheintermediate\nvariable,and\nw\ni\ntheinputvariable,wehavethat\n@o\n@w\ni\n=\n@o\n@z\n\n@z\n@w\ni\nCombiningtheequationabovewithequation(7.33),wehave\n@J\n@w\ni\n=\n@J\n@o\n\n@o\n@z\n\n@z\n@w\ni\n=(\no\n\ny\n)\n\nReLU\n0\n(\nz\n)\n\nx\ni\n(because\n@J\n@o\n=(\no\n\ny\n)and\n@o\n@z\n=ReLU\n0\n(\nz\n)and\n@z\n@w\ni\n=\nx\ni\n)\nHere,thekeyisthatwereducethecomputationof\n@J\n@w\ni\ntothecomputa-\ntionofthreesimplermore\\local\"objects\n@J\n@o\n;\n@o\n@z\n,and\n@z\n@w\ni\n,whicharemuch\nsimplertocomputebecause\nJ\ndirectlydependson\no\nviaequation(7.32),\no\ndirectlydependson\na\nviaequation(7.31),and\nz\ndirectlydependson\nw\ni\nvia\nequation(7.30).Notethatinavectorizedform,wecanalsowrite\nr\nw\nJ\n=(\no\n\ny\n)\n\nReLU\n0\n(\nz\n)\n\nx\nSimilarly,wecomputethegradientw.r.t\nb\nby\n@J\n@b\n=\n@J\n@o\n\n@o\n@z\n\n@z\n@b\n=(\no\n\ny\n)\n\nReLU\n0\n(\nz\n)\n(because\n@J\n@o\n=(\no\n\ny\n)and\n@o\n@z\n=ReLU\n0\n(\nz\n)and\n@z\n@b\n=1)\n7.3.3Two-layerneuralnetworks:alow-levelunpacked\ncomputation\nNote:\nthissubsectionderivesthederivativeswithlow-levelnotationsto\nhelpyoubuildupintuitiononbackpropagation.Ifyouarelookingfora\ncleanformula,oryouarefamiliarwithmatrixderivatives,thenfeelfreeto\njumptothenextsubsectiondirectly.\nNowweconsiderthetwo-layerneuralnetworkdinequation(7.11).\nWecomputetheloss\nJ\nbyfollowingsequenceofoperations\n8\nj\n2\n[1\n;:::;m\n]\n;z\nj\n=\nw\n[1]\nj\n>\nx\n+\nb\n[1]\nj\nwhere\nw\n[1]\nj\n2\nR\nd\n;b\n[1]\nj\na\nj\n=ReLU(\nz\nj\n)\n;\na\n=[\na\n1\n;:::;a\nm\n]\n>\n2\nR\nm\no\n=\nw\n[2]\n>\na\n+\nb\n[2]\nwhere\nw\n[2]\n2\nR\nm\n;b\n[2]\n2\nR\nJ\n=\n1\n2\n(\ny\n\no\n)\n2\n(7.34)\n"}, {"page_number": 95, "text": "95\nWewilluse(\nw\n[2]\n)\n`\ntodenotethe\n`\n-thcoordinateof\nw\n[2]\n,and(\nw\n[1]\nj\n)\n`\ntodenote\nthe\n`\n-coordinateof\nw\n[1]\nj\n.(Wewillavoidusingthesecumbersomenotations\nonceweouthowtowriteeverythinginmatrixandvectorforms.)\nByinvokingchainrulewith\nJ\nastheoutputvariable,\no\nasintermediate\nvariable,and(\nw\n[2]\n)\n`\nastheinputvariable,wehave\n@J\n@\n(\nw\n[2]\n)\n`\n=\n@J\n@o\n@o\n@\n(\nw\n[2]\n)\n`\n=(\no\n\ny\n)\n@o\n@\n(\nw\n[2]\n)\n`\n=(\no\n\ny\n)\na\n`\nIt'smorechallengingtocompute\n@J\n@\n(\nw\n[1]\nj\n)\n`\n.Towardscomputingit,werst\ninvokethechainrulewith\nJ\nastheoutputvariable,\nz\nj\nastheintermediate\nvariable,and(\nw\n[1]\nj\n)\n`\nastheinputvariable.\n@J\n@\n(\nw\n[1]\nj\n)\n`\n=\n@J\n@z\nj\n\n@z\nj\n@\n(\nw\n[1]\nj\n)\n`\n=\n@J\n@z\nj\n\nx\n`\n(becaues\n@z\nj\n@\n(\nw\n[1]\nj\n)\n`\n=\nx\n`\n.)\nThus,ittocomputethe\n@J\n@z\nj\n.Weinvokethechainrulewith\nJ\nasthe\noutputvariable,\na\nj\nastheintermediatevariable,and\nz\nj\nastheinputvariable,\n@J\n@z\nj\n=\n@J\n@a\nj\n@a\nj\n@z\nj\n=\n@J\n@a\nj\nReLU\n0\n(\nz\nj\n)\nNowittocompute\n@J\n@a\nj\n,andweinvokethechainrulewith\nJ\nasthe\noutputvariable,\no\nastheintermediatevariable,and\na\nj\nastheinputvariable,\n@J\n@a\nj\n=\n@J\n@o\n@o\n@a\nj\n=(\no\n\ny\n)\n\n(\nw\n[2]\n)\nj\nNowcombiningtheequationsabove,weobtain\n@J\n@\n(\nw\n[1]\nj\n)\n`\n=(\no\n\ny\n)\n\n(\nw\n[2]\n)\nj\nReLU\n0\n(\nz\nj\n)\nx\n`\n"}, {"page_number": 96, "text": "96\nNextwegaugetheruntimeofcomputingthesepartialderivatives.Let\np\ndenotesthetotalnumberofparametersinthenetwork.Wenotethat\np\n\nmd\nwhere\nm\nisthenumberofhiddenunitsand\nd\nistheinputdimension.For\nevery\nj\nand\n`\n,tocompute\n@J\n@\n(\nw\n[1]\nj\n)\n`\n,apparentlyweneedtocomputeatleast\ntheoutput\no\n,whichtakesatleast\np\n\nmd\noperations.Thereforeatthe\nglancecomputingasinglegradienttakesatleast\nmd\ntime,andthetotaltime\ntocomputethederivativesw.r.ttoalltheparametersisatleast(\nmd\n)\n2\n,which\nist.\nHowever,thekeyofthebackpropagationisthatfortchoicesof\n`\n,\ntheformulasaboveforcomputing\n@J\n@\n(\nw\n[1]\nj\n)\n`\nsharemanyterms,suchas,(\no\n\ny\n),\n(\nw\n[2]\n)\nj\nandReLU\n0\n(\nz\nj\n).Thissuggeststhatwecanre-organizethecomputation\ntoleveragethesharedcomputation.\nItturnsoutthecrucialsharedquantitiesintheseformulasare\n@J\n@o\n,\n@J\n@z\n1\n;:::;\n@J\n@z\nm\n.Wenowwritethefollowingformulastocomputethegradi-\nentstlyinAlgorithm3.\nAlgorithm3\nBackpropagationfortwo-layerneuralnetworks\n1:\nComputethevaluesof\nz\n1\n;:::;z\nm\n,\na\n1\n;:::;a\nm\nand\no\nasintheof\nneuralnetwork(equation(7.34)).\n2:\nCompute\n@J\n@o\n=(\no\n\ny\n).\n3:\nCompute\n@J\n@z\nj\nfor\nj\n=1\n;:::;m\nby\n@J\n@z\nj\n=\n@J\n@o\n@o\n@a\nj\n@a\nj\n@z\nj\n=\n@J\n@o\n\n(\nw\n[2]\n)\nj\n\nReLU\n0\n(\nz\nj\n)(7.35)\n4:\nCompute\n@J\n@\n(\nw\n[1]\nj\n)\n`\n,\n@J\n@b\n[1]\nj\n,\n@J\n@\n(\nw\n[2]\n)\nj\n,and\n@J\n@b\n[2]\nby\n@J\n@\n(\nw\n[1]\nj\n)\n`\n=\n@J\n@z\nj\n\n@z\nj\n@\n(\nw\n[1]\nj\n)\n`\n=\n@J\n@z\nj\n\nx\n`\n@J\n@b\n[1]\nj\n=\n@J\n@z\nj\n\n@z\nj\n@b\n[1]\nj\n=\n@J\n@z\nj\n@J\n@\n(\nw\n[2]\n)\nj\n=\n@J\n@o\n@o\n@\n(\nw\n[2]\n)\nj\n=\n@J\n@o\n\na\nj\n@J\n@b\n[2]\n=\n@J\n@o\n@o\n@b\n[2]\n=\n@J\n@o\n"}, {"page_number": 97, "text": "97\n7.3.4Two-layerneuralnetworkwithvectornotation\nAswehavedonebeforeinthedeofneuralnetworks,theequationsfor\nbackpropagationbecomesmuchcleanerwithpropermatrixnotation.Here\nwestatethealgorithmandalsoprovideacleanerproofviamatrixcal-\nculus.\nLet\n\n[2]\n,\n@J\n@o\n2\nR\n\n[1]\n,\n@J\n@z\n2\nR\nm\n(7.36)\nHerewenotethatwhen\nA\nisareal-valuedvariable,\n5\nand\nB\nisavectoror\nmatrixvariable,then\n@A\n@B\ndenotesthecollectionofthepartialderivativeswith\nthesameshapeas\nB\n.\n6\nInotherwords,if\nB\nisamatrixofdimension\nm\n\nd\n,\nthen\n@A\n@B\nisamatrixin\nR\nm\n\nd\nwith\n@A\n@B\nij\nasthe\nij\nth-entry.Let\nv\n\nw\ndenote\ntheentry-wiseproductoftwovectors\nv\nand\nw\nofthesamedimension.Now\nwearereadytodescribebackpropagationinAlgorithm4.\nAlgorithm4\nBack-propagationfortwo-layerneuralnetworksinvectorized\nnotations.\n.\n1:\nComputethevaluesof\nz\n2\nR\nm\n,\na\n2\nR\nm\n,and\no\n2:\nCompute\n\n[2]\n=(\no\n\ny\n)\n2\nR\n3:\nCompute\n\n[1]\n=(\no\n\ny\n)\n\nW\n[2]\n>\n\nReLU\n0\n(\nz\n)\n2\nR\nm\n\n1\n4:\nCompute\n@J\n@W\n[2]\n=\n\n[2]\na\n>\n2\nR\n1\n\nm\n@J\n@b\n[2]\n=\n\n[2]\n2\nR\n@J\n@W\n[1]\n=\n\n[1]\nx\n>\n2\nR\nm\n\nd\n@J\n@b\n[1]\n=\n\n[1]\n2\nR\nm\n5\nWewillavoidusingthenotation\n@A\n@B\nfor\nA\nthatisnotareal-valuedvariable.\n6\nIfyouarefamiliarwiththenotionoftotalderivatives,wenotethatthedimensionality\nhereistfromthatfortotalderivatives.\n"}, {"page_number": 98, "text": "98\nDerivationusingthechainruleformatrixmultiplication.\nTo\nhaveasuccinctderivationofthebackpropagationalgorithminAlgorithm4\nwithoutworkingwiththecomplexindices,westatetheextensionsofthe\nchainruleinvectorizednotations.Itrequiresmoreknowledgeofmatrix\ncalculustostatethemostgeneralresult,andthereforewewillintroduce\nafewspecialcasesthataremostrelevantfordeeplearning.Suppose\nJ\nisareal-valuedoutputvariable,\nz\n2\nR\nm\nistheintermediatevariableand\nW\n2\nR\nm\n\nd\n;u\n2\nR\nd\naretheinputvariables.Supposetheysatisfy:\nz\n=\nWu\n+\nb;\nwhere\nW\n2\nR\nm\n\nd\nJ\n=\nJ\n(\nz\n)(7.37)\nThenwecancompute\n@J\n@u\nand\n@J\n@W\nby:\n@J\n@u\n=\nW\n>\n@J\n@z\n(7.38)\n@J\n@W\n=\n@J\n@z\n\nu\n>\n(7.39)\n@J\n@b\n=\n@J\n@z\n(7.40)\nWecanverifythedimensionalityisindeedcompatiblebecause\n@J\n@z\n2\nR\nm\n,\nW\n>\n2\nR\nd\n\nm\n,\n@J\n@u\n2\nR\nd\n,\n@J\n@W\n2\nR\nm\n\nd\n,\nu\n>\n2\nR\n1\n\nd\n.\nHerethechainruleinequation(7.38)onlyworksforthespecialcases\nwhere\nz\n=\nWu\n.Anotherusefulcaseisthefollowing:\na\n=\n\u02d9\n(\nz\n)\n;\nwhere\n\u02d9\nisanelement-wiseactivation,\nz;a\n2\nR\nd\nJ\n=\nJ\n(\na\n)\nThen,wehavethat\n@J\n@z\n=\n@J\n@a\n\n\u02d9\n0\n(\nz\n)(7.41)\nwhere\n\u02d9\n0\n(\n\n)istheelement-wisederivativeoftheactivationfunction\n\u02d9\n,and\n\niselement-wiseproductoftwovectorsofthesamedimensionality.\nUsingequation(7.38),(7.39),and(7.41),wecanverifythecorrectnessof\nAlgorithm4.Indeed,usingthenotationsinthetwo-layerneuralnetwork\n@J\n@z\n=\n@J\n@a\n\nReLU\n0\n(\nz\n)(\nbyinvokingequation(7.41)withsetting\nJ\n \nJ\n,\na\n \na\n,\nz\n \na\n,\n\u02d9\n \nReLU.\n)\n=(\no\n\ny\n)\nW\n[2]\n>\n\nReLU\n0\n(\nz\n)(\nbyinvokingequation(7.38)withsetting\nJ\n \nJ\n,\nz\n \no\n,\nW\n \nW\n[2]\n,\nu\n \na\n,\nb\n \nb\n[2]\n)\n"}, {"page_number": 99, "text": "99\nTherefore,\n\n[1]\n=\n@J\n@z\n,andweverifythecorrectnessofLine3inAlgorithm4.\nSimilarly,let'sverifythethirdequationinLine4,\n@J\n@W\n[1]\n=\n@J\n@z\n\nx\n>\n(\nbyinvokingequation(7.39)withsetting\nJ\n \nJ\n,\nz\n \nz\n,\nW\n \nW\n[1]\n,\nu\n \nx\n,\nb\n \nb\n[1]\n)\n=\n\n[1]\nx\n>\n(becausewehaveproved\n\n[1]\n=\n@J\n@z\n)\n7.3.5Multi-layerneuralnetworks\nInthissection,wewillderivethebackpropagationalgorithmsforthemodel\nin(7.17).Recallthatwehave\na\n[1]\n=ReLU(\nW\n[1]\nx\n+\nb\n[1]\n)\na\n[2]\n=ReLU(\nW\n[2]\na\n[1]\n+\nb\n[2]\n)\n\na\n[\nr\n\n1]\n=ReLU(\nW\n[\nr\n\n1]\na\n[\nr\n\n2]\n+\nb\n[\nr\n\n1]\n)\na\n[\nr\n]\n=\nz\n[\nr\n]\n=\nW\n[\nr\n]\na\n[\nr\n\n1]\n+\nb\n[\nr\n]\nJ\n=\n1\n2\n(\na\n[\nr\n]\n\ny\n)\n2\nHereweboth\na\n[\nr\n]\nand\nz\n[\nr\n]\nas\nh\n\n(\nx\n)fornotationalsimplicity.\n\n\n[\nk\n]\n=\n@J\n@z\n[\nk\n]\n(7.42)\nThebackpropagationalgorithmcomputes\n\n[\nk\n]\n'sfrom\nk\n=\nr\nto1,and\ncomputes\n@J\n@W\n[\nk\n]\nfrom\n\n[\nk\n]\nasdescribedinAlgorithm5.\n7.4Vectorizationovertrainingexamples\nAswediscussedinSection7.1,intheimplementationofneuralnetworks,\nwewillleveragetheparallelismacrossthemultipleexamples.Thismeans\nthatwewillneedtowritetheforwardpass(theevaluationoftheoutputs)\noftheneuralnetworkandthebackwardpass(backpropagation)formultiple\ntrainingexamplesinmatrixnotation.\nThebasicidea.\nThebasicideaissimple.Supposeyouhaveatraining\nsetwiththreeexamples\nx\n(1)\n;x\n(2)\n;x\n(3)\n.Theyeractivationsforeach\n"}, {"page_number": 100, "text": "100\nAlgorithm5\nBack-propagationformulti-layerneuralnetworks.\n.\n1:\nComputeandstorethevaluesof\na\n[\nk\n]\n'sand\nz\n[\nk\n]\n'sfor\nk\n=1\n;:::;r\n\n1,and\nJ\n.\n.\nThisisoftencalledthe\\forwardpass\"\n2:\nCompute\n\n[\nr\n]\n=\n@J\n@z\n[\nr\n]\n=(\nz\n[\nr\n]\n\no\n).\n3:\nfor\nk\n=\nr\n\n1to1\ndo\n4:\nCompute\n\n[\nk\n]\n=\n@J\n@z\n[\nk\n]\n=\n\nW\n[\nk\n+1]\n>\n\n[\nk\n+1]\n\n\nReLU\n0\n(\nz\n[\nk\n]\n)\n5:\nCompute\n@J\n@W\n[\nk\n+1]\n=\n\n[\nk\n+1]\na\n[\nk\n]\n>\n@J\n@b\n[\nk\n+1]\n=\n\n[\nk\n+1]\nexampleareasfollows:\nz\n[1](1)\n=\nW\n[1]\nx\n(1)\n+\nb\n[1]\nz\n[1](2)\n=\nW\n[1]\nx\n(2)\n+\nb\n[1]\nz\n[1](3)\n=\nW\n[1]\nx\n(3)\n+\nb\n[1]\nNotethebetweensquarebrackets[\n\n],whichrefertothelayernum-\nber,andparenthesis(\n\n),whichrefertothetrainingexamplenumber.In-\ntuitively,onewouldimplementthisusingaforloop.Itturnsout,wecan\nvectorizetheseoperationsaswell.First,\nX\n=\n2\n4\njjj\nx\n(1)\nx\n(2)\nx\n(3)\njjj\n3\n5\n2\nR\nd\n\n3\n(7.43)\nNotethatwearestackingtrainingexamplesincolumnsand\nnot\nrows.We\ncanthencombinethisintoasingleformulation:\nZ\n[1]\n=\n2\n4\njjj\nz\n[1](1)\nz\n[1](2)\nz\n[1](3)\njjj\n3\n5\n=\nW\n[1]\nX\n+\nb\n[1]\n(7.44)\nYoumaynoticethatweareattemptingtoadd\nb\n[1]\n2\nR\n4\n\n1\nto\nW\n[1]\nX\n2\nR\n4\n\n3\n.Strictlyfollowingtherulesoflinearalgebra,thisisnotallowed.In\n"}, {"page_number": 101, "text": "101\npracticehowever,thisadditionisperformedusing\nbroadcasting\n.Wecreate\nanintermediate\n~\nb\n[1]\n2\nR\n4\n\n3\n:\n~\nb\n[1]\n=\n2\n4\njjj\nb\n[1]\nb\n[1]\nb\n[1]\njjj\n3\n5\n(7.45)\nWecanthenperformthecomputation:\nZ\n[1]\n=\nW\n[1]\nX\n+\n~\nb\n[1]\n.Oftentimes,it\nisnotnecessarytoexplicitlyconstruct\n~\nb\n[1]\n.Byinspectingthedimensionsin\n(7.44),youcanassume\nb\n[1]\n2\nR\n4\n\n1\niscorrectlybroadcastto\nW\n[1]\nX\n2\nR\n4\n\n3\n.\nThematricizationapproachasabovecaneasilygeneralizetomultiple\nlayers,withonesubtletythough,asdiscussedbelow.\nComplications/SubtletyintheImplementation.\nAllthedeeplearn-\ningpackagesorimplementationsputthedatapointsintherowsofadata\nmatrix.(Ifthedatapointitselfisamatrixortensor,thenthedataarecon-\ncentratedalongthezero-thdimension.)However,mostofthedeeplearning\npapersuseasimilarnotationtothesenoteswherethedatapointsaretreated\nascolumnvectors.\n7\nThereisasimpleconversiontodealwiththemismatch:\nintheimplementation,allthecolumnsbecomerowvectors,rowvectorsbe-\ncomecolumnvectors,allthematricesaretransposed,andtheordersofthe\nmatrixmultiplicationsareed.Intheexampleabove,usingtherowma-\njorconvention,thedatamatrixis\nX\n2\nR\n3\n\nd\n,thelayerweightmatrix\nhasdimensionality\nd\n\nm\n(insteadof\nm\n\nd\nasinthetwolayerneuralnet\nsection),andthebiasvector\nb\n[1]\n2\nR\n1\n\nm\n.Thecomputationforthehidden\nactivationbecomes\nZ\n[1]\n=\nXW\n[1]\n+\nb\n[1]\n2\nR\n3\n\nm\n(7.46)\n7\nTheinstructorsuspectsthatthisismostlybecauseinmathematicswenaturallymul-\ntiplyamatrixtoavectoronthelefthandside.\n"}, {"page_number": 102, "text": "PartIII\nGeneralizationand\nregularization\n102\n"}, {"page_number": 103, "text": "Chapter8\nGeneralization\nThischapterdiscussestoolstoanalyzeandunderstandthegeneraliza-\ntionofmachinelearningmodels,i.e,theirperformancesonunseentest\nexamples.Recallthatforsupervisedlearningproblems,givenatrain-\ningdataset\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n)\ng\nn\ni\n=1\n,wetypicallylearnamodel\nh\n\nbyminimizinga\nloss/costfunction\nJ\n(\n\n),whichencourages\nh\n\ntothedata.E.g.,when\nthelossfunctionistheleastsquareloss(akameansquarederror),wehave\nJ\n(\n\n)=\n1\nn\nP\nn\ni\n=1\n(\ny\n(\ni\n)\n\nh\n\n(\nx\n(\ni\n)\n))\n2\n.Thislossfunctionfortrainingpurposesis\noftentimesreferredtoasthe\ntraining\nloss/error/cost.\nHowever,minimizingthetraininglossis\nnot\nourultimategoal|itis\nmerelyourapproachtowardsthegoaloflearningapredictivemodel.The\nmostimportantevaluationmetricofamodelisthelossonunseentestexam-\nples,whichisoftentimesreferredtoasthetesterror.Formally,wesamplea\ntestexample(\nx;y\n)fromtheso-calledtestdistribution\nD\n,andmeasurethe\nmodel'serroronit,by,e.g.,themeansquarederror,(\nh\n\n(\nx\n)\n\ny\n)\n2\n.Theex-\npectedloss/errorovertherandomnessofthetestexampleiscalledthetest\nloss/error,\n1\nL\n(\n\n)=\nE\n(\nx;y\n)\n\u02d8D\n[(\ny\n\nh\n\n(\nx\n))\n2\n](8.1)\nNotethatthemeasurementoftheerrorinvolvescomputingtheexpectation,\nandinpractice,itcanbeapproximatedbytheaverageerroronmanysampled\ntestexamples,whicharereferredtoasthetestdataset.Notethatthekey\nherebetweentrainingandtestdatasetsisthatthetestexamples\n1\nIntheoreticalandstatisticalliterature,weoftentimescalltheuniformdistribution\noverthetrainingset\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n)\ng\nn\ni\n=1\n,denotedby\nb\nD\n,anempiricaldistribution,andcall\nD\nthepopulationdistribution.Partlybecauseofthis,thetraininglossisalsoreferred\ntoastheempiricalloss/risk/error,andthetestlossisalsoreferredtoasthepopulation\nloss/risk/error.\n103\n"}, {"page_number": 104, "text": "104\nare\nunseen\n,inthesensethatthetrainingprocedurehasnotusedthetest\nexamples.Inclassicalstatisticallearningsettings,thetrainingexamplesare\nalsodrawnfromthesamedistributionasthetestdistribution\nD\n,butstill\nthetestexamplesareunseenbythelearningprocedurewhereasthetraining\nexamplesareseen.\n2\nBecauseofthiskeybetweentrainingandtestdatasets,even\niftheyarebothdrawnfromthesamedistribution\nD\n,thetesterrorisnot\nnecessarilyalwaysclosetothetrainingerror.\n3\nAsaresult,successfullymin-\nimizingthetrainingerrormaynotalwaysleadtoasmalltesterror.We\ntypicallysaythemodel\nov\nthedataifthemodelpredictsaccuratelyon\nthetrainingdatasetbutdoesn'tgeneralizewelltoothertestexamples,that\nis,ifthetrainingerrorissmallbutthetesterrorislarge.Wesaythemodel\n\nthedataifthetrainingerrorisrelativelylarge\n4\n(andinthiscase,\ntypicallythetesterrorisalsorelativelylarge.)\nThischapterstudieshowthetesterrorisdbythelearningpro-\ncedure,especiallythechoiceofmodelparameterizations.Wewilldecompose\nthetesterrorinto\\bias\"and\\variance\"termsandstudyhoweachofthemis\nbythechoiceofmodelparameterizationsandtheirUsing\nthebias-variancetrwewilldiscusswhenovand\nwilloccurandbeavoided.Wewillalsodiscussthedoubledescentphe-\nnomenoninSection8.2andsomeclassicaltheoreticalresultsinSection8.3.\n2\nThesedays,researchershaveincreasinglybeenmoreinterestedinthesettingwith\n\\domainshift\",thatis,thetrainingdistributionandtestdistributionaret.\n3\nthebetweentesterrorandtrainingerrorisoftenreferredtoasthegener-\nalizationgap.Theterm\ngeneralizationerror\ninsomeliteraturemeansthetesterror,and\ninsomeotherliteraturemeansthegeneralizationgap.\n4\ne.g.,largerthantheintrinsicnoiselevelofthedatainregressionproblems.\n"}, {"page_number": 105, "text": "105\n8.1Bias-variance\nFigure8.1:Arunningexampleoftrainingandtestdatasetforthissection.\nAsanillustratingexample,weconsiderthefollowingtrainingdatasetand\ntestdataset,whicharealsoshowninFigure8.1.Thetraininginputs\nx\n(\ni\n)\n'sare\nrandomlychosenandtheoutputs\ny\n(\ni\n)\naregeneratedby\ny\n(\ni\n)\n=\nh\n?\n(\nx\n(\ni\n)\n)+\n\u02d8\n(\ni\n)\nwherethefunction\nh\n?\n(\n\n)isaquadraticfunctionandisshowninFigure8.1\nasthesolidline,and\n\u02d8\n(\ni\n)\nistheaobservationnoiseassumedtobegenerated\nfrom\n\u02d8\nN\n(0\n;\u02d9\n2\n).Atestexample(\nx;y\n)alsohasthesameinput-output\nrelationship\ny\n=\nh\n?\n(\nx\n)+\n\u02d8\nwhere\n\u02d8\n\u02d8\nN\n(0\n;\u02d9\n2\n).It'simpossibletopredictthe\nnoise\n\u02d8\n,andthereforeessentiallyourgoalistorecoverthefunction\nh\n?\n(\n\n).\nWewillconsiderthetesterroroflearningvarioustypesofmodels.When\ntalkingaboutlinearregression,wediscussedtheproblemofwhetherto\na\\simple\"modelsuchasthelinear\\\ny\n=\n\n0\n+\n\n1\nx\n,\"oramore\\complex\"\nmodelsuchasthepolynomial\\\ny\n=\n\n0\n+\n\n1\nx\n+\n\n\n5\nx\n5\n.\"\nWestartwithalinearmodel,asshowninFigure8.2.Thebest\nlinearmodelcannotpredict\ny\nfrom\nx\naccuratelyevenonthetraining\ndataset,letaloneonthetestdataset.Thisisbecausethetruerelationship\nbetween\ny\nand\nx\nisnotlinear|anylinearmodelisfarawayfromthetrue\nfunction\nh\n?\n(\n\n).Asaresult,thetrainingerrorislargeandthisisatypical\nsituationof\n\n.\n"}, {"page_number": 106, "text": "106\nFigure8.2:Thebestlinearmodelhaslargetrainingandtesterrors.\nTheissuecannotbemitigatedwithmoretrainingexamples|evenwith\naverylargeamountof,oreventrainingexamples,thebest\nlinearmodelisstillinaccurateandfailstocapturethestructureofthedata\n(Figure8.3).Evenifthenoiseisnotpresentinthetrainingdata,theissue\nstilloccurs(Figure8.4).Therefore,thefundamentalbottleneckhereisthe\nlinearmodelfamily'sinabilitytocapturethestructureinthedata|linear\nmodelscannotrepresentthetruequadraticfunction\nh\n?\n|,butnotthelackof\nthedata.Informally,wethe\nbias\nofamodeltobethetesterroreven\nifweweretotittoavery(say,)largetrainingdataset.Thus,in\nthiscase,thelinearmodelfromlargebias,and(i.e.,failsto\ncapturestructureexhibitedby)thedata.\nFigure8.3:Thebestlinear\nmodelonamuchlargerdataset\nstillhasalargetrainingerror.\nFigure8.4:Thebestlinear\nmodelonanoiselessdatasetalso\nhasalargetraining/testerror.\nNext,wea5th-degreepolynomialtothedata.Figure8.5showsthat\nitfailstolearnagoodmodeleither.However,thefailurepatternist\nfromthelinearmodelcase.Speci,eventhoughthelearnt5th-degree\n"}, {"page_number": 107, "text": "107\npolynomialdidaverygoodjobpredicting\ny\n(\ni\n)\n'sfrom\nx\n(\ni\n)\n'sfortrainingex-\namples,itdoesnotworkwellontestexamples(Figure8.5).Inotherwords,\nthemodellearntfromthetrainingsetdoesnot\ngeneralize\nwelltoothertest\nexamples|thetesterrorishigh.Contrarytothebehavioroflinearmodels,\nthebiasofthe5-thdegreepolynomialsissmall|ifweweretoa5-thde-\ngreepolynomialtoanextremelylargedataset,theresultingmodelwouldbe\nclosetoaquadraticfunctionandbeaccurate(Figure8.6).Thisisbecause\nthefamilyof5-thdegreepolynomialscontainsallthequadraticfunctions\n(setting\n\n5\n=\n\n4\n=\n\n3\n=0resultsinaquadraticfunction),and,therefore,\n5-thdegreepolynomialsareinprinciplecapableofcapturingthestructure\nofthedata.\nFigure8.5:Best5-thdegreepolynomialhaszerotrainingerror,butstill\nhasalargetesterroranddoesnotrecoverthethegroundtruth.Thisisa\nclassicsituationofove\nFigure8.6:Thebest5-thdegreepolynomialonahugedatasetnearly\nrecoverstheground-truth|suggestingthattheculpritinFigure8.5isthe\nvariance(orlackofdata)butnotbias.\nThefailureof5-thdegreepolynomialscanbecapturedbyanother\n"}, {"page_number": 108, "text": "108\ncomponentofthetesterror,called\nvariance\nofamodelprocedure.\nSp,whena5-thdegreepolynomialasinFigure8.7,thereisa\nlargeriskthatwe'repatternsinthedatathathappenedtobepresent\ninour\nsmall,\ntrainingset,butthatdonotthewiderpatternof\ntherelationshipbetween\nx\nand\ny\n.These\\spurious\"patternsinthetraining\nsetare(mostly)duetotheobservationnoise\n\u02d8\n(\ni\n)\n,andthesespurious\npattersresultsinamodelwithlargetesterror.Inthiscase,wesaythemodel\nhasalargevariance.\nFigure8.7:Thebest5-thdegreemodelsonthreeditdatasetsgen-\neratedfromthesamedistributionbehavequitetly,suggestingthe\nexistenceofalargevariance.\nThevariancecanbeintuitively(andmathematically,asshowninSec-\ntion8.1.1)characterizedbytheamountofvariationsacrossmodelslearnt\nonmultipleerenttrainingdatasets(drawnfromthesameunderlyingdis-\ntribution).The\\spuriouspatterns\"aresptotherandomnessofthe\nnoise(andinputs)inaparticulardataset,andthusaretacrossmul-\ntipletrainingdatasets.Therefore,ovtothe\\spuriouspatterns\"of\nmultipledatasetsshouldresultinveryntmodels.Indeed,asshown\ninFigure8.7,themodelslearnedonthethreettrainingdatasetsare\nquitet,ovtothe\\spuriouspatterns\"ofeachdatasets.\nOften,thereisabetweenbiasandvariance.Ifourmodelistoo\n\\simple\"andhasveryfewparameters,thenitmayhavelargebias(butsmall\nvariance),andittypicallymayfromIfitistoo\\complex\"\nandhasverymanyparameters,thenitmayfromlargevariance(but\nhavesmallerbias),andthusovSeeFigure8.8foratypical\nbetweenbiasandvariance.\n"}, {"page_number": 109, "text": "109\nFigure8.8:Anillustrationofthetypicalbias-variance\nAswewillseeformallyinSection8.1.1,thetesterrorcanbedecomposed\nasasummationofbiasandvariance.Thismeansthatthetesterrorwill\nhaveaconvexcurveasthemodelcomplexityincreases,andinpracticewe\nshouldtunethemodelcomplexitytoachievethebestForinstance,\nintheexampleabove,gaquadraticfunctiondoesbetterthaneitherof\ntheextremesofaora5-thdegreepolynomial,asshowninFigure8.9.\nFigure8.9:Bestquadraticmodelhassmalltrainingandtesterrorbecause\nquadraticmodelachievesabetter\nInterestingly,thebias-variancecurvesorthetesterrorcurves\ndonotuniversallyfollowtheshapeinFigure8.8,atleastnotuniversally\nwhenthemodelcomplexityissimplymeasuredbythenumberofparameters.\n(Wewilldiscusstheso-calleddoubledescentphenomenoninSection8.2.)\nNevertheless,theprincipleofbias-varianceisperhapsstillthe\nresortwhenanalyzingandpredictingthebehavioroftesterrors.\n"}, {"page_number": 110, "text": "110\n8.1.1Amathematicaldecomposition(forregression)\nToformallystatethebias-varianceforregressionproblems,wecon-\nsiderthefollowingsetup(whichisanextensionofthebeginningparagraph\nofSection8.1).\n\u2039\nDrawatrainingdataset\nS\n=\nf\nx\n(\ni\n)\n;y\n(\ni\n)\ng\nn\ni\n=1\nsuchthat\ny\n(\ni\n)\n=\nh\n?\n(\nx\n(\ni\n)\n)+\n\u02d8\n(\ni\n)\nwhere\n\u02d8\n(\ni\n)\n2\nN\n(0\n;\u02d9\n2\n).\n\u2039\nTrainamodelonthedataset\nS\n,denotedby\n^\nh\nS\n.\n\u2039\nTakeatestexample(\nx;y\n)suchthat\ny\n=\nh\n?\n(\nx\n)+\n\u02d8\nwhere\n\u02d8\n\u02d8\nN\n(0\n;\u02d9\n2\n),\nandmeasuretheexpectedtesterror(averagedovertherandomdrawof\nthetrainingset\nS\nandtherandomnessof\n\u02d8\n)\n56\nMSE(\nx\n)=\nE\nS;\u02d8\n[(\ny\n\nh\nS\n(\nx\n))\n2\n](8.2)\nWewilldecomposetheMSEintoabiasandvarianceterm.Westartby\nstatingafollowingsimplemathematicaltoolthatwillbeusedtwicebelow.\nClaim8.1.1\n:\nSuppose\nA\nand\nB\naretwoindependentrealrandomvariables\nand\nE\n[\nA\n]=0.Then,\nE\n[(\nA\n+\nB\n)\n2\n]=\nE\n[\nA\n2\n]+\nE\n[\nB\n2\n].\nAsacorollary,becausearandomvariable\nA\nisindependentwithacon-\nstant\nc\n,when\nE\n[\nA\n]=0,wehave\nE\n[(\nA\n+\nc\n)\n2\n]=\nE\n[\nA\n2\n]+\nc\n2\n.\nTheproofoftheclaimfollowsfromexpandingthesquare:\nE\n[(\nA\n+\nB\n)\n2\n]=\nE\n[\nA\n2\n]+\nE\n[\nB\n2\n]+2\nE\n[\nAB\n]=\nE\n[\nA\n2\n]+\nE\n[\nB\n2\n].Hereweusedtheindependenceto\nshowthat\nE\n[\nAB\n]=\nE\n[\nA\n]\nE\n[\nB\n]=0.\nUsingClaim8.1.1with\nA\n=\n\u02d8\nand\nB\n=\nh\n?\n(\nx\n)\n\n^\nh\nS\n(\nx\n),wehave\nMSE(\nx\n)=\nE\n[(\ny\n\nh\nS\n(\nx\n))\n2\n]=\nE\n[(\n\u02d8\n+(\nh\n?\n(\nx\n)\n\nh\nS\n(\nx\n)))\n2\n](8.3)\n=\nE\n[\n\u02d8\n2\n]+\nE\n[(\nh\n?\n(\nx\n)\n\nh\nS\n(\nx\n))\n2\n](byClaim8.1.1)\n=\n\u02d9\n2\n+\nE\n[(\nh\n?\n(\nx\n)\n\nh\nS\n(\nx\n))\n2\n](8.4)\nThen,let's\nh\navg\n(\nx\n)=\nE\nS\n[\nh\nS\n(\nx\n)]asthe\\averagemodel\"|themodel\nobtainedbydrawingannumberofdatasets,trainingonthem,and\naveragingtheirpredictionson\nx\n.Notethat\nh\navg\nisahypotheticalmodelfor\nanalyticalpurposesthatcannotbeobtainedinreality(becausewedon't\n5\nForsimplicity,thetestinput\nx\nisconsideredtobehere,butthesameconceptual\nmessageholdswhenweaverageoverthechoiceof\nx\n's.\n6\nThesubscriptundertheexpectationsymbolistoemphasizethevariablesthatare\nconsideredasrandombytheexpectationoperation.\n"}, {"page_number": 111, "text": "111\nhavenumberofdatasets).Itturnsoutthatformanycases,\nh\navg\nis(approximately)equaltothethemodelobtainedbytrainingona\nsingle\ndatasetwithsamples.Thus,wecanalsointuitivelyinterpret\nh\navg\nthis\nway,whichisconsistentwithourintuitivenofbiasintheprevious\nsubsection.\nWecanfurtherdecomposeMSE(\nx\n)byletting\nc\n=\nh\n?\n(\nx\n)\n\nh\navg\n(\nx\n)(whichis\naconstantthatdoesnotdependonthechoiceof\nS\n!)and\nA\n=\nh\navg\n(\nx\n)\n\nh\nS\n(\nx\n)\ninthecorollarypartofClaim8.1.1:\nMSE(\nx\n)=\n\u02d9\n2\n+\nE\n[(\nh\n?\n(\nx\n)\n\nh\nS\n(\nx\n))\n2\n](8.5)\n=\n\u02d9\n2\n+(\nh\n?\n(\nx\n)\n\nh\navg\n(\nx\n))\n2\n+\nE\n[(\nh\navg\n\nh\nS\n(\nx\n))\n2\n](8.6)\n=\n\u02d9\n2\n|{z}\nunavoidable\n+(\nh\n?\n(\nx\n)\n\nh\navg\n(\nx\n))\n2\n|\n{z\n}\n,\nbias\n2\n+var(\nh\nS\n(\nx\n))\n|\n{z\n}\n,\nvariance\n(8.7)\nWecallthesecondtermthebias(square)andthethirdtermthevariance.As\ndiscussedbefore,thebiascapturesthepartoftheerrorthatareintroduced\nduetothelackofexpressivityofthemodel.Recallthat\nh\navg\ncanbethought\nofasthebestpossiblemodellearnedevenwithdata.Thus,thebiasis\nnotduetothelackofdata,butisrathercausedbythatthefamilyofmodels\nfundamentallycannotapproximatethe\nh\n?\n.Forexample,intheillustrating\nexampleinFigure8.2,becauseanylinearmodelcannotapproximatethe\ntruequadraticfunction\nh\n?\n,neithercan\nh\navg\n,andthusthebiastermhasto\nbelarge.\nThevariancetermcaptureshowtherandomnatureofthedataset\nintroduceserrorsinthelearnedmodel.Itmeasuresthesensitivityofthe\nlearnedmodeltotherandomnessinthedataset.Itoftendecreasesasthe\nsizeofthedatasetincreases.\nThereisnothingwecandoabouttherstterm\n\u02d9\n2\naswecannotpredict\nthenoise\n\u02d8\nby\nFinally,wenotethatthebias-variancedecompositionfor\nismuchlessclearthanforregressionproblems.Therehavebeenseveral\nproposals,butthereisasyetnoagreementonwhatisthe\\right\"and/or\nthemostusefulformalism.\n8.2Thedoubledescentphenomenon\nModel-wisedoubledescent.\nRecentworkshavedemonstratedthatthe\ntesterrorcanpresenta\\doubledescent\"phenomenoninarangeofmachine\n"}, {"page_number": 112, "text": "112\nlearningmodelsincludinglinearmodelsanddeepneuralnetworks.\n7\nThe\nconventionalwisdom,asdiscussedinSection8.1,isthatasweincreasethe\nmodelcomplexity,thetesterrordecreasesandthenincreases,asillus-\ntratedinFigure8.8.However,inmanycases,weempiricallyobservethat\nthetesterrorcanhaveaseconddescent|itdecreases,thenincreases\ntoapeakaroundwhenthemodelsizeislargeenoughtoallthetraining\ndataverywell,andthendecreasesagainintheso-calledoverparameterized\nregime,wherethenumberofparametersislargerthanthenumberofdata\npoints.SeeFigure8.10foranillustrationofthetypicalcurvesoftesterrors\nagainstmodelcomplexity(measuredbythenumberofparameters).Tosome\nextent,theoverparameterizedregimewiththeseconddescentisconsideredas\nnewtothemachinelearningcommunity|partlybecauselightly-regularized,\noverparameterizedmodelsareonlyextensivelyusedinthedeeplearningera.\nApracticalimplicationofthephenomenonisthatoneshouldnotholdback\nfromscalingintoandexperimentingwithover-parametrizedmodelsbecause\nthetesterrormaywelldecreaseagaintoalevelevensmallerthantheprevi-\nouslowestpoint.Actually,inmanycases,largeroverparameterizedmodels\nalwaysleadtoabettertestperformance(meaningtherewon'tbeasecond\nascentaftertheseconddescent).\nFigure8.10:Atypicalmodel-wisedoubledescentphenomenon.Asthenum-\nberofparametersincreases,thetesterrordecreaseswhenthenumberof\nparametersissmallerthanthetrainingdata.Thenintheoverparameterized\nregime,thetesterrordecreasesagain.\n7\nThediscoveryofthephenomenonperhapsdatesbacktoOpper[1995,2001],andhas\nbeenrecentlypopularizedbyBelkinetal.[2020],Hastieetal.[2019],etc.\n"}, {"page_number": 113, "text": "113\nSample-wisedoubledescent.\nApriori,wewouldexpectthatmore\ntrainingexamplesalwaysleadtosmallertesterrors|moresamplesgive\nstrictlymoreinformationforthealgorithmtolearnfrom.However,recent\nwork[Nakkiran,2019]observesthatthetesterrorisnotmonotonicallyde-\ncreasingasweincreasethesamplesize.Instead,asshowninFigure8.11,the\ntesterrordecreases,andthenincreasesandpeaksaroundwhenthenumber\nofexamples(denotedby\nn\n)issimilartothenumberofparameters(denoted\nby\nd\n),andthendecreasesagain.Werefertothisasthesample-wisedou-\nbledescentphenomenon.Tosomeextent,sample-wisedoubledescentand\nmodel-wisedoubledescentareessentiallydescribingsimilarphenomena|the\ntesterrorispeakedwhen\nn\n\u02c7\nd\n.\nExplanationandmitigationstrategy.\nThesample-wisedoubledescent,\nor,inparticular,thepeakoftesterrorat\nn\n\u02c7\nd\n,suggeststhattheexisting\ntrainingalgorithmsevaluatedintheseexperimentsarefarfromoptimalwhen\nn\n\u02c7\nd\n.Wewillbebetterbytossingawaysomeexamplesandrunthe\nalgorithmswithasmallersamplesizetosteerclearofthepeak.Inother\nwords,inprinciple,thereareotheralgorithmsthatcanachievesmallertest\nerrorwhen\nn\n\u02c7\nd\n,butthealgorithmsevaluatedintheseexperimentsfailto\ndoso.Thesub-optimalityofthelearningprocedureappearstobetheculprit\nofthepeakinbothsample-wiseandmodel-wisedoubledescent.\nIndeed,withanoptimally-tunedregularization(whichwillbediscussed\nmoreinSection9),thetesterrorinthe\nn\n\u02c7\nd\nregimecanbedramatically\nimproved,andthemodel-wiseandsample-wisedoubledescentarebothmit-\nigated.SeeFigure8.11.\nTheintuitionaboveonlyexplainsthepeakinthemodel-wiseandsample-\nwisedoubledescent,butdoesnotexplaintheseconddescentinthemodel-\nwisedoubledescent|whyoverparameterizedmodelsareabletogeneralize\nsowell.Thetheoreticalunderstandingofoverparameterizedmodelsisanac-\ntiveresearchareawithmanyrecentadvances.Atypicalexplanationisthat\nthecommonly-usedoptimizerssuchasgradientdescentprovideanimplicit\nregularization(whichwillbediscussedinmoredetailinSection9.2).\nInotherwords,evenintheoverparameterizedregimeandwithanunregular-\nizedlossfunction,themodelisstillimplicitlyregularized,andthusexhibits\nabettertestperformancethananarbitrarysolutionthatthedata.For\nexample,forlinearmodels,when\nn\n\u02dd\nd\n,thegradientdescentoptimizerwith\nzeroinitializationthe\nminimumnorm\nsolutionthatthedata(in-\nsteadofanarbitrarysolutionthatthedata),andtheminimumnormreg-\nularizerturnsouttobeatlygoodfortheoverparameterizedregime\n(butit'snotagoodregularizerwhen\nn\n\u02c7\nd\n,resultinginthepeakoftest\n"}, {"page_number": 114, "text": "114\nerror).\nFigure8.11:\nLeft:\nThesample-wisedoubledescentphenomenonforlinear\nmodels.\nRight:\nThesample-wisedoubledescentwithtregularization\nstrengthforlinearmodels.Usingtheoptimalregularizationparameter\n\n(optimallytunedforeach\nn\n,showningreensolidcurve)mitigatesdouble\ndescent.\nSetup:\nThedatadistributionof(\nx;y\n)is\nx\n\u02d8N\n(0\n;I\nd\n)and\ny\n\u02d8\nx\n>\n\n+\nN\n(0\n;\u02d9\n2\n)where\nd\n=500\n;\u02d9\n=0\n:\n5and\nk\n\nk\n2\n=1.\n8\nFinally,wealsoremarkthatthedoubledescentphenomenonhasbeen\nmostlyobservedwhenthemodelcomplexityismeasuredbythenumberof\nparameters.Itisunclearifandwhenthenumberofparametersisthebest\ncomplexitymeasureofamodel.Forexample,inmanysituations,thenorm\nofthemodelsisusedasacomplexitymeasure.AsshowninFigure8.12\nright,foraparticularlinearcase,ifweplotthetesterroragainstthenorm\nofthelearntmodel,thedoubledescentphenomenonnolongeroccurs.This\nispartlybecausethenormofthelearnedmodelisalsopeakedaround\nn\n\u02c7\nd\n(SeeFigure8.12(middle)orBelkinetal.[2019],MeiandMontanari[2022],\nanddiscussionsinSection10.8ofJamesetal.[2021]).Fordeepneural\nnetworks,thecorrectcomplexitymeasureisevenmoreelusive.Thestudyof\ndoubledescentphenomenonisanactiveresearchtopic.\n8\nTheisreproducedfromFigure1ofNakkiranetal.[2020].Similarphenomenon\narealsoobservedinHastieetal.[2022],MeiandMontanari[2022]\n"}, {"page_number": 115, "text": "115\nFigure8.12:\nLeft:\nThedoubledescentphenomenon,wherethenumberofpa-\nrametersisusedasthemodelcomplexity.\nMiddle:\nThenormofthelearned\nmodelispeakedaround\nn\n\u02c7\nd\n.\nRight:\nThetesterroragainstthenormof\nthelearntmodel.Thecolorbarindicatethenumberofparametersandthe\narrowsindicatesthedirectionofincreasingmodelsize.Theirrelationship\nareclosertotheconventionwisdomthantoadoubledescent.\nSetup:\nWe\nconsideralinearregressionwithadatasetofsize\nn\n=500\n:\nTheinput\nx\nisarandomReLUfeatureonFashion-MNIST,andoutput\ny\n2\nR\n10\nisthe\none-hotlabel.ThisisthesamesettingasinSection5.2ofNakkiranetal.\n[2020].\n"}, {"page_number": 116, "text": "116\n8.3Samplecomplexitybounds(optional\nreadings)\n8.3.1Preliminaries\nInthissetofnotes,webeginourforayintolearningtheory.Apartfrom\nbeinginterestingandenlighteninginitsownright,thisdiscussionwillalso\nhelpushoneourintuitionsandderiverulesofthumbabouthowtobest\napplylearningalgorithmsintsettings.Wewillalsoseektoanswer\nafewquestions:First,canwemakeformalthebias/variancethat\nwasjustdiscussed?Thiswillalsoeventuallyleadustotalkaboutmodel\nselectionmethods,whichcan,forinstance,automaticallydecidewhatorder\npolynomialtotoatrainingset.Second,inmachinelearningit'sreally\ngeneralizationerrorthatwecareabout,butmostlearningalgorithmstheir\nmodelstothetrainingset.Whyshoulddoingwellonthetrainingsettellus\nanythingaboutgeneralizationerror?Sp,canwerelateerroronthe\ntrainingsettogeneralizationerror?Thirdand,arethereconditions\nunderwhichwecanactuallyprovethatlearningalgorithmswillworkwell?\nWestartwithtwosimplebutveryusefullemmas.\nLemma.\n(Theunionbound).Let\nA\n1\n;A\n2\n;:::;A\nk\nbe\nk\ntevents(that\nmaynotbeindependent).Then\nP\n(\nA\n1\n[[\nA\nk\n)\n\nP\n(\nA\n1\n)+\n:::\n+\nP\n(\nA\nk\n)\n:\nInprobabilitytheory,theunionboundisusuallystatedasanaxiom\n(andthuswewon'ttrytoproveit),butitalsomakesintuitivesense:The\nprobabilityofanyoneof\nk\neventshappeningisatmostthesumofthe\nprobabilitiesofthe\nk\ntevents.\nLemma.\n(Hoinequality)Let\nZ\n1\n;:::;Z\nn\nbe\nn\nindependentandiden-\nticallydistributed(iid)randomvariablesdrawnfromaBernoulli(\n\u02da\n)distri-\nbution.I.e.,\nP\n(\nZ\ni\n=1)=\n\u02da\n,and\nP\n(\nZ\ni\n=0)=1\n\n\u02da\n.Let\n^\n\u02da\n=(1\n=n\n)\nP\nn\ni\n=1\nZ\ni\nbethemeanoftheserandomvariables,andletany\n>\n0beThen\nP\n(\nj\n\u02da\n\n^\n\u02da\nj\n>\n)\n\n2exp(\n\n2\n\n2\nn\n)\nThislemma(whichinlearningtheoryisalsocalledthe\nbound\n)\nsaysthatifwetake\n^\n\u02da\n|theaverageof\nn\nBernoulli(\n\u02da\n)randomvariables|to\nbeourestimateof\n\u02da\n,thentheprobabilityofourbeingfarfromthetruevalue\nissmall,solongas\nn\nislarge.Anotherwayofsayingthisisthatifyouhave\nabiasedcoinwhosechanceoflandingonheadsis\n\u02da\n,thenifyoutossit\nn\n"}, {"page_number": 117, "text": "117\ntimesandcalculatethefractionoftimesthatitcameupheads,thatwillbe\nagoodestimateof\n\u02da\nwithhighprobability(if\nn\nislarge).\nUsingjustthesetwolemmas,wewillbeabletoprovesomeofthedeepest\nandmostimportantresultsinlearningtheory.\nTosimplifyourexposition,let'srestrictourattentiontobinaryclassi\ntioninwhichthelabelsare\ny\n2f\n0\n;\n1\ng\n.Everythingwe'llsayheregeneralizes\ntootherproblems,includingregressionandmulti-class\nWeassumewearegivenatrainingset\nS\n=\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n);\ni\n=1\n;:::;n\ng\nofsize\nn\n,wherethetrainingexamples(\nx\n(\ni\n)\n;y\n(\ni\n)\n)aredrawniidfromsomeprobability\ndistribution\nD\n.Forahypothesis\nh\n,wethe\ntrainingerror\n(alsocalled\nthe\nempiricalrisk\nor\nempiricalerror\ninlearningtheory)tobe\n^\n\"\n(\nh\n)=\n1\nn\nn\nX\ni\n=1\n1\nf\nh\n(\nx\n(\ni\n)\n)\n6\n=\ny\n(\ni\n)\ng\n:\nThisisjustthefractionoftrainingexamplesthat\nh\nWhenwe\nwanttomakeexplicitthedependenceof^\n\"\n(\nh\n)onthetrainingset\nS\n,wemay\nalsowritethisa^\n\"\nS\n(\nh\n).Wealsothegeneralizationerrortobe\n\"\n(\nh\n)=\nP\n(\nx;y\n)\n\u02d8D\n(\nh\n(\nx\n)\n6\n=\ny\n)\n:\nI.e.thisistheprobabilitythat,ifwenowdrawanewexample(\nx;y\n)from\nthedistribution\nD\n,\nh\nwillmisclassifyit.\nNotethatwehaveassumedthatthetrainingdatawasdrawnfromthe\nsame\ndistribution\nD\nwithwhichwe'regoingtoevaluateourhypotheses(in\ntheofgeneralizationerror).Thisissometimesalsoreferredtoas\noneofthe\nPAC\nassumptions.\n9\nConsiderthesettingoflinearandlet\nh\n\n(\nx\n)=1\nf\n\nT\nx\n\n0\ng\n.\nWhat'sareasonablewayofttingtheparameters\n\n?Oneapproachistotry\ntominimizethetrainingerror,andpick\n^\n\n=argmin\n\n^\n\"\n(\nh\n\n)\n:\nWecallthisprocess\nempiricalriskminimization\n(ERM),andtheresulting\nhypothesisoutputbythelearningalgorithmis\n^\nh\n=\nh\n^\n\n.WethinkofERM\nasthemost\\basic\"learningalgorithm,anditwillbethisalgorithmthatwe\n9\nPACstandsfor\\probablyapproximatelycorrect,\"whichisaframeworkandsetof\nassumptionsunderwhichnumerousresultsonlearningtheorywereproved.Ofthese,the\nassumptionoftrainingandtestingonthesamedistribution,andtheassumptionofthe\nindependentlydrawntrainingexamples,werethemostimportant.\n"}, {"page_number": 118, "text": "118\nfocusoninthesenotes.(Algorithmssuchaslogisticregressioncanalsobe\nviewedasapproximationstoempiricalriskminimization.)\nInourstudyoflearningtheory,itwillbeusefultoabstractawayfrom\nthespparameterizationofhypothesesandfromissuessuchaswhether\nwe'reusingalinearclasser.Wethe\nhypothesisclass\nH\nusedbya\nlearningalgorithmtobethesetofallconsideredbyit.Forlinear\n\nH\n=\nf\nh\n\n:\nh\n\n(\nx\n)=1\nf\n\nT\nx\n\n0\ng\n;\n2\nR\nd\n+1\ng\nisthusthesetof\nallover\nX\n(thedomainoftheinputs)wherethedecisionboundary\nislinear.Morebroadly,ifwewerestudying,say,neuralnetworks,thenwe\ncouldlet\nH\nbethesetofallrepresentablebysomeneuralnetwork\narchitecture.\nEmpiricalriskminimizationcannowbethoughtofasaminimizationover\ntheclassoffunctions\nH\n,inwhichthelearningalgorithmpicksthehypothesis:\n^\nh\n=argmin\nh\n2H\n^\n\"\n(\nh\n)\n8.3.2Thecaseof\nH\nLet'sstartbyconsideringalearningprobleminwhichwehaveahy-\npothesisclass\nH\n=\nf\nh\n1\n;:::;h\nk\ng\nconsistingof\nk\nhypotheses.Thus,\nH\nisjusta\nsetof\nk\nfunctionsmappingfrom\nX\nto\nf\n0\n;\n1\ng\n,andempiricalriskminimization\nselects\n^\nh\ntobewhicheverofthese\nk\nfunctionshasthesmallesttrainingerror.\nWewouldliketogiveguaranteesonthegeneralizationerrorof\n^\nh\n.Our\nstrategyfordoingsowillbeintwoparts:First,wewillshowthat^\n\"\n(\nh\n)isa\nreliableestimateof\n\"\n(\nh\n)forall\nh\n.Second,wewillshowthatthisimpliesan\nupper-boundonthegeneralizationerrorof\n^\nh\n.\nTakeanyone,\nh\ni\n2H\n.ConsideraBernoullirandomvariable\nZ\nwhosedistributionisasfollows.We'regoingtosample(\nx;y\n)\n\u02d8D\n.\nThen,weset\nZ\n=1\nf\nh\ni\n(\nx\n)\n6\n=\ny\ng\n.I.e.,we'regoingtodrawoneexample,\nandlet\nZ\nindicatewhether\nh\ni\nit.Similarly,wealso\nZ\nj\n=\n1\nf\nh\ni\n(\nx\n(\nj\n)\n)\n6\n=\ny\n(\nj\n)\ng\n.Sinceourtrainingsetwasdrawniidfrom\nD\n,\nZ\nandthe\nZ\nj\n'shavethesamedistribution.\nWeseethatthemprobabilityonarandomlydrawn\nexample|thatis,\n\"\n(\nh\n)|isexactlytheexpectedvalueof\nZ\n(and\nZ\nj\n).More-\nover,thetrainingerrorcanbewritten\n^\n\"\n(\nh\ni\n)=\n1\nn\nn\nX\nj\n=1\nZ\nj\n:\nThus,^\n\"\n(\nh\ni\n)isexactlythemeanofthe\nn\nrandomvariables\nZ\nj\nthataredrawn\niidfromaBernoullidistributionwithmean\n\"\n(\nh\ni\n).Hence,wecanapplythe\n"}, {"page_number": 119, "text": "119\nHoinequality,andobtain\nP\n(\nj\n\"\n(\nh\ni\n)\n\n^\n\"\n(\nh\ni\n)\nj\n>\n)\n\n2exp(\n\n2\n\n2\nn\n)\n:\nThisshowsthat,forourparticular\nh\ni\n,trainingerrorwillbecloseto\ngeneralizationerrorwithhighprobability,assuming\nn\nislarge.Butwedon't\njustwanttoguaranteethat\n\"\n(\nh\ni\n)willbecloseto^\n\"\n(\nh\ni\n)(withhighprobability)\nforjustonlyoneparticular\nh\ni\n.Wewanttoprovethatthiswillbetrue\nsimultaneouslyfor\nall\nh\n2H\n.Todoso,let\nA\ni\ndenotetheeventthat\nj\n\"\n(\nh\ni\n)\n\n^\n\"\n(\nh\ni\n)\nj\n>\n.We'vealreadyshownthat,foranyparticular\nA\ni\n,itholdstrue\nthat\nP\n(\nA\ni\n)\n\n2exp(\n\n2\n\n2\nn\n).Thus,usingtheunionbound,wehavethat\nP\n(\n9\nh\n2H\n:\nj\n\"\n(\nh\ni\n)\n\n^\n\"\n(\nh\ni\n)\nj\n>\n)=\nP\n(\nA\n1\n[[\nA\nk\n)\n\nk\nX\ni\n=1\nP\n(\nA\ni\n)\n\nk\nX\ni\n=1\n2exp(\n\n2\n\n2\nn\n)\n=2\nk\nexp(\n\n2\n\n2\nn\n)\nIfwesubtractbothsidesfrom1,wethat\nP\n(\n:9\nh\n2H\n:\nj\n\"\n(\nh\ni\n)\n\n^\n\"\n(\nh\ni\n)\nj\n>\n)=\nP\n(\n8\nh\n2H\n:\nj\n\"\n(\nh\ni\n)\n\n^\n\"\n(\nh\ni\n)\nj\n\n)\n\n1\n\n2\nk\nexp(\n\n2\n\n2\nn\n)\n(The\\\n:\n\"symbolmeans\\not.\")So,withprobabilityatleast1\n\n2\nk\nexp(\n\n2\n\n2\nn\n),wehavethat\n\"\n(\nh\n)willbewithin\n\nof^\n\"\n(\nh\n)forall\nh\n2H\n.\nThisiscalleda\nuniformconvergence\nresult,becausethisisaboundthat\nholdssimultaneouslyforall(asopposedtojustone)\nh\n2H\n.\nInthediscussionabove,whatwedidwas,forparticularvaluesof\nn\nand\n\n,giveaboundontheprobabilitythatforsome\nh\n2H\n,\nj\n\"\n(\nh\n)\n\n^\n\"\n(\nh\n)\nj\n>\n.\nTherearethreequantitiesofinteresthere:\nn\n,\n\n,andtheprobabilityoferror;\nwecanboundeitheroneintermsoftheothertwo.\nForinstance,wecanaskthefollowingquestion:Given\n\nandsome\n>\n0,\nhowlargemust\nn\nbebeforewecanguaranteethatwithprobabilityatleast\n1\n\n\n,trainingerrorwillbewithin\n\nofgeneralizationerror?Bysetting\n\n=2\nk\nexp(\n\n2\n\n2\nn\n)andsolvingfor\nn\n,[youshouldconvinceyourselfthisis\ntherightthingtodo!],wethatif\nn\n\n1\n2\n\n2\nlog\n2\nk\n\n;\n"}, {"page_number": 120, "text": "120\nthenwithprobabilityatleast1\n\n\n,wehavethat\nj\n\"\n(\nh\n)\n\n^\n\"\n(\nh\n)\nj\n\nforall\nh\n2H\n.(Equivalently,thisshowsthattheprobabilitythat\nj\n\"\n(\nh\n)\n\n^\n\"\n(\nh\n)\nj\n>\nforsome\nh\n2H\nisatmost\n\n.)Thisboundtellsushowmanytraining\nexamplesweneedinordermakeaguarantee.Thetrainingsetsize\nn\nthat\nacertainmethodoralgorithmrequiresinordertoachieveacertainlevelof\nperformanceisalsocalledthealgorithm's\nsamplecomplexity\n.\nThekeypropertyoftheboundaboveisthatthenumberoftraining\nexamplesneededtomakethisguaranteeisonly\nlogarithmic\nin\nk\n,thenumber\nofhypothesesin\nH\n.Thiswillbeimportantlater.\nSimilarly,wecanalsohold\nn\nand\n\nandsolvefor\n\nintheprevious\nequation,andshow[again,convinceyourselfthatthisisright!]thatwith\nprobability1\n\n\n,wehavethatforall\nh\n2H\n,\nj\n^\n\"\n(\nh\n)\n\n\"\n(\nh\n)\nj\nr\n1\n2\nn\nlog\n2\nk\n\n:\nNow,let'sassumethatuniformconvergenceholds,i.e.,that\nj\n\"\n(\nh\n)\n\n^\n\"\n(\nh\n)\nj\n\nforall\nh\n2H\n.Whatcanweproveaboutthegeneralizationofourlearning\nalgorithmthatpicked\n^\nh\n=argmin\nh\n2H\n^\n\"\n(\nh\n)?\n\nh\n\n=argmin\nh\n2H\n\"\n(\nh\n)tobethebestpossiblehypothesisin\nH\n.Note\nthat\nh\n\nisthebestthatwecouldpossiblydogiventhatweareusing\nH\n,so\nitmakessensetocompareourperformancetothatof\nh\n\n.Wehave:\n\"\n(\n^\nh\n)\n\n^\n\"\n(\n^\nh\n)+\n\n\n^\n\"\n(\nh\n\n)+\n\n\n\"\n(\nh\n\n)+2\n\nThelineusedthefactthat\nj\n\"\n(\n^\nh\n)\n\n^\n\"\n(\n^\nh\n)\nj\n\n(byouruniformconvergence\nassumption).Thesecondusedthefactthat\n^\nh\nwaschosentominimize^\n\"\n(\nh\n),\nandhence^\n\"\n(\n^\nh\n)\n\n^\n\"\n(\nh\n)forall\nh\n,andinparticular^\n\"\n(\n^\nh\n)\n\n^\n\"\n(\nh\n\n).Thethird\nlineusedtheuniformconvergenceassumptionagain,toshowthat^\n\"\n(\nh\n\n)\n\n\"\n(\nh\n\n)+\n\n.So,whatwe'veshownisthefollowing:Ifuniformconvergence\noccurs,thenthegeneralizationerrorof\n^\nh\nisatmost2\n\nworsethanthebest\npossiblehypothesisin\nH\n!\nLet'sputallthistogetherintoatheorem.\nTheorem.\nLet\njHj\n=\nk\n,andletany\nn;\nbeThenwithprobabilityat\nleast1\n\n\n,wehavethat\n\"\n(\n^\nh\n)\n\n\nmin\nh\n2H\n\"\n(\nh\n)\n\n+2\nr\n1\n2\nn\nlog\n2\nk\n\n:\n"}, {"page_number": 121, "text": "121\nThisisprovedbyletting\n\nequalthe\np\n\nterm,usingourpreviousargu-\nmentthatuniformconvergenceoccurswithprobabilityatleast1\n\n\n,and\nthennotingthatuniformconvergenceimplies\n\"\n(\nh\n)isatmost2\n\nhigherthan\n\"\n(\nh\n\n)=min\nh\n2H\n\"\n(\nh\n)(asweshowedpreviously).\nThisalsoquanwhatweweresayingpreviouslysayingaboutthe\nbias/varianceinmodelselection.Sp,supposewehavesome\nhypothesisclass\nH\n,andareconsideringswitchingtosomemuchlargerhy-\npothesisclass\nH\n0\nH\n.Ifweswitchto\nH\n0\n,thenthetermmin\nh\n\"\n(\nh\n)\ncanonlydecrease(sincewe'dthenbetakingaminoveralargersetoffunc-\ntions).Hence,bylearningusingalargerhypothesisclass,our\\bias\"can\nonlydecrease.However,ifkincreases,thenthesecond2\np\n\ntermwouldalso\nincrease.Thisincreasecorrespondstoour\\variance\"increasingwhenweuse\nalargerhypothesisclass.\nByholding\n\nand\n\nandsolvingfor\nn\nlikewedidbefore,wecanalso\nobtainthefollowingsamplecomplexitybound:\nCorollary.\nLet\njHj\n=\nk\n,andletany\n;\nbeThenfor\n\"\n(\n^\nh\n)\n\nmin\nh\n2H\n\"\n(\nh\n)+2\n\ntoholdwithprobabilityatleast1\n\n\n,itthat\nn\n\n1\n2\n\n2\nlog\n2\nk\n\n=\nO\n\n1\n\n2\nlog\nk\n\n\n;\n8.3.3Thecaseof\nH\nWehaveprovedsomeusefultheoremsforthecaseofhypothesisclasses.\nButmanyhypothesisclasses,includinganyparameterizedbyrealnumbers\n(asinlinearactuallycontainannumberoffunctions.\nCanweprovesimilarresultsforthissetting?\nLet'sstartbygoingthroughsomethingthatis\nnot\nthe\\right\"argument.\nBetterandmoregeneralargumentsexist\n,butthiswillbeusefulforhoning\nourintuitionsaboutthedomain.\nSupposewehavean\nH\nthatisparameterizedby\nd\nrealnumbers.Sincewe\nareusingacomputertorepresentrealnumbers,andIEEEdouble-precision\npoint(\ndouble\n'sinC)uses64bitstorepresentapointnum-\nber,thismeansthatourlearningalgorithm,assumingwe'reusingdouble-\nprecisionpoint,isparameterizedby64\nd\nbits.Thus,ourhypothesis\nclassreallyconsistsofatmost\nk\n=2\n64\nd\nthypotheses.FromtheCorol-\nlaryattheendoftheprevioussection,wethereforethat,toguarantee\n"}, {"page_number": 122, "text": "122\n\"\n(\n^\nh\n)\n\n\"\n(\nh\n\n)+2\n\n,withtoholdwithprobabilityatleast1\n\n\n,itthat\nn\n\nO\n\n1\n\n2\nlog\n2\n64\nd\n\n\n=\nO\n\nd\n\n2\nlog\n1\n\n\n=\nO\n\n(\nd\n).(The\n;\nsubscriptsindicate\nthatthelastbig-\nO\nishidingconstantsthatmaydependon\n\nand\n\n.)Thus,\nthenumberoftrainingexamplesneededisatmost\nlinear\nintheparameters\nofthemodel.\nThefactthatwereliedon64-bitpointmakesthisargumentnot\nentirelysatisfying,buttheconclusionisnonethelessroughlycorrect:Ifwhat\nwetrytodoisminimizetrainingerror,theninordertolearn\\well\"usinga\nhypothesisclassthathas\nd\nparameters,generallywe'regoingtoneedonthe\norderofalinearnumberoftrainingexamplesin\nd\n.\n(Atthispoint,it'sworthnotingthattheseresultswereprovedforanal-\ngorithmthatusesempiricalriskminimization.Thus,whilethelineardepen-\ndenceofsamplecomplexityon\nd\ndoesgenerallyholdformostdiscriminative\nlearningalgorithmsthattrytominimizetrainingerrororsomeapproxima-\ntiontotrainingerror,theseconclusionsdonotalwaysapplyasreadilyto\ndiscriminativelearningalgorithms.Givinggoodtheoreticalguaranteeson\nmanynon-ERMlearningalgorithmsisstillanareaofactiveresearch.)\nTheotherpartofourpreviousargumentthat'sslightlyunsatisfyingis\nthatitreliesontheparameterizationof\nH\n.Intuitively,thisdoesn'tseemlike\nitshouldmatter:Wehadwrittentheclassoflinearas\nh\n\n(\nx\n)=\n1\nf\n\n0\n+\n\n1\nx\n1\n+\n\n\nd\nx\nd\n\n0\ng\n,with\nn\n+1parameters\n\n0\n;:::;\nd\n.Butitcould\nalsobewritten\nh\nu;v\n(\nx\n)=1\nf\n(\nu\n2\n0\n\nv\n2\n0\n)+(\nu\n2\n1\n\nv\n2\n1\n)\nx\n1\n+\n\n(\nu\n2\nd\n\nv\n2\nd\n)\nx\nd\n\n0\ng\nwith2\nd\n+2parameters\nu\ni\n;v\ni\n.Yet,bothofthesearejustthesame\nH\n:Thesetoflinearin\nd\ndimensions.\nToderiveamoresatisfyingargument,let'safewmorethings.\nGivenaset\nS\n=\nf\nx\n(\ni\n)\n;:::;x\n(\nD\n)\ng\n(norelationtothetrainingset)ofpoints\nx\n(\ni\n)\n2X\n,wesaythat\nH\nshatters\nS\nif\nH\ncanrealizeanylabelingon\nS\n.\nI.e.,ifforanysetoflabels\nf\ny\n(1)\n;:::;y\n(\nD\n)\ng\n,thereexistssome\nh\n2H\nsothat\nh\n(\nx\n(\ni\n)\n)=\ny\n(\ni\n)\nforall\ni\n=1\n;:::\nD\n.\nGivenahypothesisclass\nH\n,wethenits\nVapnik-Chervonenkis\ndimension\n,writtenVC(\nH\n),tobethesizeofthelargestsetthatisshattered\nby\nH\n.(If\nH\ncanshatterarbitrarilylargesets,thenVC(\nH\n)=\n1\n.)\nForinstance,considerthefollowingsetofthreepoints:\n"}, {"page_number": 123, "text": "123\nCantheset\nH\noflinearintwodimensions(\nh\n(\nx\n)=1\nf\n\n0\n+\n\n1\nx\n1\n+\n\n2\nx\n2\n\n0\ng\n)canshatterthesetabove?Theanswerisyes.Sp,we\nseethat,foranyoftheeightpossiblelabelingsofthesepoints,wecana\nlinearthatobtains\\zerotrainingerror\"onthem:\nMoreover,itispossibletoshowthatthereisnosetof4pointsthatthis\nhypothesisclasscanshatter.Thus,thelargestsetthat\nH\ncanshatterisof\nsize3,andhenceVC(\nH\n)=3.\nNotethattheVCdimensionof\nH\nhereis3eventhoughtheremaybe\nsetsofsize3thatitcannotshatter.Forinstance,ifwehadasetofthree\npointslyinginastraightline(leftthenthereisnowaytoalinear\nseparatorforthelabelingofthethreepointsshownbelow(right\n"}, {"page_number": 124, "text": "124\nInorderwords,undertheoftheVCdimension,inorderto\nprovethatVC(\nH\n)isatleast\nD\n,weneedtoshowonlythatthere'satleast\none\nsetofsize\nD\nthat\nH\ncanshatter.\nThefollowingtheorem,duetoVapnik,canthenbeshown.(Thisis,many\nwouldargue,themostimportanttheoreminalloflearningtheory.)\nTheorem.\nLet\nH\nbegiven,andlet\nD\n=VC(\nH\n).Thenwithprobabilityat\nleast1\n\n\n,wehavethatforall\nh\n2H\n,\nj\n\"\n(\nh\n)\n\n^\n\"\n(\nh\n)\nj\nO\n \nr\nD\nn\nlog\nn\nD\n+\n1\nn\nlog\n1\n\n!\n:\nThus,withprobabilityatleast1\n\n\n,wealsohavethat:\n\"\n(\n^\nh\n)\n\n\"\n(\nh\n\n)+\nO\n \nr\nD\nn\nlog\nn\nD\n+\n1\nn\nlog\n1\n\n!\n:\nInotherwords,ifahypothesisclasshasVCdimension,thenuniform\nconvergenceoccursas\nn\nbecomeslarge.Asbefore,thisallowsustogivea\nboundon\n\"\n(\nh\n)intermsof\n\"\n(\nh\n\n).Wealsohavethefollowingcorollary:\nCorollary.\nFor\nj\n\"\n(\nh\n)\n\n^\n\"\n(\nh\n)\nj\n\ntoholdforall\nh\n2H\n(andhence\n\"\n(\n^\nh\n)\n\n\"\n(\nh\n\n)+2\n\n)withprobabilityatleast1\n\n\n,itthat\nn\n=\nO\n\n(\nD\n).\nInotherwords,thenumberoftrainingexamplesneededtolearn\\well\"\nusing\nH\nislinearintheVCdimensionof\nH\n.Itturnsoutthat,for\\most\"\nhypothesisclasses,theVCdimension(assuminga\\reasonable\"parameter-\nization)isalsoroughlylinearinthenumberofparameters.Puttingthese\ntogether,weconcludethatforagivenhypothesisclass\nH\n(andforanalgo-\nrithmthattriestominimizetrainingerror),thenumberoftrainingexamples\nneededtoachievegeneralizationerrorclosetothatoftheoptimal\nisusuallyroughlylinearinthenumberofparametersof\nH\n.\n"}, {"page_number": 125, "text": "Chapter9\nRegularizationandmodel\nselection\n9.1Regularization\nRecallthatasdiscussedinSection8.1,overfttingistypicallyaresultofusing\ntoocomplexmodels,andweneedtochooseapropermodelcomplexityto\nachievetheoptimalbias-varianceWhenthemodelcomplexityis\nmeasuredbythenumberofparameters,wecanvarythesizeofthemodel\n(e.g.,thewidthofaneuralnet).However,thecorrect,informativecomplex-\nitymeasureofthemodelscanbeafunctionoftheparameters(e.g.,\n`\n2\nnorm\noftheparameters),whichmaynotnecessarilydependonthenumberofpa-\nrameters.Insuchcases,wewilluseregularization,animportanttechnique\ninmachinelearning,controlthemodelcomplexityandpreventov\nRegularizationtypicallyinvolvesaddinganadditionalterm,calledareg-\nularizeranddenotedby\nR\n(\n\n)here,tothetrainingloss/costfunction:\nJ\n\n(\n\n)=\nJ\n(\n\n)+\n\n(\n\n)(9.1)\nHere\nJ\n\nisoftencalledtheregularizedloss,and\n\n\n0iscalledtheregular-\nizationparameter.Theregularizer\nR\n(\n\n)isanonnegativefunction(inalmost\nallcases).Inclassicalmethods,\nR\n(\n\n)ispurelyafunctionoftheparameter\n\n,\nbutsomemodernapproachallows\nR\n(\n\n)todependonthetrainingdataset.\n1\nTheregularizer\nR\n(\n\n)istypicallychosentobesomemeasureofthecom-\nplexityofthemodel\n\n.Thus,whenusingtheregularizedloss,weaimto\namodelthatboththedata(asmallloss\nJ\n(\n\n))andhaveasmall\n1\nHereournotationsgenerallyomitthedependencyonthetrainingdatasetfor\nsimplicity|wewrite\nJ\n(\n\n)eventhoughitobviouslyneedstodependonthetrainingdataset.\n125\n"}, {"page_number": 126, "text": "126\nmodelcomplexity(asmall\nR\n(\n\n)).Thebalancebetweenthetwoobjectivesis\ncontrolledbytheregularizationparameter\n\n.When\n\n=0,theregularized\nlossisequivalenttotheoriginalloss.When\n\nisatlysmallpositive\nnumber,minimizingtheregularizedlossiselyminimizingtheoriginal\nlosswiththeregularizerasthetie-breaker.Whentheregularizerisextremely\nlarge,thentheoriginallossisnotctive(andlikelythemodelwillhavea\nlargebias.)\nThemostcommonlyusedregularizationisperhaps\n`\n2\nregularization,\nwhere\nR\n(\n\n)=\n1\n2\nk\n\nk\n2\n2\n.Itencouragestheoptimizertoamodelwith\nsmall\n`\n2\nnorm.Indeeplearning,it'softentimesreferredtoas\nweightde-\ncay\n,becausegradientdescentwithlearningrate\n\nontheregularizedloss\nR\n\n(\n\n)isequivalenttoshrinking/decaying\n\nbyascalarfactorof1\n\n\nand\nthenapplyingthestandardgradient\n\n \n\n\n\nr\nJ\n\n(\n\n)=\n\n\n\n\n\nr\nJ\n(\n\n)\n=(1\n\n\n)\n\n|\n{z\n}\ndecayingweights\n\n\nr\nJ\n(\n\n)(9.2)\nBesidesencouragingsimplermodels,regularizationcanalsoimposein-\nductivebiasesorstructuresonthemodelparameters.Forexample,suppose\nwehadapriorbeliefthatthenumberofnon-zerosintheground-truthmodel\nparametersissmall,\n2\n|whichisoftentimescalledsparsityofthemodel|,we\ncanimposearegularizationonthenumberofnon-zerosin\n\n,denotedby\nk\n\nk\n0\n,toleveragesuchapriorbelief.Imposingadditionalstructureofthe\nparametersnarrowsoursearchspaceandmakesthecomplexityofthemodel\nfamilysmaller,|e.g.,thefamilyofsparsemodelscanbethoughtofashaving\nlowercomplexitythanthefamilyofallmodels|,andthustendstoleadtoa\nbettergeneralization.Ontheotherhand,imposingadditionalstructuremay\nriskincreasingthebias.Forexample,ifweregularizethesparsitystrongly\nbutnosparsemodelscanpredictthelabelaccurately,wewillfrom\nlargebias(analogouslytothesituationwhenweuselinearmodelstolearn\ndatathancanonlyberepresentedbyquadraticfunctionsinSection8.1.)\nThesparsityoftheparametersisnotacontinuousfunctionoftheparam-\neters,andthuswecannotoptimizeitwith(stochastic)gradientdescent.A\ncommonrelaxationistouse\nR\n(\n\n)=\nk\n\nk\n1\nasacontinuoussurrogate.\n3\n2\nForlinearmodels,thismeansthemodeljustusesafewcoordinatesoftheinputsto\nmakeanaccurateprediction.\n3\nTherehasbeenarichlineoftheoreticalworkthatexplainswhy\nk\n\nk\n1\nisagoodsur-\nrogateforencouragingsparsity,butit'sbeyondthescopeofthiscourse.Anintuitionis:\nassumingtheparameterisontheunitsphere,theparameterwithsmallest\n`\n1\nnormalso\n"}, {"page_number": 127, "text": "127\nThe\nR\n(\n\n)=\nk\n\nk\n1\n(alsocalledLASSO)and\nR\n(\n\n)=\n1\n2\nk\n\nk\n2\n2\nareperhaps\namongthemostcommonlyusedregularizersforlinearmodels.Othernorm\nandpowersofnormsaresometimesalsoused.The\n`\n2\nnormregularizationis\nmuchmorecommonlyusedwithkernelmethodsbecause\n`\n1\nregularizationis\ntypicallynotcompatiblewiththekerneltrick(theoptimalsolutioncannot\nbewrittenasfunctionsofinnerproductsoffeatures.)\nIndeeplearning,themostcommonlyusedregularizeris\n`\n2\nregularization\norweightdecay.Othercommononesincludedropout,dataaugmentation,\nregularizingthespectralnormoftheweightmatrices,andregularizingthe\nLipschitznessofthemodel,etc.Regularizationindeeplearningisanac-\ntiveresearcharea,andit'sknownthatthereisanotherimplicitsourceof\nregularization,asdiscussedinthenextsection.\n9.2Implicitregularization\nTheimplicitregularizationofoptimizers,orimplicitbiasoralgorithmic\nregularization,isanewconcept/phenomenonobservedinthedeeplearning\nera.Itlargelyreferstothattheoptimizerscanimplicitlyimposestructures\nonparametersbeyondwhathasbeenimposedbytheregularizedloss.\nInmostclassicalsettings,thelossorregularizedlosshasauniqueglobal\nminimum,andthusanyreasonableoptimizershouldconvergetothatglobal\nminimumandcannotimposeanyadditionalpreferences.However,indeep\nlearning,oftentimesthelossorregularizedlosshasmorethanone(approx-\nimate)globalminima,andoptimizersmayconvergetot\nglobalminima.Thoughtheseglobalminimahavethesameorsimilartrain-\ninglosses,theymaybeoftnatureandhavedramaticallyt\ngeneralizationperformance.SeeFigures9.1and9.2anditscaptionforan\nillustrationandsomeexperimentresults.Forexample,it'spossiblethatone\nglobalminimumgivesamuchmoreLipschitzorsparsemodelthanothers\nandthushasabettertesterror.Itturnsoutthatmanycommonly-usedop-\ntimizers(ortheircomponents)preferorbiastowardsglobalminima\nofcertainproperties,leadingtoabettertestperformance.\nhappentobethesparsestparameterwithonly1non-zerocoordinate.Thus,sparsityand\n`\n1\nnormgivesthesameextremalpointstosomeextent.\n"}, {"page_number": 128, "text": "128\nFigure9.1:AnIllustrationthattglobalminimaofthetrainingloss\ncanhavettestperformance.\nFigure9.2:\nLeft:\nPerformanceofneuralnetworkstrainedbytwot\nlearningratesschedulesontheCIFAR-10dataset.Althoughbothexper-\nimentsusedexactlythesameregularizedlossesandtheoptimizersthe\ntrainingdataperfectly,themodels'generalizationperformancemuch.\nRight:\nOnatsyntheticdataset,optimizerswithtinitializa-\ntionshavethesametrainingerrorbuttgeneralizationperformance.\n4\nInsummary,thetakehomemessagehereisthatthechoiceofoptimizer\ndoesnotonlyminimizingthetrainingloss,butalsoimposesimplicit\nregularizationandthegeneralizationofthemodel.Evenifyourcur-\nrentoptimizeralreadyconvergestoasmalltrainingerrorperfectly,youmay\nstillneedtotuneyouroptimizerforabettergeneralization,.\n4\nThesettingisthesameasinWoodworthetal.[2020],HaoChenetal.[2020]\n"}, {"page_number": 129, "text": "129\nOnemaywonderwhichcomponentsoftheoptimizersbiastowardswhat\ntypeofglobalminimaandwhattypeofglobalminimamaygeneralizebet-\nter.Theseareopenquestionsthatresearchersareactivelyinvestigating.\nEmpiricalandtheoreticalresearchhavesomecluesandheuristics.\nInmany(butfarfromall)situations,amongthosesettingwhere\noptimizationcansucceedinminimizingthetrainingloss,theuseoflarger\ninitiallearningrate,smallerinitialization,smallerbatchsize,andmomen-\ntumappearstohelpwithbiasingtowardsmoregeneralizablesolutions.A\nconjecture(thatcanbeprovenincertaincase)isthatstochas-\nticityintheoptimizationprocesshelptheoptimizertoglobal\nminima(globalminimawherethecurvatureofthelossissmall),and\nglobalminimatendtogivemoreLipschitzmodelsandbettergeneralization.\nCharacterizingtheimplicitregularizationformallyisstillachallenging\nopenresearchquestion.\n9.3Modelselectionviacrossvalidation\nSupposewearetryingselectamongseveraltmodelsforalearning\nproblem.Forinstance,wemightbeusingapolynomialregressionmodel\nh\n\n(\nx\n)=\ng\n(\n\n0\n+\n\n1\nx\n+\n\n2\nx\n2\n+\n\n+\n\nk\nx\nk\n),andwishtodecideif\nk\nshouldbe\n0,1,...,or10.Howcanweautomaticallyselectamodelthatrepresents\nagoodbetweenthetwinevilsofbiasandvariance\n5\n?Alternatively,\nsupposewewanttoautomaticallychoosethebandwidthparameter\n\u02dd\nfor\nlocallyweightedregression,ortheparameter\nC\nforour\n`\n1\n-regularizedSVM.\nHowcanwedothat?\nForthesakeofconcreteness,inthesenotesweassumewehavesome\nsetofmodels\nM\n=\nf\nM\n1\n;:::;M\nd\ng\nthatwe'retryingtoselectamong.\nForinstance,inourexampleabove,themodel\nM\ni\nwouldbean\ni\n-th\ndegreepolynomialregressionmodel.(Thegeneralizationto\nM\nis\nnothard.\n6\n)Alternatively,ifwearetryingtodecidebetweenusinganSVM,\naneuralnetworkorlogisticregression,then\nM\nmaycontainthesemodels.\n5\nGiventhatwesaidintheprevioussetofnotesthatbiasandvariancearetwovery\ntbeasts,somereadersmaybewonderingifweshouldbecallingthem\\twin\"evils\nhere.Perhapsit'dbebettertothinkofthemasnon-identicaltwins.Thephrase\\the\nfraternaltwinevilsofbiasandvariance\"doesn'thavethesameringtoit,though.\n6\nIfwearetryingtochoosefromansetofmodels,saycorrespondingtothe\npossiblevaluesofthebandwidth\n\u02dd\n2\nR\n+\n,wemaydiscretize\n\u02dd\nandconsideronlya\nnumberofpossiblevaluesforit.Moregenerally,mostofthealgorithmsdescribedhere\ncanallbeviewedasperformingoptimizationsearchinthespaceofmodels,andwecan\nperformthissearchovermodelclassesaswell.\n"}, {"page_number": 130, "text": "130\nCrossvalidation.\nLetssupposeweare,asusual,givenatrainingset\nS\n.\nGivenwhatweknowaboutempiricalriskminimization,here'swhatmight\ninitiallyseemlikeaalgorithm,resultingfromusingempiricalriskminimiza-\ntionformodelselection:\n1.\nTraineachmodel\nM\ni\non\nS\n,togetsomehypothesis\nh\ni\n.\n2.\nPickthehypotheseswiththesmallesttrainingerror.\nThisalgorithmdoes\nnot\nwork.Considerchoosingthedegreeofapoly-\nnomial.Thehigherthedegreeofthepolynomial,thebetteritwillthe\ntrainingset\nS\n,andthusthelowerthetrainingerror.Hence,thismethodwill\nalwaysselectahigh-variance,high-degreepolynomialmodel,whichwesaw\npreviouslyisoftenpoorchoice.\nHere'sanalgorithmthatworksbetter.In\nhold-outcrossvalidation\n(alsocalled\nsimplecrossvalidation\n),wedothefollowing:\n1.\nRandomlysplit\nS\ninto\nS\ntrain\n(say,70%ofthedata)and\nS\ncv\n(theremain-\ning30%).Here,\nS\ncv\niscalledthehold-outcrossvalidationset.\n2.\nTraineachmodel\nM\ni\non\nS\ntrain\nonly,togetsomehypothesis\nh\ni\n.\n3.\nSelectandoutputthehypothesis\nh\ni\nthathadthesmallesterror^\n\"\nS\ncv\n(\nh\ni\n)\nontheholdoutcrossvalidationset.(Here^\n\"\nS\ncv\n(\nh\n)denotestheaverage\nerrorof\nh\nonthesetofexamplesin\nS\ncv\n.)Theerrorontheholdout\nvalidationsetisalsoreferredtoasthevalidationerror.\nBytesting/validatingonasetofexamples\nS\ncv\nthatthemodelswerenot\ntrainedon,weobtainabetterestimateofeachhypothesis\nh\ni\n'struegeneral-\nization/testerror.Thus,thisapproachisessentiallypickingthemodelwith\nthesmallestestimatedgeneralization/testerror.Thesizeofthevalidation\nsetdependsonthetotalnumberofavailableexamples.Usually,somewhere\nbetween1\n=\n4\n\n1\n=\n3ofthedataisusedintheholdoutcrossvalidationset,and\n30%isatypicalchoice.However,whenthetotaldatasetishuge,validation\nsetcanbeasmallerfractionofthetotalexamplesaslongastheabsolute\nnumberofvalidationexamplesisdecent.Forexample,fortheImageNet\ndatasetthathasabout1Mtrainingimages,thevalidationsetissometimes\nsettobe50Kimages,whichisonlyabout5%ofthetotalexamples.\nOptionally,step3inthealgorithmmayalsobereplacedwithselecting\nthemodel\nM\ni\naccordingtoargmin\ni\n^\n\"\nS\ncv\n(\nh\ni\n),andthenretraining\nM\ni\nonthe\nentiretrainingset\nS\n.(Thisisoftenagoodidea,withoneexceptionbeing\nlearningalgorithmsthatarebeverysensitivetoperturbationsoftheinitial\n"}, {"page_number": 131, "text": "131\nconditionsand/ordata.Forthesemethods,\nM\ni\ndoingwellon\nS\ntrain\ndoesnot\nnecessarilymeanitwillalsodowellon\nS\ncv\n,anditmightbebettertoforgo\nthisretrainingstep.)\nThedisadvantageofusingholdoutcrossvalidationisthatit\\wastes\"\nabout30%ofthedata.Evenifweweretotaketheoptionalstepofretraining\nthemodelontheentiretrainingset,it'sstillasifwe'retryingtoagood\nmodelforalearningprobleminwhichwehad0\n:\n7\nn\ntrainingexamples,rather\nthan\nn\ntrainingexamples,sincewe'retestingmodelsthatweretrainedon\nonly0\n:\n7\nn\nexampleseachtime.Whilethisisifdataisabundantand/or\ncheap,inlearningproblemsinwhichdataisscarce(consideraproblemwith\nn\n=20,say),we'dliketodosomethingbetter.\nHereisamethod,called\nk\n-foldcrossvalidation\n,thatholdsoutless\ndataeachtime:\n1.\nRandomlysplit\nS\ninto\nk\ndisjointsubsetsof\nm=k\ntrainingexampleseach.\nLetscallthesesubsets\nS\n1\n;:::;S\nk\n.\n2.\nForeachmodel\nM\ni\n,weevaluateitasfollows:\nFor\nj\n=1\n;:::;k\nTrainthemodel\nM\ni\non\nS\n1\n[[\nS\nj\n\n1\n[\nS\nj\n+1\n[\nS\nk\n(i.e.,train\nonallthedataexcept\nS\nj\n)togetsomehypothesis\nh\nij\n.\nTestthehypothesis\nh\nij\non\nS\nj\n,toget^\n\"\nS\nj\n(\nh\nij\n).\nTheestimatedgeneralizationerrorofmodel\nM\ni\nisthencalculated\nastheaverageofthe^\n\"\nS\nj\n(\nh\nij\n)'s(averagedover\nj\n).\n3.\nPickthemodel\nM\ni\nwiththelowestestimatedgeneralizationerror,and\nretrainthatmodelontheentiretrainingset\nS\n.Theresultinghypothesis\nisthenoutputasouranswer.\nAtypicalchoiceforthenumberoffoldstouseherewouldbe\nk\n=10.\nWhilethefractionofdataheldouteachtimeisnow1\n=k\n|muchsmaller\nthanbefore|thisproceduremayalsobemorecomputationallyexpensive\nthanhold-outcrossvalidation,sincewenowneedtraintoeachmodel\nk\ntimes.\nWhile\nk\n=10isacommonlyusedchoice,inproblemsinwhichdatais\nreallyscarce,sometimeswewillusetheextremechoiceof\nk\n=\nm\ninorder\ntoleaveoutaslittledataaspossibleeachtime.Inthissetting,wewould\nrepeatedlytrainonallbutoneofthetrainingexamplesin\nS\n,andtestonthat\nheld-outexample.Theresulting\nm\n=\nk\nerrorsarethenaveragedtogetherto\nobtainourestimateofthegeneralizationerrorofamodel.Thismethodhas\n"}, {"page_number": 132, "text": "132\nitsownname;sincewe'reholdingoutonetrainingexampleatatime,this\nmethodiscalled\nleave-one-outcrossvalidation.\nFinally,eventhoughwehavedescribedthetversionsofcrossvali-\ndationasmethodsforselectingamodel,theycanalsobeusedmoresimplyto\nevaluatea\nsingle\nmodeloralgorithm.Forexample,ifyouhaveimplemented\nsomelearningalgorithmandwanttoestimatehowwellitperformsforyour\napplication(orifyouhaveinventedanovellearningalgorithmandwantto\nreportinatechnicalpaperhowwellitperformsonvarioustestsets),cross\nvalidationwouldgiveareasonablewayofdoingso.\n9.4Bayesianstatisticsandregularization\nInthissection,wewilltalkaboutonemoretoolinourarsenalforourbattle\nagainstov\nAtthebeginningofthequarter,wetalkedaboutparameterusing\nmaximumlikelihoodestimation(MLE),andchoseourparametersaccording\nto\n\nMLE\n=argmax\n\nn\nY\ni\n=1\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\n:\nThroughoutoursubsequentdiscussions,weviewed\n\nasanunknownparam-\neteroftheworld.Thisviewofthe\n\nasbeing\nconstant-valuedbutunknown\nistakenin\nfrequentist\nstatistics.Inthefrequentistthisviewoftheworld,\n\nisnotrandom|itjusthappenstobeunknown|andit'sourjobtocomeup\nwithstatisticalprocedures(suchasmaximumlikelihood)totrytoestimate\nthisparameter.\nAnalternativewaytoapproachourparameterestimationproblemsisto\ntakethe\nBayesian\nviewoftheworld,andthinkof\n\nasbeinga\nrandom\nvariable\nwhosevalueisunknown.Inthisapproach,wewouldspecifya\npriordistribution\np\n(\n\n)on\n\nthatexpressesour\\priorbeliefs\"aboutthe\nparameters.Givenatrainingset\nS\n=\nf\n(\nx\n(\ni\n)\n;y\n(\ni\n)\n)\ng\nn\ni\n=1\n,whenweareaskedto\nmakeapredictiononanewvalueof\nx\n,wecanthencomputetheposterior\ndistributionontheparameters\np\n(\n\nj\nS\n)=\np\n(\nS\nj\n\n)\np\n(\n\n)\np\n(\nS\n)\n=\n\nQ\nn\ni\n=1\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n)\n\np\n(\n\n)\nR\n\n(\nQ\nn\ni\n=1\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n)\np\n(\n\n))\n\n(9.3)\nIntheequationabove,\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n)comesfromwhatevermodelyou'reusing\n"}, {"page_number": 133, "text": "133\nforyourlearningproblem.Forexample,ifyouareusingBayesianlogisticre-\ngression,thenyoumightchoose\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n)=\nh\n\n(\nx\n(\ni\n)\n)\ny\n(\ni\n)\n(1\n\nh\n\n(\nx\n(\ni\n)\n))\n(1\n\ny\n(\ni\n)\n)\n,\nwhere\nh\n\n(\nx\n(\ni\n)\n)=1\n=\n(1+exp(\n\n\nT\nx\n(\ni\n)\n)).\n7\nWhenwearegivenanewtestexample\nx\nandaskedtomakeitprediction\nonit,wecancomputeourposteriordistributionontheclasslabelusingthe\nposteriordistributionon\n\n:\np\n(\ny\nj\nx;S\n)=\nZ\n\np\n(\ny\nj\nx;\n)\np\n(\n\nj\nS\n)\n\n(9.4)\nIntheequationabove,\np\n(\n\nj\nS\n)comesfromEquation(9.3).Thus,forexample,\nifthegoalistothepredicttheexpectedvalueof\ny\ngiven\nx\n,thenwewould\noutput\n8\nE[\ny\nj\nx;S\n]=\nZ\ny\nyp\n(\ny\nj\nx;S\n)\ndy\nTheprocedurethatwe'veoutlinedherecanbethoughtofasdoing\\fully\nBayesian\"prediction,whereourpredictioniscomputedbytakinganaverage\nwithrespecttotheposterior\np\n(\n\nj\nS\n)over\n\n.Unfortunately,ingeneralitis\ncomputationallyverytocomputethisposteriordistribution.Thisis\nbecauseitrequirestakingintegralsoverthe(usuallyhigh-dimensional)\n\nas\ninEquation(9.3),andthistypicallycannotbedoneinclosed-form.\nThus,inpracticewewillinsteadapproximatetheposteriordistribution\nfor\n\n.Onecommonapproximationistoreplaceourposteriordistributionfor\n\n(asinEquation9.4)withasinglepointestimate.The\nMAP(maximum\naposteriori)\nestimatefor\n\nisgivenby\n\nMAP\n=argmax\n\nn\nY\ni\n=1\np\n(\ny\n(\ni\n)\nj\nx\n(\ni\n)\n;\n)\np\n(\n\n)\n:\n(9.5)\nNotethatthisisthesameformulasasfortheMLE(maximumlikelihood)\nestimatefor\n\n,exceptfortheprior\np\n(\n\n)termattheend.\nInpracticalapplications,acommonchoicefortheprior\np\n(\n\n)istoassume\nthat\n\n\u02d8N\n(0\n;\u02dd\n2\nI\n).Usingthischoiceofprior,theparameters\n\nMAP\nwill\nhavesmallernormthanthatselectedbymaximumlikelihood.Inpractice,\nthiscausestheBayesianMAPestimatetobelesssusceptibletoov\nthantheMLestimateoftheparameters.Forexample,Bayesianlogistic\nregressionturnsouttobeanealgorithmfortexteven\nthoughintextweusuallyhave\nd\n\u02db\nn\n.\n7\nSincewearenowviewing\n\nasarandomvariable,itisokaytoconditiononitvalue,\nandwrite\\\np\n(\ny\nj\nx;\n)\"insteadof\\\np\n(\ny\nj\nx\n;\n\n).\"\n8\nTheintegralbelowwouldbereplacedbyasummationif\ny\nisdiscrete-valued.\n"}, {"page_number": 134, "text": "PartIV\nUnsupervisedlearning\n134\n"}, {"page_number": 135, "text": "Chapter10\nClusteringandthe\nk\n-means\nalgorithm\nIntheclusteringproblem,wearegivenatrainingset\nf\nx\n(1)\n;:::;x\n(\nn\n)\ng\n,and\nwanttogroupthedataintoafewcohesive\\clusters.\"Here,\nx\n(\ni\n)\n2\nR\nd\nasusual;butnolabels\ny\n(\ni\n)\naregiven.So,thisisanunsupervisedlearning\nproblem.\nThe\nk\n-meansclusteringalgorithmisasfollows:\n1.\nInitialize\nclustercentroids\n\n1\n;\n2\n;:::;\nk\n2\nR\nd\nrandomly.\n2.\nRepeatuntilconvergence:\nf\nForevery\ni\n,set\nc\n(\ni\n)\n:=argmin\nj\njj\nx\n(\ni\n)\n\n\nj\njj\n2\n:\nForeach\nj\n,set\n\nj\n:=\nP\nn\ni\n=1\n1\nf\nc\n(\ni\n)\n=\nj\ng\nx\n(\ni\n)\nP\nn\ni\n=1\n1\nf\nc\n(\ni\n)\n=\nj\ng\n:\ng\nInthealgorithmabove,\nk\n(aparameterofthealgorithm)isthenumber\nofclusterswewanttoandtheclustercentroids\n\nj\nrepresentourcurrent\nguessesforthepositionsofthecentersoftheclusters.Toinitializethecluster\ncentroids(instep1ofthealgorithmabove),wecouldchoose\nk\ntraining\nexamplesrandomly,andsettheclustercentroidstobeequaltothevaluesof\nthese\nk\nexamples.(Otherinitializationmethodsarealsopossible.)\nTheinner-loopofthealgorithmrepeatedlycarriesouttwosteps:(i)\n\\Assigning\"eachtrainingexample\nx\n(\ni\n)\ntotheclosestclustercentroid\n\nj\n,and\n135\n"}, {"page_number": 136, "text": "136\nFigure10.1:K-meansalgorithm.Trainingexamplesareshownasdots,and\nclustercentroidsareshownascrosses.(a)Originaldataset.(b)Randomini-\ntialclustercentroids(inthisinstance,notchosentobeequaltotwotraining\nexamples).(c-f)Illustrationofrunningtwoiterationsof\nk\n-means.Ineach\niteration,weassigneachtrainingexampletotheclosestclustercentroid\n(shownby\\painting\"thetrainingexamplesthesamecolorasthecluster\ncentroidtowhichisassigned);thenwemoveeachclustercentroidtothe\nmeanofthepointsassignedtoit.(Bestviewedincolor.)Imagescourtesy\nMichaelJordan.\n(ii)Movingeachclustercentroid\n\nj\ntothemeanofthepointsassignedtoit.\nFigure10.1showsanillustrationofrunning\nk\n-means.\nIsthe\nk\n-meansalgorithmguaranteedtoconverge?Yesitis,inacertain\nsense.Inparticular,letusthe\ndistortionfunction\ntobe:\nJ\n(\nc;\n)=\nn\nX\ni\n=1\njj\nx\n(\ni\n)\n\n\nc\n(\ni\n)\njj\n2\nThus,\nJ\nmeasuresthesumofsquareddistancesbetweeneachtrainingexam-\nple\nx\n(\ni\n)\nandtheclustercentroid\n\nc\n(\ni\n)\ntowhichithasbeenassigned.Itcan\nbeshownthat\nk\n-meansisexactlycoordinatedescenton\nJ\n.Sply,the\ninner-loopof\nk\n-meansrepeatedlyminimizes\nJ\nwithrespectto\nc\nwhileholding\n\nandthenminimizes\nJ\nwithrespectto\n\nwhileholding\nc\nThus,\n"}, {"page_number": 137, "text": "137\nJ\nmustmonotonicallydecrease,andthevalueof\nJ\nmustconverge.(Usu-\nally,thisimpliesthat\nc\nand\n\nwillconvergetoo.Intheory,itispossiblefor\nk\n-meanstooscillatebetweenafewtclusterings|i.e.,afewt\nvaluesfor\nc\nand/or\n\n|thathaveexactlythesamevalueof\nJ\n,butthisalmost\nneverhappensinpractice.)\nThedistortionfunction\nJ\nisanon-convexfunction,andsocoordinate\ndescenton\nJ\nisnotguaranteedtoconvergetotheglobalminimum.Inother\nwords,\nk\n-meanscanbesusceptibletolocaloptima.Veryoften\nk\n-meanswill\nworkandcomeupwithverygoodclusteringsdespitethis.Butifyou\nareworriedaboutgettingstuckinbadlocalminima,onecommonthingto\ndoisrun\nk\n-meansmanytimes(usingtrandominitialvaluesforthe\nclustercentroids\n\nj\n).Then,outofallthetclusteringsfound,pick\ntheonethatgivesthelowestdistortion\nJ\n(\nc;\n).\n"}, {"page_number": 138, "text": "Chapter11\nEMalgorithms\nInthissetofnotes,wediscusstheEM(Expectation-Maximization)algorithm\nfordensityestimation.\n11.1EMformixtureofGaussians\nSupposethatwearegivenatrainingset\nf\nx\n(1)\n;:::;x\n(\nn\n)\ng\nasusual.Sincewe\nareintheunsupervisedlearningsetting,thesepointsdonotcomewithany\nlabels.\nWewishtomodelthedatabyspecifyingajointdistribution\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n)=\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n)\np\n(\nz\n(\ni\n)\n).Here,\nz\n(\ni\n)\n\u02d8\nMultinomial(\n\u02da\n)(where\n\u02da\nj\n\n0,\nP\nk\nj\n=1\n\u02da\nj\n=1,\nandtheparameter\n\u02da\nj\ngives\np\n(\nz\n(\ni\n)\n=\nj\n)),and\nx\n(\ni\n)\nj\nz\n(\ni\n)\n=\nj\n\u02d8N\n(\n\nj\n;\n\nj\n).We\nlet\nk\ndenotethenumberofvaluesthatthe\nz\n(\ni\n)\n'scantakeon.Thus,our\nmodelpositsthateach\nx\n(\ni\n)\nwasgeneratedbyrandomlychoosing\nz\n(\ni\n)\nfrom\nf\n1\n;:::;k\ng\n,andthen\nx\n(\ni\n)\nwasdrawnfromoneof\nk\nGaussiansdependingon\nz\n(\ni\n)\n.Thisiscalledthe\nmixtureofGaussians\nmodel.Also,notethatthe\nz\n(\ni\n)\n'sare\nlatent\nrandomvariables,meaningthatthey'rehidden/unobserved.\nThisiswhatwillmakeourestimationproblem\nTheparametersofourmodelarethus\n\u02da\n,\n\nandToestimatethem,we\ncanwritedownthelikelihoodofourdata:\n`\n(\n\u02da;\n=\nn\nX\ni\n=1\nlog\np\n(\nx\n(\ni\n)\n;\n\u02da;\n\n=\nn\nX\ni\n=1\nlog\nk\nX\nz\n(\ni\n)\n=1\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n;\n\n\np\n(\nz\n(\ni\n)\n;\n\u02da\n)\n:\nHowever,ifwesettozerothederivativesofthisformulawithrespectto\n138\n"}, {"page_number": 139, "text": "139\ntheparametersandtrytosolve,we'llthatitisnotpossibletothe\nmaximumlikelihoodestimatesoftheparametersinclosedform.(Trythis\nyourselfathome.)\nTherandomvariables\nz\n(\ni\n)\nindicatewhichofthe\nk\nGaussianseach\nx\n(\ni\n)\nhadcomefrom.Notethatifweknewwhatthe\nz\n(\ni\n)\n'swere,themaximum\nlikelihoodproblemwouldhavebeeneasy.Sp,wecouldthenwrite\ndownthelikelihoodas\n`\n(\n\u02da;\n=\nn\nX\ni\n=1\nlog\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n;\n\n+log\np\n(\nz\n(\ni\n)\n;\n\u02da\n)\n:\nMaximizingthiswithrespectto\n\u02da\n,\n\nandgivestheparameters:\n\u02da\nj\n=\n1\nn\nn\nX\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\n;\n\nj\n=\nP\nn\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\nx\n(\ni\n)\nP\nn\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\n;\n\nj\n=\nP\nn\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\n(\nx\n(\ni\n)\n\n\nj\n)(\nx\n(\ni\n)\n\n\nj\n)\nT\nP\nn\ni\n=1\n1\nf\nz\n(\ni\n)\n=\nj\ng\n:\nIndeed,weseethatifthe\nz\n(\ni\n)\n'swereknown,thenmaximumlikelihood\nestimationbecomesnearlyidenticaltowhatwehadwhenestimatingthe\nparametersoftheGaussiandiscriminantanalysismodel,exceptthathere\nthe\nz\n(\ni\n)\n'splayingtheroleoftheclasslabels.\n1\nHowever,inourdensityestimationproblem,the\nz\n(\ni\n)\n'sare\nnot\nknown.\nWhatcanwedo?\nTheEMalgorithmisaniterativealgorithmthathastwomainsteps.\nAppliedtoourproblem,intheE-step,ittriesto\\guess\"thevaluesofthe\nz\n(\ni\n)\n's.IntheM-step,itupdatestheparametersofourmodelbasedonour\nguesses.SinceintheM-stepwearepretendingthattheguessesinthe\npartwerecorrect,themaximizationbecomeseasy.Here'sthealgorithm:\nRepeatuntilconvergence:\nf\n(E-step)Foreach\ni;j\n,set\nw\n(\ni\n)\nj\n:=\np\n(\nz\n(\ni\n)\n=\nj\nj\nx\n(\ni\n)\n;\n\u02da;\n\n1\nThereareotherminordierencesintheformulasherefromwhatwe'dobtainedin\nPS1withGaussiandiscriminantanalysis,becausewe'vegeneralizedthe\nz\n(\ni\n)\n'stobe\nmultinomialratherthanBernoulli,andsecondbecausehereweareusingat\nj\nforeachGaussian.\n"}, {"page_number": 140, "text": "140\n(M-step)Updatetheparameters:\n\u02da\nj\n:=\n1\nn\nn\nX\ni\n=1\nw\n(\ni\n)\nj\n;\n\nj\n:=\nP\nn\ni\n=1\nw\n(\ni\n)\nj\nx\n(\ni\n)\nP\nn\ni\n=1\nw\n(\ni\n)\nj\n;\n\nj\n:=\nP\nn\ni\n=1\nw\n(\ni\n)\nj\n(\nx\n(\ni\n)\n\n\nj\n)(\nx\n(\ni\n)\n\n\nj\n)\nT\nP\nn\ni\n=1\nw\n(\ni\n)\nj\ng\nIntheE-step,wecalculatetheposteriorprobabilityofourparameters\nthe\nz\n(\ni\n)\n's,giventhe\nx\n(\ni\n)\nandusingthecurrentsettingofourparameters.I.e.,\nusingBayesrule,weobtain:\np\n(\nz\n(\ni\n)\n=\nj\nj\nx\n(\ni\n)\n;\n\u02da;\n=\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n=\nj\n;\n\n\np\n(\nz\n(\ni\n)\n=\nj\n;\n\u02da\n)\nP\nk\nl\n=1\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n=\nl\n;\n\n\np\n(\nz\n(\ni\n)\n=\nl\n;\n\u02da\n)\nHere,\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n=\nj\n;\n\nisgivenbyevaluatingthedensityofaGaussian\nwithmean\n\nj\nandcovariance\nj\nat\nx\n(\ni\n)\n;\np\n(\nz\n(\ni\n)\n=\nj\n;\n\u02da\n)isgivenby\n\u02da\nj\n,andso\non.Thevalues\nw\n(\ni\n)\nj\ncalculatedintheE-steprepresentour\\soft\"guesses\n2\nfor\nthevaluesof\nz\n(\ni\n)\n.\nAlso,youshouldcontrasttheupdatesintheM-stepwiththeformulaswe\nhadwhenthe\nz\n(\ni\n)\n'swereknownexactly.Theyareidentical,exceptthatin-\nsteadoftheindicatorfunctions\\1\nf\nz\n(\ni\n)\n=\nj\ng\n\"indicatingfromwhichGaussian\neachdatapointhadcome,wenowinsteadhavethe\nw\n(\ni\n)\nj\n's.\nTheEM-algorithmisalsoreminiscentoftheK-meansclusteringalgo-\nrithm,exceptthatinsteadofthe\\hard\"clusterassignments\nc\n(\ni\n),weinstead\nhavethe\\soft\"assignments\nw\n(\ni\n)\nj\n.SimilartoK-means,itisalsosusceptible\ntolocaloptima,soreinitializingatseveraltinitialparametersmay\nbeagoodidea.\nIt'sclearthattheEMalgorithmhasaverynaturalinterpretationof\nrepeatedlytryingtoguesstheunknown\nz\n(\ni\n)\n's;buthowdiditcomeabout,\nandcanwemakeanyguaranteesaboutit,suchasregardingitsconvergence?\nInthenextsetofnotes,wewilldescribeamoregeneralviewofEM,one\n2\nTheterm\\soft\"referstoourguessesbeingprobabilitiesandtakingvaluesin[0\n;\n1];in\ncontrast,a\\hard\"guessisonethatrepresentsasinglebestguess(suchastakingvalues\nin\nf\n0\n;\n1\ng\nor\nf\n1\n;:::;k\ng\n).\n"}, {"page_number": 141, "text": "141\nthatwillallowustoeasilyapplyittootherestimationproblemsinwhich\ntherearealsolatentvariables,andwhichwillallowustogiveaconvergence\nguarantee.\n11.2Jensen'sinequality\nWebeginourdiscussionwithaveryusefulresultcalled\nJensen'sinequality\nLet\nf\nbeafunctionwhosedomainisthesetofrealnumbers.Recallthat\nf\nisaconvexfunctionif\nf\n00\n(\nx\n)\n\n0(forall\nx\n2\nR\n).Inthecaseof\nf\ntaking\nvector-valuedinputs,thisisgeneralizedtotheconditionthatitshessian\nH\nispositive(\nH\n\n0).If\nf\n00\n(\nx\n)\n>\n0forall\nx\n,thenwesay\nf\nis\nstrictly\nconvex(inthevector-valuedcase,thecorrespondingstatementis\nthat\nH\nmustbepositivewritten\nH>\n0).Jensen'sinequalitycan\nthenbestatedasfollows:\nTheorem.\nLet\nf\nbeaconvexfunction,andlet\nX\nbearandomvariable.\nThen:\nE[\nf\n(\nX\n)]\n\nf\n(E\nX\n)\n:\nMoreover,if\nf\nisstrictlyconvex,thenE[\nf\n(\nX\n)]=\nf\n(E\nX\n)holdstrueifand\nonlyif\nX\n=E[\nX\n]withprobability1(i.e.,if\nX\nisaconstant).\nRecallourconventionofoccasionallydroppingtheparentheseswhenwrit-\ningexpectations,sointhetheoremabove,\nf\n(E\nX\n)=\nf\n(E[\nX\n]).\nForaninterpretationofthetheorem,considerthebelow.\nHere,\nf\nisaconvexfunctionshownbythesolidline.Also,\nX\nisarandom\nvariablethathasa0.5chanceoftakingthevalue\na\n,anda0.5chanceof\n"}, {"page_number": 142, "text": "142\ntakingthevalue\nb\n(indicatedonthe\nx\n-axis).Thus,theexpectedvalueof\nX\nisgivenbythemidpointbetween\na\nand\nb\n.\nWealsoseethevalues\nf\n(\na\n),\nf\n(\nb\n)and\nf\n(E[\nX\n])indicatedonthe\ny\n-axis.\nMoreover,thevalueE[\nf\n(\nX\n)]isnowthemidpointonthe\ny\n-axisbetween\nf\n(\na\n)\nand\nf\n(\nb\n).Fromourexample,weseethatbecause\nf\nisconvex,itmustbethe\ncasethatE[\nf\n(\nX\n)]\n\nf\n(E\nX\n).\nIncidentally,quitealotofpeoplehavetroublerememberingwhichway\ntheinequalitygoes,andrememberingapicturelikethisisagoodwayto\nquicklyouttheanswer.\nRemark.\nRecallthat\nf\nis[strictly]concaveifandonlyif\n\nf\nis[strictly]\nconvex(i.e.,\nf\n00\n(\nx\n)\n\n0or\nH\n\n0).Jensen'sinequalityalsoholdsforconcave\nfunctions\nf\n,butwiththedirectionofalltheinequalitiesreversed(E[\nf\n(\nX\n)]\n\nf\n(E\nX\n),etc.).\n11.3GeneralEMalgorithms\nSupposewehaveanestimationprobleminwhichwehaveatrainingset\nf\nx\n(1)\n;:::;x\n(\nn\n)\ng\nconsistingof\nn\nindependentexamples.Wehavealatentvari-\nablemodel\np\n(\nx;z\n;\n\n)with\nz\nbeingthelatentvariable(whichforsimplicityis\nassumedtotakenumberofvalues).Thedensityfor\nx\ncanbeobtained\nbymarginalizedoverthelatentvariable\nz\n:\np\n(\nx\n;\n\n)=\nX\nz\np\n(\nx;z\n;\n\n)(11.1)\nWewishtottheparameters\n\nbymaximizingthelog-likelihoodofthe\ndata,by\n`\n(\n\n)=\nn\nX\ni\n=1\nlog\np\n(\nx\n(\ni\n)\n;\n\n)\n(11.2)\nWecanrewritetheobjectiveintermsofthejointdensity\np\n(\nx;z\n;\n\n)by\n`\n(\n\n)=\nn\nX\ni\n=1\nlog\np\n(\nx\n(\ni\n)\n;\n\n)\n(11.3)\n=\nn\nX\ni\n=1\nlog\nX\nz\n(\ni\n)\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\n:\n(11.4)\nBut,explicitlythemaximumlikelihoodestimatesoftheparameters\n\nmaybehardsinceitwillresultinnon-convexoptimizationprob-\n"}, {"page_number": 143, "text": "143\nlems.\n3\nHere,the\nz\n(\ni\n)\n'sarethelatentrandomvariables;anditisoftenthecase\nthatifthe\nz\n(\ni\n)\n'swereobserved,thenmaximumlikelihoodestimationwould\nbeeasy.\nInsuchasetting,theEMalgorithmgivesantmethodformax-\nimumlikelihoodestimation.Maximizing\n`\n(\n\n)explicitlymightbe\nandourstrategywillbetoinsteadrepeatedlyconstructalower-boundon\n`\n(E-step),andthenoptimizethatlower-bound(M-step).\n4\nItturnsoutthatthesummation\nP\nn\ni\n=1\nisnotessentialhere,andtowardsa\nsimplerexpositionoftheEMalgorithm,wewillconsideroptimizingthe\nthelikelihoodlog\np\n(\nx\n)for\nasingleexample\nx\n.Afterwederivethealgorithm\nforoptimizinglog\np\n(\nx\n),wewillconvertittoanalgorithmthatworksfor\nn\nexamplesbyaddingbackthesumtoeachoftherelevantequations.Thus,\nnowweaimtooptimizelog\np\n(\nx\n;\n\n)whichcanberewrittenas\nlog\np\n(\nx\n;\n\n)=log\nX\nz\np\n(\nx;z\n;\n\n)(11.5)\nLet\nQ\nbeadistributionoverthepossiblevaluesof\nz\n.Thatis,\nP\nz\nQ\n(\nz\n)=1,\nQ\n(\nz\n)\n\n0).\nConsiderthefollowing:\n5\nlog\np\n(\nx\n;\n\n)=log\nX\nz\np\n(\nx;z\n;\n\n)\n=log\nX\nz\nQ\n(\nz\n)\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n(11.6)\n\nX\nz\nQ\n(\nz\n)log\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n(11.7)\nThelaststepofthisderivationusedJensen'sinequality.Sp,\nf\n(\nx\n)=log\nx\nisaconcavefunction,since\nf\n00\n(\nx\n)=\n\n1\n=x\n2\n<\n0overitsdomain\n3\nIt'smostlyanempiricalobservationthattheoptimizationproblemistoop-\ntimize.\n4\nEmpirically,theE-stepandM-stepcanoftenbecomputedmoretlythanop-\ntimizingthefunction\n`\n(\n\n)directly.However,itdoesn'tnecessarilymeanthatalternating\nthetwostepscanalwaysconvergetotheglobaloptimumof\n`\n(\n\n).Evenformixtureof\nGaussians,theEMalgorithmcaneitherconvergetoaglobaloptimumorgetstuck,de-\npendingonthepropertiesofthetrainingdata.Empirically,forreal-worlddata,oftenEM\ncanconvergetoasolutionwithrelativelyhighlikelihood(ifnottheoptimum),andthe\ntheorybehinditisstilllargelynotunderstood.\n5\nIf\nz\nwerecontinuous,then\nQ\nwouldbeadensity,andthesummationsover\nz\ninour\ndiscussionarereplacedwithintegralsover\nz\n.\n"}, {"page_number": 144, "text": "144\nx\n2\nR\n+\n.Also,theterm\nX\nz\nQ\n(\nz\n)\n\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n\ninthesummationisjustanexpectationofthequantity[\np\n(\nx;z\n;\n\n)\n=Q\n(\nz\n)]with\nrespectto\nz\ndrawnaccordingtothedistributiongivenby\nQ\n.\n6\nByJensen's\ninequality,wehave\nf\n\nE\nz\n\u02d8\nQ\n\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n\n\nE\nz\n\u02d8\nQ\n\nf\n\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n\n;\nwherethe\\\nz\n\u02d8\nQ\n\"subscriptsaboveindicatethattheexpectationsarewith\nrespectto\nz\ndrawnfrom\nQ\n.ThisallowedustogofromEquation(11.6)to\nEquation(11.7).\nNow,for\nany\ndistribution\nQ\n,theformula(11.7)givesalower-boundon\nlog\np\n(\nx\n;\n\n).Therearemanypossiblechoicesforthe\nQ\n's.Whichshouldwe\nchoose?Well,ifwehavesomecurrentguess\n\noftheparameters,itseems\nnaturaltotrytomakethelower-boundtightatthatvalueof\n\n.I.e.,wewill\nmaketheinequalityaboveholdwithequalityatourparticularvalueof\n\n.\nTomaketheboundtightforaparticularvalueof\n\n,weneedforthestep\ninvolvingJensen'sinequalityinourderivationabovetoholdwithequality.\nForthistobetrue,weknowitistthattheexpectationbetaken\novera\\constant\"-valuedrandomvariable.I.e.,werequirethat\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n=\nc\nforsomeconstant\nc\nthatdoesnotdependon\nz\n.Thisiseasilyaccomplished\nbychoosing\nQ\n(\nz\n)\n/\np\n(\nx;z\n;\n\n)\n:\nActually,sinceweknow\nP\nz\nQ\n(\nz\n)=1(becauseitisadistribution),this\nfurthertellsusthat\nQ\n(\nz\n)=\np\n(\nx;z\n;\n\n)\nP\nz\np\n(\nx;z\n;\n\n)\n=\np\n(\nx;z\n;\n\n)\np\n(\nx\n;\n\n)\n=\np\n(\nz\nj\nx\n;\n\n)\n(11.8)\n6\nWenotethatthenotion\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\nonlymakessenseif\nQ\n(\nz\n)\n6\n=0whenever\np\n(\nx;z\n;\n\n)\n6\n=0.\nHereweimplicitlyassumethatweonlyconsiderthose\nQ\nwithsuchaproperty.\n"}, {"page_number": 145, "text": "145\nThus,wesimplysetthe\nQ\n'stobetheposteriordistributionofthe\nz\n'sgiven\nx\nandthesettingoftheparameters\n\n.\nIndeed,wecandirectlyverifythatwhen\nQ\n(\nz\n)=\np\n(\nz\nj\nx\n;\n\n),thenequa-\ntion(11.7)isanequalitybecause\nX\nz\nQ\n(\nz\n)log\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n=\nX\nz\np\n(\nz\nj\nx\n;\n\n)log\np\n(\nx;z\n;\n\n)\np\n(\nz\nj\nx\n;\n\n)\n=\nX\nz\np\n(\nz\nj\nx\n;\n\n)log\np\n(\nz\nj\nx\n;\n\n)\np\n(\nx\n;\n\n)\np\n(\nz\nj\nx\n;\n\n)\n=\nX\nz\np\n(\nz\nj\nx\n;\n\n)log\np\n(\nx\n;\n\n)\n=log\np\n(\nx\n;\n\n)\nX\nz\np\n(\nz\nj\nx\n;\n\n)\n=log\np\n(\nx\n;\n\n)(because\nP\nz\np\n(\nz\nj\nx\n;\n\n)=1)\nForconvenience,wecalltheexpressioninEquation(11.7)the\nevidence\nlowerbound\n(ELBO)andwedenoteitby\nELBO(\nx\n;\nQ;\n)=\nX\nz\nQ\n(\nz\n)log\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\n(11.9)\nWiththisequation,wecanre-writeequation(11.7)as\n8\nQ;;x;\nlog\np\n(\nx\n;\n\n)\n\nELBO(\nx\n;\nQ;\n)(11.10)\nIntuitively,theEMalgorithmalternativelyupdates\nQ\nand\n\nbya)set-\nting\nQ\n(\nz\n)=\np\n(\nz\nj\nx\n;\n\n)followingEquation(11.8)sothatELBO(\nx\n;\nQ;\n)=\nlog\np\n(\nx\n;\n\n)for\nx\nandthecurrent\n\n,andb)maximizingELBO(\nx\n;\nQ;\n)w.r.t\n\nwhilethechoiceof\nQ\n.\nRecallthatallthediscussionabovewasundertheassumptionthatwe\naimtooptimizethelog-likelihoodlog\np\n(\nx\n;\n\n)forasingleexample\nx\n.Itturns\noutthatwithmultipletrainingexamples,thebasicideaisthesameandwe\nonlyneedstotakeasumoverexamplesatrelevantplaces.Next,wewill\nbuildtheevidencelowerboundformultipletrainingexamplesandmakethe\nEMalgorithmformal.\nRecallwehaveatrainingset\nf\nx\n(1)\n;:::;x\n(\nn\n)\ng\n.Notethattheoptimalchoice\nof\nQ\nis\np\n(\nz\nj\nx\n;\n\n),anditdependsontheparticularexample\nx\n.Thereforehere\nwewillintroduce\nn\ndistributions\nQ\n1\n;:::;Q\nn\n,oneforeachexample\nx\n(\ni\n)\n.For\neachexample\nx\n(\ni\n)\n,wecanbuildtheevidencelowerbound\nlog\np\n(\nx\n(\ni\n)\n;\n\n)\n\nELBO(\nx\n(\ni\n)\n;\nQ\ni\n;\n)=\nX\nz\n(\ni\n)\nQ\ni\n(\nz\n(\ni\n)\n)log\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n"}, {"page_number": 146, "text": "146\nTakingsumoveralltheexamples,weobtainalowerboundforthelog-\nlikelihood\n`\n(\n\n)\n\nX\ni\nELBO(\nx\n(\ni\n)\n;\nQ\ni\n;\n)(11.11)\n=\nX\ni\nX\nz\n(\ni\n)\nQ\ni\n(\nz\n(\ni\n)\n)log\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\nFor\nany\nsetofdistributions\nQ\n1\n;:::;Q\nn\n,theformula(11.11)givesalower-\nboundon\n`\n(\n\n),andanalogoustotheargumentaroundequation(11.8),the\nQ\ni\nthatattainsequality\nQ\ni\n(\nz\n(\ni\n)\n)=\np\n(\nz\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\nThus,wesimplysetthe\nQ\ni\n'stobetheposteriordistributionofthe\nz\n(\ni\n)\n's\ngiven\nx\n(\ni\n)\nwiththecurrentsettingoftheparameters\n\n.\nNow,forthischoiceofthe\nQ\ni\n's,Equation(11.11)givesalower-boundon\ntheloglikelihood\n`\nthatwe'retryingtomaximize.ThisistheE-step.Inthe\nM-stepofthealgorithm,wethenmaximizeourformulainEquation(11.11)\nwithrespecttotheparameterstoobtainanewsettingofthe\n\n's.Repeatedly\ncarryingoutthesetwostepsgivesustheEMalgorithm,whichisasfollows:\nRepeatuntilconvergence\nf\n(E-step)Foreach\ni\n,set\nQ\ni\n(\nz\n(\ni\n)\n):=\np\n(\nz\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n)\n:\n(M-step)Set\n\n:=argmax\n\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\ni\n;\n)\n=argmax\n\nX\ni\nX\nz\n(\ni\n)\nQ\ni\n(\nz\n(\ni\n)\n)log\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n:\n(11.12)\ng\nHowdoweknowifthisalgorithmwillconverge?Well,suppose\n\n(\nt\n)\nand\n\n(\nt\n+1)\naretheparametersfromtwosuccessiveiterationsofEM.Wewillnow\nprovethat\n`\n(\n\n(\nt\n)\n)\n\n`\n(\n\n(\nt\n+1)\n),whichshowsEMalwaysmonotonicallyim-\nprovesthelog-likelihood.Thekeytoshowingthisresultliesinourchoiceof\n"}, {"page_number": 147, "text": "147\nthe\nQ\ni\n's.Sp,ontheiterationofEMinwhichtheparametershad\nstartedoutas\n\n(\nt\n)\n,wewouldhavechosen\nQ\n(\nt\n)\ni\n(\nz\n(\ni\n)\n):=\np\n(\nz\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n(\nt\n)\n).We\nsawearlierthatthischoiceensuresthatJensen'sinequality,asappliedtoget\nEquation(11.11),holdswithequality,andhence\n`\n(\n\n(\nt\n)\n)=\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\n(\nt\n)\ni\n;\n(\nt\n)\n)(11.13)\nTheparameters\n\n(\nt\n+1)\narethenobtainedbymaximizingtherighthandside\noftheequationabove.Thus,\n`\n(\n\n(\nt\n+1)\n)\n\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\n(\nt\n)\ni\n;\n(\nt\n+1)\n)\n(becauseineqaulity(11.11)holdsforall\nQ\nand\n\n)\n\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\n(\nt\n)\ni\n;\n(\nt\n)\n)(seereasonbelow)\n=\n`\n(\n\n(\nt\n)\n)(byequation(11.13))\nwherethelastinequalityfollowsfromthat\n\n(\nt\n+1)\nischosenexplicitlytobe\nargmax\n\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\n(\nt\n)\ni\n;\n)\nHence,EMcausesthelikelihoodtoconvergemonotonically.Inourde-\nscriptionoftheEMalgorithm,wesaidwe'drunituntilconvergence.Given\ntheresultthatwejustshowed,onereasonableconvergencetestwouldbe\ntocheckiftheincreasein\n`\n(\n\n)betweensuccessiveiterationsissmallerthan\nsometoleranceparameter,andtodeclareconvergenceifEMisimproving\n`\n(\n\n)tooslowly.\nRemark.\nIfwede(byoverloadingELBO(\n\n))\nELBO(\nQ;\n)=\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\ni\n;\n)=\nX\ni\nX\nz\n(\ni\n)\nQ\ni\n(\nz\n(\ni\n)\n)log\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n(11.14)\nthenweknow\n`\n(\n\n)\n\nELBO(\nQ;\n)fromourpreviousderivation.TheEM\ncanalsobeviewedanalternatingmaximizationalgorithmonELBO(\nQ;\n),\ninwhichtheE-stepmaximizesitwithrespectto\nQ\n(checkthisyourself),and\ntheM-stepmaximizesitwithrespectto\n\n.\n"}, {"page_number": 148, "text": "148\n11.3.1OtherinterpretationofELBO\nLetELBO(\nx\n;\nQ;\n)=\nP\nz\nQ\n(\nz\n)log\np\n(\nx;z\n;\n\n)\nQ\n(\nz\n)\nbeasinequation(11.9).\nThereareseveralotherformsofELBO.First,wecanrewrite\nELBO(\nx\n;\nQ;\n)=E\nz\n\u02d8\nQ\n[log\np\n(\nx;z\n;\n\n)]\n\nE\nz\n\u02d8\nQ\n[log\nQ\n(\nz\n)]\n=E\nz\n\u02d8\nQ\n[log\np\n(\nx\nj\nz\n;\n\n)]\n\nD\nKL\n(\nQ\nk\np\nz\n)(11.15)\nwhereweuse\np\nz\ntodenotethemarginaldistributionof\nz\n(underthedistri-\nbution\np\n(\nx;z\n;\n\n)),and\nD\nKL\n()denotestheKLdivergence\nD\nKL\n(\nQ\nk\np\nz\n)=\nX\nz\nQ\n(\nz\n)log\nQ\n(\nz\n)\np\n(\nz\n)\n(11.16)\nInmanycases,themarginaldistributionof\nz\ndoesnotdependontheparam-\neter\n\n.Inthiscase,wecanseethatmaximizingELBOover\n\nisequivalent\ntomaximizingthetermin(11.15).Thiscorrespondstomaximizingthe\nconditionallikelihoodof\nx\nconditionedon\nz\n,whichisoftenasimplerquestion\nthantheoriginalquestion.\nAnotherformofELBO(\n\n)is(pleaseverifyyourself)\nELBO(\nx\n;\nQ;\n)=log\np\n(\nx\n)\n\nD\nKL\n(\nQ\nk\np\nz\nj\nx\n)(11.17)\nwhere\np\nz\nj\nx\nistheconditionaldistributionof\nz\ngiven\nx\nundertheparameter\n\n.ThisformsshowsthatthemaximizerofELBO(\nQ;\n)over\nQ\nisobtained\nwhen\nQ\n=\np\nz\nj\nx\n,whichwasshowninequation(11.8)before.\n11.4MixtureofGaussiansrevisited\nArmedwithourgeneraloftheEMalgorithm,let'sgobacktoour\noldexampleofingtheparameters\n\u02da\n,\n\nandinamixtureofGaussians.\nForthesakeofbrevity,wecarryoutthederivationsfortheM-stepupdates\nonlyfor\n\u02da\nand\n\nj\n,andleavetheupdatesfor\nj\nasanexerciseforthereader.\nTheE-stepiseasy.Followingouralgorithmderivationabove,wesimply\ncalculate\nw\n(\ni\n)\nj\n=\nQ\ni\n(\nz\n(\ni\n)\n=\nj\n)=\nP\n(\nz\n(\ni\n)\n=\nj\nj\nx\n(\ni\n)\n;\n\u02da;\n\n:\nHere,\\\nQ\ni\n(\nz\n(\ni\n)\n=\nj\n)\"denotestheprobabilityof\nz\n(\ni\n)\ntakingthevalue\nj\nunder\nthedistribution\nQ\ni\n.\n"}, {"page_number": 149, "text": "149\nNext,intheM-step,weneedtomaximize,withrespecttoourparameters\n\u02da;\nthequantity\nn\nX\ni\n=1\nX\nz\n(\ni\n)\nQ\ni\n(\nz\n(\ni\n)\n)log\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\u02da;\n\nQ\ni\n(\nz\n(\ni\n)\n)\n=\nn\nX\ni\n=1\nk\nX\nj\n=1\nQ\ni\n(\nz\n(\ni\n)\n=\nj\n)log\np\n(\nx\n(\ni\n)\nj\nz\n(\ni\n)\n=\nj\n;\n\n\np\n(\nz\n(\ni\n)\n=\nj\n;\n\u02da\n)\nQ\ni\n(\nz\n(\ni\n)\n=\nj\n)\n=\nn\nX\ni\n=1\nk\nX\nj\n=1\nw\n(\ni\n)\nj\nlog\n1\n(2\n\u02c7\n)\nd=\n2\nj\n\nj\nj\n1\n=\n2\nexp\n\n\n1\n2\n(\nx\n(\ni\n)\n\n\nj\n)\nT\n\n\n1\nj\n(\nx\n(\ni\n)\n\n\nj\n)\n\n\n\u02da\nj\nw\n(\ni\n)\nj\nLet'smaximizethiswithrespectto\n\nl\n.Ifwetakethederivativewithrespect\nto\n\nl\n,we\nr\n\nl\nn\nX\ni\n=1\nk\nX\nj\n=1\nw\n(\ni\n)\nj\nlog\n1\n(2\n\u02c7\n)\nd=\n2\nj\n\nj\nj\n1\n=\n2\nexp\n\n\n1\n2\n(\nx\n(\ni\n)\n\n\nj\n)\nT\n\n\n1\nj\n(\nx\n(\ni\n)\n\n\nj\n)\n\n\n\u02da\nj\nw\n(\ni\n)\nj\n=\n\n\nl\nn\nX\ni\n=1\nk\nX\nj\n=1\nw\n(\ni\n)\nj\n1\n2\n(\nx\n(\ni\n)\n\n\nj\n)\nT\n\n\n1\nj\n(\nx\n(\ni\n)\n\n\nj\n)\n=\n1\n2\nn\nX\ni\n=1\nw\n(\ni\n)\nl\nr\n\nl\n2\n\nT\nl\n\n\n1\nl\nx\n(\ni\n)\n\n\nT\nl\n\n\n1\nl\n\nl\n=\nn\nX\ni\n=1\nw\n(\ni\n)\nl\n\n\n\n1\nl\nx\n(\ni\n)\n\n\n\n1\nl\n\nl\n\nSettingthistozeroandsolvingfor\n\nl\nthereforeyieldstheupdaterule\n\nl\n:=\nP\nn\ni\n=1\nw\n(\ni\n)\nl\nx\n(\ni\n)\nP\nn\ni\n=1\nw\n(\ni\n)\nl\n;\nwhichwaswhatwehadintheprevioussetofnotes.\nLet'sdoonemoreexample,andderivetheM-stepupdatefortheparam-\neters\n\u02da\nj\n.Groupingtogetheronlythetermsthatdependon\n\u02da\nj\n,wethat\nweneedtomaximize\nn\nX\ni\n=1\nk\nX\nj\n=1\nw\n(\ni\n)\nj\nlog\n\u02da\nj\n:\nHowever,thereisanadditionalconstraintthatthe\n\u02da\nj\n'ssumto1,sincethey\nrepresenttheprobabilities\n\u02da\nj\n=\np\n(\nz\n(\ni\n)\n=\nj\n;\n\u02da\n).Todealwiththeconstraint\n"}, {"page_number": 150, "text": "150\nthat\nP\nk\nj\n=1\n\u02da\nj\n=1,weconstructtheLagrangian\nL\n(\n\u02da\n)=\nn\nX\ni\n=1\nk\nX\nj\n=1\nw\n(\ni\n)\nj\nlog\n\u02da\nj\n+\n\n(\nk\nX\nj\n=1\n\u02da\nj\n\n1)\n;\nwhere\n\nistheLagrangemultiplier.\n7\nTakingderivatives,we\n@\n@\u02da\nj\nL\n(\n\u02da\n)=\nn\nX\ni\n=1\nw\n(\ni\n)\nj\n\u02da\nj\n+\n\nSettingthistozeroandsolving,weget\n\u02da\nj\n=\nP\nn\ni\n=1\nw\n(\ni\n)\nj\n\n\nI.e.,\n\u02da\nj\n/\nP\nn\ni\n=1\nw\n(\ni\n)\nj\n.Usingtheconstraintthat\nP\nj\n\u02da\nj\n=1,weeasily\nthat\n\n\n=\nP\nn\ni\n=1\nP\nk\nj\n=1\nw\n(\ni\n)\nj\n=\nP\nn\ni\n=1\n1=\nn\n.(Thisusedthefactthat\nw\n(\ni\n)\nj\n=\nQ\ni\n(\nz\n(\ni\n)\n=\nj\n),andsinceprobabilitiessumto1,\nP\nj\nw\n(\ni\n)\nj\n=1.)Wetherefore\nhaveourM-stepupdatesfortheparameters\n\u02da\nj\n:\n\u02da\nj\n:=\n1\nn\nn\nX\ni\n=1\nw\n(\ni\n)\nj\n:\nThederivationfortheM-stepupdatesto\nj\narealsoentirelystraightfor-\nward.\n11.5Variationalinferenceandvariational\nauto-encoder(optionalreading)\nLooselyspeaking,variationalauto-encoderKingmaandWelling[2013]gen-\nerallyreferstoafamilyofalgorithmsthatextendtheEMalgorithmstomore\ncomplexmodelsparameterizedbyneuralnetworks.Itextendsthetechnique\nofvariationalinferencewiththeadditional\\re-parametrizationtrick\"which\nwillbeintroducedbelow.Variationalauto-encodermaynotgivethebest\nperformanceformanydatasets,butitcontainsseveralcentralideasabout\nhowtoextendEMalgorithmstohigh-dimensionalcontinuouslatentvariables\n7\nWedon'tneedtoworryabouttheconstraintthat\n\u02da\nj\n\n0,becauseaswe'llshortlysee,\nthesolutionwe'llfromthisderivationwillautomaticallysatisfythatanyway.\n"}, {"page_number": 151, "text": "151\nwithnon-linearmodels.Understandingitwilllikelygiveyouthelanguage\nandbackgroundstounderstandvariousrecentpapersrelatedtoit.\nAsarunningexample,wewillconsiderthefollowingparameterizationof\np\n(\nx;z\n;\n\n)byaneuralnetwork.Let\n\nbethecollectionoftheweightsofa\nneuralnetwork\ng\n(\nz\n;\n\n)thatmaps\nz\n2\nR\nk\nto\nR\nd\n.Let\nz\n\u02d8N\n(0\n;I\nk\n\nk\n)(11.18)\nx\nj\nz\n\u02d8N\n(\ng\n(\nz\n;\n\n)\n;\u02d9\n2\nI\nd\n\nd\n)(11.19)\nHere\nI\nk\n\nk\ndenotesidentitymatrixofdimension\nk\nby\nk\n,and\n\u02d9\nisascalarthat\nweassumetobeknownforsimplicity.\nFortheGaussianmixturemodelsinSection11.4,theoptimalchoiceof\nQ\n(\nz\n)=\np\n(\nz\nj\nx\n;\n\n)foreach\n\n,thatistheposteriordistributionof\nz\n,\ncanbeanalyticallycomputed.Inmanymorecomplexmodelssuchasthe\nmodel(11.19),it'sintractabletocomputetheexacttheposteriordistribution\np\n(\nz\nj\nx\n;\n\n).\nRecallthatfromequation(11.10),ELBOisalwaysalowerboundforany\nchoiceof\nQ\n,andtherefore,wecanalsoaimforan\napproximation\nof\nthetrueposteriordistribution.Often,onehastousesomeparticularform\ntoapproximatethetrueposteriordistribution.Let\nQ\nbeafamilyof\nQ\n'sthat\nweareconsidering,andwewillaimtoa\nQ\nwithinthefamilyof\nQ\nthatis\nclosesttothetrueposteriordistribution.Toformalize,recalltheof\ntheELBOlowerboundasafunctionof\nQ\nand\n\ninequation(11.14)\nELBO(\nQ;\n)=\nn\nX\ni\n=1\nELBO(\nx\n(\ni\n)\n;\nQ\ni\n;\n)=\nX\ni\nX\nz\n(\ni\n)\nQ\ni\n(\nz\n(\ni\n)\n)log\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\nRecallthatEMcanbeviewedasalternatingmaximizationof\nELBO(\nQ;\n).Hereinstead,weoptimizetheEBLOover\nQ\n2Q\nmax\nQ\n2Q\nmax\n\nELBO(\nQ;\n)(11.20)\nNowthenextquestioniswhatformof\nQ\n(orwhatstructuralassumptions\ntomakeabout\nQ\n)allowsustotlymaximizetheobjectiveabove.When\nthelatentvariable\nz\narehigh-dimensionaldiscretevariables,onepopularas-\nsumptionisthe\nmeanassumption\n,whichassumesthat\nQ\ni\n(\nz\n)givesa\ndistributionwithindependentcoordinates,orinotherwords,\nQ\ni\ncanbede-\ncomposedinto\nQ\ni\n(\nz\n)=\nQ\n1\ni\n(\nz\n1\n)\n\nQ\nk\ni\n(\nz\nk\n).Therearetremendousapplications\nofmeanassumptionstolearninggenerativemodelswithdiscretelatent\nvariables,andwerefertoBleietal.[2017]forasurveyofthesemodelsand\n"}, {"page_number": 152, "text": "152\ntheirimpacttoawiderangeofapplicationsincludingcomputationalbiology,\ncomputationalneuroscience,socialsciences.Wewillnotgetintothedetails\naboutthediscretelatentvariablecases,andourmainfocusistodealwith\ncontinuouslatentvariables,whichrequiresnotonlymeanassumptions,\nbutadditionaltechniques.\nWhen\nz\n2\nR\nk\nisacontinuouslatentvariable,thereareseveraldecisionsto\nmaketowardssuccessfullyoptimizing(11.20).Firstweneedtogiveasuccinct\nrepresentationofthedistribution\nQ\ni\nbecauseitisoverannumberof\npoints.Anaturalchoiceistoassume\nQ\ni\nisaGaussiandistributionwithsome\nmeanandvariance.Wewouldalsoliketohavemoresuccinctrepresentation\nofthemeansof\nQ\ni\nofalltheexamples.Notethat\nQ\ni\n(\nz\n(\ni\n)\n)issupposedto\napproximate\np\n(\nz\n(\ni\n)\nj\nx\n(\ni\n)\n;\n\n).Itwouldmakesenseletallthemeansofthe\nQ\ni\n's\nbesomefunctionof\nx\n(\ni\n)\n.Concretely,let\nq\n(\n\n;\n\u02da\n)\n;v\n(\n\n;\n\u02da\n)betwofunctionsthat\nmapfromdimension\nd\nto\nk\n,whichareparameterizedby\n\u02da\nand\n \n,weassume\nthat\nQ\ni\n=\nN\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)\n;\ndiag(\nv\n(\nx\n(\ni\n)\n;\n \n))\n2\n)(11.21)\nHerediag(\nw\n)meansthe\nk\n\nk\nmatrixwiththeentriesof\nw\n2\nR\nk\nonthe\ndiagonal.Inotherwords,thedistribution\nQ\ni\nisassumedtobeaGaussian\ndistributionwithindependentcoordinates,andthemeanandstandardde-\nviationsaregovernedby\nq\nand\nv\n.Ofteninvariationalauto-encoder,\nq\nand\nv\narechosentobeneuralnetworks.\n8\nInrecentdeeplearningliterature,often\nq;v\narecalled\nencoder\n(inthesenseofencodingthedataintolatentcode),\nwhereas\ng\n(\nz\n;\n\n)ifoftenreferredtoasthe\ndecoder.\nWeremarkthat\nQ\ni\nofsuchforminmanycasesareveryfarfromagoodap-\nproximationofthetrueposteriordistribution.However,someapproximation\nisnecessaryforfeasibleoptimization.Infact,theformof\nQ\ni\nneedstosatisfy\notherrequirements(whichhappenedtobebytheform(11.21))\nBeforeoptimizingtheELBO,let'sverifywhetherwecanently\nevaluatethevalueoftheELBOfor\nQ\noftheform(11.21)and\n\n.We\nrewritetheELBOasafunctionof\n\u02da; ;\nby\nELBO(\n\u02da; ;\n)=\nn\nX\ni\n=1\nE\nz\n(\ni\n)\n\u02d8\nQ\ni\n\nlog\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n\n;\n(11.22)\nwhere\nQ\ni\n=\nN\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)\n;\ndiag(\nv\n(\nx\n(\ni\n)\n;\n \n))\n2\n)\nNotethattoevaluate\nQ\ni\n(\nz\n(\ni\n)\n)insidetheexpectation,weshouldbeable\nto\ncomputethedensityof\nQ\ni\n.ToestimatetheexpectationE\nz\n(\ni\n)\n\u02d8\nQ\ni\n,we\n8\nq\nand\nv\ncanalsoshareparameters.Wesweepthislevelofdetailsundertheruginthis\nnote.\n"}, {"page_number": 153, "text": "153\nshouldbeable\ntosamplefromdistribution\nQ\ni\nsothatwecanbuildan\nempiricalestimatorwithsamples.IthappensthatforGaussiandistribution\nQ\ni\n=\nN\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)\n;\ndiag(\nv\n(\nx\n(\ni\n)\n;\n \n))\n2\n),weareabletobebothetly.\nNowlet'soptimizetheELBO.Itturnsoutthatwecanrungradientascent\nover\n\u02da; ;\ninsteadofalternatingmaximization.Thereisnostrongneedto\ncomputethemaximumovereachvariableatamuchgreatercost.(ForGaus-\nsianmixturemodelinSection11.4,computingthemaximumisanalytically\nfeasibleandrelativelycheap,andthereforewedidalternatingmaximization.)\nMathematically,let\n\nbethelearningrate,thegradientascentstepis\n\n:=\n\n+\n\nr\n\nELBO(\n\u02da; ;\n)\n\u02da\n:=\n\u02da\n+\n\nr\n\u02da\nELBO(\n\u02da; ;\n)\n \n:=\n \n+\n\nr\n \nELBO(\n\u02da; ;\n)\nComputingthegradientover\n\nissimplebecause\nr\n\nELBO(\n\u02da; ;\n)=\nr\n\nn\nX\ni\n=1\nE\nz\n(\ni\n)\n\u02d8\nQ\ni\n\nlog\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n\n=\nr\n\nn\nX\ni\n=1\nE\nz\n(\ni\n)\n\u02d8\nQ\ni\n\nlog\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\n\n=\nn\nX\ni\n=1\nE\nz\n(\ni\n)\n\u02d8\nQ\ni\n\nr\n\nlog\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\n\n;\n(11.23)\nButcomputingthegradientover\n\u02da\nand\n \nistrickybecausethesam-\nplingdistribution\nQ\ni\ndependson\n\u02da\nand\n \n.(Abstractlyspeaking,theis-\nsuewefacecanbeastheproblemofcomputingthegradi-\nentE\nz\n\u02d8\nQ\n\u02da\n[\nf\n(\n\u02da\n)]withrespecttovariable\n\u02da\n.Weknowthatingeneral,\nr\nE\nz\n\u02d8\nQ\n\u02da\n[\nf\n(\n\u02da\n)]\n6\n=E\nz\n\u02d8\nQ\n\u02da\n[\nr\nf\n(\n\u02da\n)]becausethedependencyof\nQ\n\u02da\non\n\u02da\nhastobe\ntakenintoaccountaswell.)\nTheideathatcomestorescueistheso-called\nre-parameterization\ntrick\n:werewrite\nz\n(\ni\n)\n\u02d8\nQ\ni\n=\nN\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)\n;\ndiag(\nv\n(\nx\n(\ni\n)\n;\n \n))\n2\n)inanequivalent\nway:\nz\n(\ni\n)\n=\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\nwhere\n\u02d8\n(\ni\n)\n\u02d8N\n(0\n;I\nk\n\nk\n)(11.24)\nHere\nx\n\ny\ndenotestheentry-wiseproductoftwovectorsofthesame\ndimension.Hereweusedthefactthat\nx\n\u02d8\nN\n(\n\u02d9\n2\n)isequivalenttothat\nx\n=\n\n+\n\u02d8\u02d9\nwith\n\u02d8\n\u02d8\nN\n(0\n;\n1).Wemostlyjustusedthisfactineverydimension\nsimultaneouslyfortherandomvariable\nz\n(\ni\n)\n\u02d8\nQ\ni\n.\n"}, {"page_number": 154, "text": "154\nWiththisre-parameterization,wehavethat\nE\nz\n(\ni\n)\n\u02d8\nQ\ni\n\nlog\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n\n(11.25)\n=E\n\u02d8\n(\ni\n)\n\u02d8N\n(0\n;\n1)\n\nlog\np\n(\nx\n(\ni\n)\n;q\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\n;\n\n)\nQ\ni\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\n)\n\nItfollowsthat\nr\n\u02da\nE\nz\n(\ni\n)\n\u02d8\nQ\ni\n\nlog\np\n(\nx\n(\ni\n)\n;z\n(\ni\n)\n;\n\n)\nQ\ni\n(\nz\n(\ni\n)\n)\n\n=\nr\n\u02da\nE\n\u02d8\n(\ni\n)\n\u02d8N\n(0\n;\n1)\n\nlog\np\n(\nx\n(\ni\n)\n;q\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\n;\n\n)\nQ\ni\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\n)\n\n=E\n\u02d8\n(\ni\n)\n\u02d8N\n(0\n;\n1)\n\nr\n\u02da\nlog\np\n(\nx\n(\ni\n)\n;q\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\n;\n\n)\nQ\ni\n(\nq\n(\nx\n(\ni\n)\n;\n\u02da\n)+\nv\n(\nx\n(\ni\n)\n;\n \n)\n\n\u02d8\n(\ni\n)\n)\n\nWecannowsamplemultiplecopiesof\n\u02d8\n(\ni\n)\n'stoestimatethetheexpecta-\ntionintheRHSoftheequationabove.\n9\nWecanestimatethegradientwith\nrespectto\n \nsimilarly,andwiththese,wecanimplementthegradientascent\nalgorithmtooptimizetheELBOover\n\u02da; ;:\nTherearenotmanyhigh-dimensionaldistributionswithanalyticallycom-\nputabledensityfunctionareknowntobere-parameterizable.Wereferto\nKingmaandWelling[2013]forafewotherchoicesthatcanreplaceGaussian\ndistribution.\n9\nEmpiricallypeoplesometimesjustuseonesampletoestimateitformaximumcom-\nputational.\n"}, {"page_number": 155, "text": "Chapter12\nPrincipalcomponentsanalysis\nInthissetofnotes,wewilldevelopamethod,PrincipalComponentsAnalysis\n(PCA),thattriestoidentifythesubspaceinwhichthedataapproximately\nlies.PCAiscomputationallyt:itwillrequireonlyaneigenvector\ncalculation(easilydonewiththe\neig\nfunctioninMatlab).\nSupposewearegivenadataset\nf\nx\n(\ni\n)\n;\ni\n=1\n;:::;n\ng\nofattributesof\nn\ndif-\nferenttypesofautomobiles,suchastheirmaximumspeed,turnradius,and\nsoon.Let\nx\n(\ni\n)\n2\nR\nd\nforeach\ni\n(\nd\n\u02dd\nn\n).Butunknowntous,twot\nattributes|some\nx\ni\nand\nx\nj\n|respectivelygiveacar'smaximumspeedmea-\nsuredinmilesperhour,andthemaximumspeedmeasuredinkilometersper\nhour.Thesetwoattributesarethereforealmostlinearlydependent,upto\nonlysmallcesintroducedbyroundingtothenearestmphorkph.\nThus,thedatareallyliesapproximatelyonan\nn\n\n1dimensionalsubspace.\nHowcanweautomaticallydetect,andperhapsremove,thisredundancy?\nForalesscontrivedexample,consideradatasetresultingfromasurveyof\npilotsforradio-controlledhelicopters,where\nx\n(\ni\n)\n1\nisameasureofthepiloting\nskillofpilot\ni\n,and\nx\n(\ni\n)\n2\ncaptureshowmuchhe/sheenjoysBecause\nRChelicoptersareveryto,onlythemostcommittedstudents,\nonesthattrulyenjoy,becomegoodpilots.So,thetwoattributes\nx\n1\nand\nx\n2\narestronglycorrelated.Indeed,wemightpositthatthatthe\ndataactuallylikesalongsomediagonalaxis(the\nu\n1\ndirection)capturingthe\nintrinsicpiloting\\karma\"ofaperson,withonlyasmallamountofnoise\nlyingthisaxis.(SeeHowcanweautomaticallycomputethis\nu\n1\ndirection?\n155\n"}, {"page_number": 156, "text": "156\nWewillshortlydevelopthePCAalgorithm.ButpriortorunningPCA\nperse,typicallywepreprocessthedatabynormalizingeachfeature\ntohavemean0andvariance1.Wedothisbysubtractingthemeanand\ndividingbytheempiricalstandarddeviation:\nx\n(\ni\n)\nj\n \nx\n(\ni\n)\nj\n\n\nj\n\u02d9\nj\nwhere\n\nj\n=\n1\nn\nP\nn\ni\n=1\nx\n(\ni\n)\nj\nand\n\u02d9\n2\nj\n=\n1\nn\nP\nn\ni\n=1\n(\nx\n(\ni\n)\nj\n\n\nj\n)\n2\narethemeanvarianceof\nfeature\nj\n,respectively.\nSubtracting\n\nj\nzerosoutthemeanandmaybeomittedfordataknown\ntohavezeromean(forinstance,timeseriescorrespondingtospeechorother\nacousticsignals).Dividingbythestandarddeviation\n\u02d9\nj\nrescaleseachcoor-\ndinatetohaveunitvariance,whichensuresthattattributesareall\ntreatedonthesame\\scale.\"Forinstance,if\nx\n1\nwascars'maximumspeedin\nmph(takingvaluesinthehightensorlowhundreds)and\nx\n2\nwerethenum-\nberofseats(takingvaluesaround2-4),thenthisrenormalizationrescales\ntherentattributestomakethemmorecomparable.Thisrescalingmay\nbeomittedifwehadaprioriknowledgethatthetattributesareall\nonthesamescale.Oneexampleofthisisifeachdatapointrepresenteda\ngrayscaleimage,andeach\nx\n(\ni\n)\nj\ntookavaluein\nf\n0\n;\n1\n;:::;\n255\ng\ncorresponding\ntotheintensityvalueofpixel\nj\ninimage\ni\n.\nNow,havingnormalizedourdata,howdowecomputethe\\majoraxis\nofvariation\"\nu\n|thatis,thedirectiononwhichthedataapproximatelylies?\nOnewayistoposethisproblemastheunitvector\nu\nsothatwhen\n"}, {"page_number": 157, "text": "157\nthedataisprojectedontothedirectioncorrespondingto\nu\n,thevarianceof\ntheprojecteddataismaximized.Intuitively,thedatastartswithsome\namountofvariance/informationinit.Wewouldliketochooseadirection\nu\nsothatifweweretoapproximatethedataaslyinginthedirection/subspace\ncorrespondingto\nu\n,asmuchaspossibleofthisvarianceisstillretained.\nConsiderthefollowingdataset,onwhichwehavealreadycarriedoutthe\nnormalizationsteps:\nNow,supposewepick\nu\ntocorrespondthethedirectionshowninthe\nbelow.Thecirclesdenotetheprojectionsoftheoriginaldataontothis\nline.\n"}, {"page_number": 158, "text": "158\nWeseethattheprojecteddatastillhasafairlylargevariance,andthe\npointstendtobefarfromzero.Incontrast,supposehadinsteadpickedthe\nfollowingdirection:\nHere,theprojectionshaveatlysmallervariance,andaremuch\nclosertotheorigin.\nWewouldliketoautomaticallyselectthedirection\nu\ncorrespondingto\ntheofthetwoshownabove.Toformalizethis,notethatgivena\n"}, {"page_number": 159, "text": "159\nunitvector\nu\nandapoint\nx\n,thelengthoftheprojectionof\nx\nonto\nu\nisgiven\nby\nx\nT\nu\n.I.e.,if\nx\n(\ni\n)\nisapointinourdataset(oneofthecrossesintheplot),\nthenitsprojectiononto\nu\n(thecorrespondingcircleinthe)isdistance\nx\nT\nu\nfromtheorigin.Hence,tomaximizethevarianceoftheprojections,we\nwouldliketochooseaunit-length\nu\nsoastomaximize:\n1\nn\nn\nX\ni\n=1\n(\nx\n(\ni\n)\nT\nu\n)\n2\n=\n1\nn\nn\nX\ni\n=1\nu\nT\nx\n(\ni\n)\nx\n(\ni\n)\nT\nu\n=\nu\nT\n \n1\nn\nn\nX\ni\n=1\nx\n(\ni\n)\nx\n(\ni\n)\nT\n!\nu:\nWeeasilyrecognizethatthemaximizingthissubjectto\nk\nu\nk\n2\n=1givesthe\nprincipaleigenvectorof=\n1\nn\nP\nn\ni\n=1\nx\n(\ni\n)\nx\n(\ni\n)\nT\n,whichisjusttheempirical\ncovariancematrixofthedata(assumingithaszeromean).\n1\nTosummarize,wehavefoundthatifwewishtoa1-dimensional\nsubspacewithwithtoapproximatethedata,weshouldchoose\nu\ntobethe\nprincipaleigenvectorofMoregenerally,ifwewishtoprojectourdata\nintoa\nk\n-dimensionalsubspace(\nk<d\n),weshouldchoose\nu\n1\n;:::;u\nk\ntobethe\ntop\nk\neigenvectorsofThe\nu\ni\n'snowformanew,orthogonalbasisforthe\ndata.\n2\nThen,torepresent\nx\n(\ni\n)\ninthisbasis,weneedonlycomputethecorre-\nspondingvector\ny\n(\ni\n)\n=\n2\n6\n6\n6\n4\nu\nT\n1\nx\n(\ni\n)\nu\nT\n2\nx\n(\ni\n)\n.\n.\n.\nu\nT\nk\nx\n(\ni\n)\n3\n7\n7\n7\n5\n2\nR\nk\n:\nThus,whereas\nx\n(\ni\n)\n2\nR\nd\n,thevector\ny\n(\ni\n)\nnowgivesalower,\nk\n-dimensional,\napproximation/representationfor\nx\n(\ni\n)\n.PCAisthereforealsoreferredtoas\na\ndimensionalityreduction\nalgorithm.Thevectors\nu\n1\n;:::;u\nk\narecalled\nthe\nk\nprincipalcomponents\nofthedata.\nRemark.\nAlthoughwehaveshownitformallyonlyforthecaseof\nk\n=1,\nusingwell-knownpropertiesofeigenvectorsitisstraightforwardtoshowthat\n1\nIfyouhaven'tseenthisbefore,tryusingthemethodofLagrangemultiplierstomax-\nimize\nu\nT\n\nu\nsubjecttothat\nu\nT\nu\n=1.Youshouldbeabletoshowthat\nu\n=\n\n,forsome\n\n,whichimplies\nu\nisaneigenvectorofwitheigenvalue\n\n.\n2\nBecauseissymmetric,the\nu\ni\n'swill(oralwayscanbechosentobe)orthogonalto\neachother.\n"}, {"page_number": 160, "text": "160\nofallpossibleorthogonalbases\nu\n1\n;:::;u\nk\n,theonethatwehavechosenmax-\nimizes\nP\ni\nk\ny\n(\ni\n)\nk\n2\n2\n.Thus,ourchoiceofabasispreservesasmuchvariability\naspossibleintheoriginaldata.\nPCAcanalsobederivedbypickingthebasisthatminimizestheap-\nproximationerrorarisingfromprojectingthedataontothe\nk\n-dimensional\nsubspacespannedbythem.(Seemoreinhomework.)\nPCAhasmanyapplications;wewillcloseourdiscussionwithafewexam-\nples.First,compression|representing\nx\n(\ni\n)\n'swithlowerdimension\ny\n(\ni\n)\n's|is\nanobviousapplication.Ifwereducehighdimensionaldatato\nk\n=2or3di-\nmensions,thenwecanalsoplotthe\ny\n(\ni\n)\n'stovisualizethedata.Forinstance,\nifweweretoreduceourautomobilesdatato2dimensions,thenwecanplot\nit(onepointinourplotwouldcorrespondtoonecartype,say)toseewhat\ncarsaresimilartoeachotherandwhatgroupsofcarsmayclustertogether.\nAnotherstandardapplicationistopreprocessadatasettoreduceits\ndimensionbeforerunningasupervisedlearninglearningalgorithmwiththe\nx\n(\ni\n)\n'sasinputs.Apartfromcomputationalbenreducingthedata's\ndimensioncanalsoreducethecomplexityofthehypothesisclassconsidered\nandhelpavoidov(e.g.,linearoverlowerdimensionalinput\nspaceswillhavesmallerVCdimension).\nLastly,asinourRCpilotexample,wecanalsoviewPCAasanoise\nreductionalgorithm.Inourexampleit,estimatestheintrinsic\\piloting\nkarma\"fromthenoisymeasuresofpilotingskillandenjoyment.Inclass,we\nalsosawtheapplicationofthisideatofaceimages,resultingin\neigenfaces\nmethod.Here,eachpoint\nx\n(\ni\n)\n2\nR\n100\n\n100\nwasa10000dimensionalvector,\nwitheachcoordinatecorrespondingtoapixelintensityvalueina100x100\nimageofaface.UsingPCA,werepresenteachimage\nx\n(\ni\n)\nwithamuchlower-\ndimensional\ny\n(\ni\n)\n.Indoingso,wehopethattheprincipalcomponentswe\nfoundretaintheinteresting,systematicvariationsbetweenfacesthatcapture\nwhatapersonreallylookslike,butnotthe\\noise\"intheimagesintroduced\nbyminorlightingvariations,slightlytimagingconditions,andsoon.\nWethenmeasuredistancesbetweenfaces\ni\nand\nj\nbyworkinginthereduced\ndimension,andcomputing\nk\ny\n(\ni\n)\n\ny\n(\nj\n)\nk\n2\n.Thisresultedinasurprisinglygood\nface-matchingandretrievalalgorithm.\n"}, {"page_number": 161, "text": "Chapter13\nIndependentcomponents\nanalysis\nOurnexttopicisIndependentComponentsAnalysis(ICA).SimilartoPCA,\nthiswillndanewbasisinwhichtorepresentourdata.However,thegoal\nisverydt.\nAsamotivatingexample,considerthe\\cocktailpartyproblem.\"Here,\nd\nspeakersarespeakingsimultaneouslyataparty,andanymicrophoneplaced\nintheroomrecordsonlyanoverlappingcombinationofthe\nd\nspeakers'voices.\nButletssaywehave\nd\ntmicrophonesplacedintheroom,andbecause\neachmicrophoneisatdistancefromeachofthespeakers,itrecordsa\ntcombinationofthespeakers'voices.Usingthesemicrophonerecord-\nings,canweseparateouttheoriginal\nd\nspeakers'speechsignals?\nToformalizethisproblem,weimaginethatthereissomedata\ns\n2\nR\nd\nthatisgeneratedvia\nd\nindependentsources.Whatweobserveis\nx\n=\nAs;\nwhere\nA\nisanunknownsquarematrixcalledthe\nmixingmatrix\n.Repeated\nobservationsgivesusadataset\nf\nx\n(\ni\n)\n;\ni\n=1\n;:::;n\ng\n,andourgoalistorecover\nthesources\ns\n(\ni\n)\nthathadgeneratedourdata(\nx\n(\ni\n)\n=\nAs\n(\ni\n)\n).\nInourcocktailpartyproblem,\ns\n(\ni\n)\nisan\nd\n-dimensionalvector,and\ns\n(\ni\n)\nj\nis\nthesoundthatspeaker\nj\nwasutteringattime\ni\n.Also,\nx\n(\ni\n)\ninan\nd\n-dimensional\nvector,and\nx\n(\ni\n)\nj\nistheacousticreadingrecordedbymicrophone\nj\nattime\ni\n.\nLet\nW\n=\nA\n\n1\nbethe\nunmixingmatrix.\nOurgoalisto\nW\n,so\nthatgivenourmicrophonerecordings\nx\n(\ni\n)\n,wecanrecoverthesourcesby\ncomputing\ns\n(\ni\n)\n=\nWx\n(\ni\n)\n.Fornotationalconvenience,wealsolet\nw\nT\ni\ndenote\n161\n"}, {"page_number": 162, "text": "162\nthe\ni\n-throwof\nW\n,sothat\nW\n=\n2\n6\n4\n|\nw\nT\n1\n|\n.\n.\n.\n|\nw\nT\nd\n|\n3\n7\n5\n:\nThus,\nw\ni\n2\nR\nd\n,andthe\nj\n-thsourcecanberecoveredas\ns\n(\ni\n)\nj\n=\nw\nT\nj\nx\n(\ni\n)\n.\n13.1ICAambiguities\nTowhatdegreecan\nW\n=\nA\n\n1\nberecovered?Ifwehavenopriorknowledge\naboutthesourcesandthemixingmatrix,itiseasytoseethattherearesome\ninherentambiguitiesin\nA\nthatareimpossibletorecover,givenonlythe\nx\n(\ni\n)\n's.\nSp,let\nP\nbeany\nd\n-by-\nd\npermutationmatrix.Thismeansthat\neachrowandeachcolumnof\nP\nhasexactlyone\\1.\"Herearesomeexamples\nofpermutationmatrices:\nP\n=\n2\n4\n010\n100\n001\n3\n5\n;\nP\n=\n\n01\n10\n\n;\nP\n=\n\n10\n01\n\n:\nIf\nz\nisavector,then\nPz\nisanothervectorthatcontainsapermutedversion\nof\nz\n'scoordinates.Givenonlythe\nx\n(\ni\n)\n's,therewillbenowaytodistinguish\nbetween\nW\nand\nPW\n.Sp,thepermutationoftheoriginalsourcesis\nambiguous,whichshouldbenosurprise.Fortunately,thisdoesnotmatter\nformostapplications.\nFurther,thereisnowaytorecoverthecorrectscalingofthe\nw\ni\n's.Forin-\nstance,if\nA\nwerereplacedwith2\nA\n,andevery\ns\n(\ni\n)\nwerereplacedwith(0\n:\n5)\ns\n(\ni\n)\n,\nthenourobserved\nx\n(\ni\n)\n=2\nA\n\n(0\n:\n5)\ns\n(\ni\n)\nwouldstillbethesame.Morebroadly,\nifasinglecolumnof\nA\nwerescaledbyafactorof\n\n,andthecorresponding\nsourcewerescaledbyafactorof1\n\n,thenthereisagainnowaytodetermine\nthatthishadhappenedgivenonlythe\nx\n(\ni\n)\n's.Thus,wecannotrecoverthe\n\\correct\"scalingofthesources.However,fortheapplicationsthatweare\nconcernedwith|includingthecocktailpartyproblem|thisambiguityalso\ndoesnotmatter.Sp,scalingaspeaker'sspeechsignal\ns\n(\ni\n)\nj\nbysome\npositivefactor\n\nonlythevolumeofthatspeaker'sspeech.Also,sign\nchangesdonotmatter,and\ns\n(\ni\n)\nj\nand\n\ns\n(\ni\n)\nj\nsoundidenticalwhenplayedona\nspeaker.Thus,ifthe\nw\ni\nfoundbyanalgorithmisscaledbyanynon-zeroreal\nnumber,thecorrespondingrecoveredsource\ns\ni\n=\nw\nT\ni\nx\nwillbescaledbythe\n"}, {"page_number": 163, "text": "163\nsamefactor;butthisusuallydoesnotmatter.(Thesecommentsalsoapply\ntoICAforthebrain/MEGdatathatwetalkedaboutinclass.)\nArethesetheonlysourcesofambiguityinICA?Itturnsoutthatthey\nare,solongasthesources\ns\ni\nare\nnon-Gaussian\n.Toseewhattheyis\nwithGaussiandata,consideranexampleinwhich\nn\n=2,and\ns\n\u02d8N\n(0\n;I\n).\nHere,\nI\nisthe2x2identitymatrix.Notethatthecontoursofthedensityof\nthestandardnormaldistribution\nN\n(0\n;I\n)arecirclescenteredontheorigin,\nandthedensityisrotationallysymmetric.\nNow,supposeweobservesome\nx\n=\nAs\n,where\nA\nisourmixingmatrix.\nThen,thedistributionof\nx\nwillbeGaussian,\nx\n\u02d8N\n(0\n;AA\nT\n),since\nE\ns\n\u02d8N\n(0\n;I\n)\n[\nx\n]=E[\nAs\n]=\nA\nE[\ns\n]=0\nCov[\nx\n]=E\ns\n\u02d8N\n(0\n;I\n)\n[\nxx\nT\n]=E[\nAss\nT\nA\nT\n]=\nA\nE[\nss\nT\n]\nA\nT\n=\nA\n\nCov[\ns\n]\n\nA\nT\n=\nAA\nT\nNow,let\nR\nbeanarbitraryorthogonal(lessformally,a\nmatrix,sothat\nRR\nT\n=\nR\nT\nR\n=\nI\n,andlet\nA\n0\n=\nAR\n.Thenifthedatahad\nbeenmixedaccordingto\nA\n0\ninsteadof\nA\n,wewouldhaveinsteadobserved\nx\n0\n=\nA\n0\ns\n.Thedistributionof\nx\n0\nisalsoGaussian,\nx\n0\n\u02d8N\n(0\n;AA\nT\n),since\nE\ns\n\u02d8N\n(0\n;I\n)\n[\nx\n0\n(\nx\n0\n)\nT\n]=E[\nA\n0\nss\nT\n(\nA\n0\n)\nT\n]=E[\nARss\nT\n(\nAR\n)\nT\n]=\nARR\nT\nA\nT\n=\nAA\nT\n.\nHence,whetherthemixingmatrixis\nA\nor\nA\n0\n,wewouldobservedatafrom\na\nN\n(0\n;AA\nT\n)distribution.Thus,thereisnowaytotellifthesourceswere\nmixedusing\nA\nand\nA\n0\n.Thereisanarbitraryrotationalcomponentinthe\nmixingmatrixthatcannotbedeterminedfromthedata,andwecannot\nrecovertheoriginalsources.\nOurargumentabovewasbasedonthefactthatthemultivariatestandard\nnormaldistributionisrotationallysymmetric.Despitethebleakpicturethat\nthispaintsforICAonGaussiandata,itturnsoutthat,solongasthedatais\nnot\nGaussian,itispossible,givenenoughdata,torecoverthe\nd\nindependent\nsources.\n13.2Densitiesandlineartransformations\nBeforemovingontoderivetheICAalgorithmproper,wedigress\ntotalkabouttheoflineartransformationsondensities.\nSupposearandomvariable\ns\nisdrawnaccordingtosomedensity\np\ns\n(\ns\n).\nForsimplicity,assumefornowthat\ns\n2\nR\nisarealnumber.Now,letthe\nrandomvariable\nx\nbeaccordingto\nx\n=\nAs\n(here,\nx\n2\nR\n;A\n2\nR\n).Let\np\nx\nbethedensityof\nx\n.Whatis\np\nx\n?\nLet\nW\n=\nA\n\n1\n.Tocalculatethe\\probability\"ofaparticularvalueof\nx\n,\nitistemptingtocompute\ns\n=\nWx\n,thenthenevaluate\np\ns\natthatpoint,and\n"}, {"page_number": 164, "text": "164\nconcludethat\\\np\nx\n(\nx\n)=\np\ns\n(\nWx\n).\"However,\nthisisincorrect\n.Forexample,\nlet\ns\n\u02d8\nUniform[0\n;\n1],so\np\ns\n(\ns\n)=1\nf\n0\n\ns\n\n1\ng\n.Now,let\nA\n=2,so\nx\n=2\ns\n.\nClearly,\nx\nisdistributeduniformlyintheinterval[0\n;\n2].Thus,itsdensityis\ngivenby\np\nx\n(\nx\n)=(0\n:\n5)1\nf\n0\n\nx\n\n2\ng\n.Thisdoesnotequal\np\ns\n(\nWx\n),where\nW\n=0\n:\n5=\nA\n\n1\n.Instead,thecorrectformulais\np\nx\n(\nx\n)=\np\ns\n(\nWx\n)\nj\nW\nj\n.\nMoregenerally,if\ns\nisavector-valueddistributionwithdensity\np\ns\n,and\nx\n=\nAs\nforasquare,invertiblematrix\nA\n,thenthedensityof\nx\nisgivenby\np\nx\n(\nx\n)=\np\ns\n(\nWx\n)\nj\nW\nj\n;\nwhere\nW\n=\nA\n\n1\n.\nRemark.\nIfyou'reseentheresultthat\nA\nmaps[0\n;\n1]\nd\ntoasetofvolume\nj\nA\nj\n,\nthenhere'sanotherwaytoremembertheformulafor\np\nx\ngivenabove,thatalso\ngeneralizesourprevious1-dimensionalexample.Sp,let\nA\n2\nR\nd\n\nd\nbe\ngiven,andlet\nW\n=\nA\n\n1\nasusual.Alsolet\nC\n1\n=[0\n;\n1]\nd\nbethe\nd\n-dimensional\nhypercube,and\nC\n2\n=\nf\nAs\n:\ns\n2\nC\n1\ng\nR\nd\ntobetheimageof\nC\n1\nunderthemappinggivenby\nA\n.Thenitisastandardresultinlinearalgebra\n(and,indeed,oneofthewaysofdeterminants)thatthevolumeof\nC\n2\nisgivenby\nj\nA\nj\n.Now,suppose\ns\nisuniformlydistributedin[0\n;\n1]\nd\n,soits\ndensityis\np\ns\n(\ns\n)=1\nf\ns\n2\nC\n1\ng\n.Thenclearly\nx\nwillbeuniformlydistributed\nin\nC\n2\n.Itsdensityisthereforefoundtobe\np\nx\n(\nx\n)=1\nf\nx\n2\nC\n2\ng\n=\nvol(\nC\n2\n)(since\nitmustintegrateover\nC\n2\nto1).Butusingthefactthatthedeterminant\noftheinverseofamatrixisjusttheinverseofthedeterminant,wehave\n1\n=\nvol(\nC\n2\n)=1\n=\nj\nA\nj\n=\nj\nA\n\n1\nj\n=\nj\nW\nj\n.Thus,\np\nx\n(\nx\n)=1\nf\nx\n2\nC\n2\ngj\nW\nj\n=1\nf\nWx\n2\nC\n1\ngj\nW\nj\n=\np\ns\n(\nWx\n)\nj\nW\nj\n.\n13.3ICAalgorithm\nWearenowreadytoderiveanICAalgorithm.Wedescribeanalgorithm\nbyBellandSejnowski,andwegiveaninterpretationoftheiralgorithmasa\nmethodformaximumlikelihoodestimation.(Thisistfromtheirorig-\ninalinterpretationinvolvingacomplicatedideacalledtheinfomaxprincipal\nwhichisnolongernecessarygiventhemodernunderstandingofICA.)\nWesupposethatthedistributionofeachsource\ns\nj\nisgivenbyadensity\np\ns\n,andthatthejointdistributionofthesources\ns\nisgivenby\np\n(\ns\n)=\nd\nY\nj\n=1\np\ns\n(\ns\nj\n)\n:\n"}, {"page_number": 165, "text": "165\nNotethatbymodelingthejointdistributionasaproductofmarginals,we\ncapturetheassumptionthatthesourcesareindependent.Usingourformulas\nfromtheprevioussection,thisimpliesthefollowingdensityon\nx\n=\nAs\n=\nW\n\n1\ns\n:\np\n(\nx\n)=\nd\nY\nj\n=1\np\ns\n(\nw\nT\nj\nx\n)\nj\nW\nj\n:\nAllthatremainsistospecifyadensityfortheindividualsources\np\ns\n.\nRecallthat,givenareal-valuedrandomvariable\nz\n,itscumulativedistri-\nbutionfunction(cdf)\nF\nisby\nF\n(\nz\n0\n)=\nP\n(\nz\n\nz\n0\n)=\nR\nz\n0\n\np\nz\n(\nz\n)\ndz\nand\nthedensityisthederivativeofthecdf:\np\nz\n(\nz\n)=\nF\n0\n(\nz\n).\nThus,tospecifyadensityforthe\ns\ni\n's,allweneedtodoistospecifysome\ncdfforit.Acdfhastobeamonotonicfunctionthatincreasesfromzero\ntoone.Followingourpreviousdiscussion,wecannotchoosetheGaussian\ncdf,asICAdoesn'tworkonGaussiandata.Whatwe'llchooseinsteadas\nareasonable\\default\"cdfthatslowlyincreasesfrom0to1,isthesigmoid\nfunction\ng\n(\ns\n)=1\n=\n(1+\ne\n\ns\n).Hence,\np\ns\n(\ns\n)=\ng\n0\n(\ns\n).\n1\nThesquarematrix\nW\nistheparameterinourmodel.Givenatraining\nset\nf\nx\n(\ni\n)\n;\ni\n=1\n;:::;n\ng\n,theloglikelihoodisgivenby\n`\n(\nW\n)=\nn\nX\ni\n=1\n \nd\nX\nj\n=1\nlog\ng\n0\n(\nw\nT\nj\nx\n(\ni\n)\n)+log\nj\nW\nj\n!\n:\nWewouldliketomaximizethisinterms\nW\n.Bytakingderivativesandusing\nthefact(fromthesetofnotes)that\nr\nW\nj\nW\nj\n=\nj\nW\nj\n(\nW\n\n1\n)\nT\n,weeasily\nderiveastochasticgradientascentlearningrule.Foratrainingexample\nx\n(\ni\n)\n,\ntheupdateruleis:\nW\n:=\nW\n+\n\n0\nB\nB\nB\n@\n2\n6\n6\n6\n4\n1\n\n2\ng\n(\nw\nT\n1\nx\n(\ni\n)\n)\n1\n\n2\ng\n(\nw\nT\n2\nx\n(\ni\n)\n)\n.\n.\n.\n1\n\n2\ng\n(\nw\nT\nd\nx\n(\ni\n)\n)\n3\n7\n7\n7\n5\nx\n(\ni\n)\nT\n+(\nW\nT\n)\n\n1\n1\nC\nC\nC\nA\n;\n1\nIfyouhavepriorknowledgethatthesources'densitiestakeacertainform,thenit\nisagoodideatosubstitutethatinhere.Butintheabsenceofsuchknowledge,the\nsigmoidfunctioncanbethoughtofasareasonabledefaultthatseemstoworkwellfor\nmanyproblems.Also,thepresentationhereassumesthateitherthedata\nx\n(\ni\n)\nhasbeen\npreprocessedtohavezeromean,orthatitcannaturallybeexpectedtohavezeromean\n(suchasacousticsignals).Thisisnecessarybecauseourassumptionthat\np\ns\n(\ns\n)=\ng\n0\n(\ns\n)\nimpliesE[\ns\n]=0(thederivativeofthelogisticfunctionisasymmetricfunction,and\nhencegivesadensitycorrespondingtoarandomvariablewithzeromean),whichimplies\nE[\nx\n]=E[\nAs\n]=0.\n"}, {"page_number": 166, "text": "166\nwhere\n\nisthelearningrate.\nAfterthealgorithmconverges,wethencompute\ns\n(\ni\n)\n=\nWx\n(\ni\n)\ntorecover\ntheoriginalsources.\nRemark.\nWhenwritingdownthelikelihoodofthedata,weimplicitlyas-\nsumedthatthe\nx\n(\ni\n)\n'swereindependentofeachother(fortvalues\nof\ni\n;notethisissueistfromwhetherthetcoordinatesof\nx\n(\ni\n)\nareindependent),sothatthelikelihoodofthetrainingsetwasgiven\nby\nQ\ni\np\n(\nx\n(\ni\n)\n;\nW\n).Thisassumptionisclearlyincorrectforspeechdataand\nothertimeserieswherethe\nx\n(\ni\n)\n'saredependent,butitcanbeshownthat\nhavingcorrelatedtrainingexampleswillnothurttheperformanceoftheal-\ngorithmifwehavecientdata.However,forproblemswheresuccessive\ntrainingexamplesarecorrelated,whenimplementingstochasticgradientas-\ncent,itsometimeshelpsaccelerateconvergenceifwevisittrainingexamples\ninarandomlypermutedorder.(I.e.,runstochasticgradientascentona\nrandomlyshcopyofthetrainingset.)\n"}, {"page_number": 167, "text": "Chapter14\nSelf-supervisedlearningand\nfoundationmodels\nDespiteitshugesuccess,supervisedlearningwithneuralnetworkstypically\nreliesontheavailabilityofalabeleddatasetofdecentsize,whichissome-\ntimescostlytocollect.Recently,AIandmachinelearningareundergoinga\nparadigmshiftwiththeriseofmodels(e.g.,BERT[Devlinetal.,2019]and\nGPT-3[Brownetal.,2020])thatarepre-trainedonbroaddataatscaleand\nareadaptabletoawiderangeofdownstreamtasks.Thesemodels,called\nfoundationmodelsbyBommasanietal.[2021],oftentimesleveragemassive\nunlabeleddatasothatmuchfewerlabeleddatainthedownstreamtasksare\nneeded.Moreover,thoughfoundationmodelsarebasedonstandarddeep\nlearningandtransferlearning,theirscaleresultsinnewemergentcapabil-\nities.Thesemodelsaretypically(pre-)trainedbyself-supervisedlearning\nmethodswherethesupervisions/labelscomefrompartsoftheinputs.\nThischapterwillintroducetheparadigmoffoundationmodelsandbasic\nrelatedconcepts.\n14.1Pretrainingandadaptation\nThefoundationmodelsparadigmconsistsoftwophases:pretraining(orsim-\nplytraining)andadaptation.Wepretrainalargemodelonamassive\nunlabeleddataset(e.g.,billionsofunlabeledimages).\n1\nThen,weadaptthe\npretrainedmodeltoadownstreamtask(e.g.,detectingcancerfromscanim-\nages).Thesedownstreamtasksareoftenpredictiontaskswithlimitedor\n1\nSometimes,pretrainingcaninvolvelarge-scalelabeleddatasetsaswell(e.g.,theIma-\ngeNetdataset).\n167\n"}, {"page_number": 168, "text": "168\nevennolabeleddata.Theintuitionisthatthepretrainedmodelslearngood\nrepresentations\nthatcaptureintrinsicsemanticstructure/informationabout\nthedata,andtheadaptationphasecustomizesthemodeltoaparticular\ndownstreamtaskby,e.g.,retrievingtheinformationsptoit.Forex-\nample,amodelpretrainedonmassiveunlabeledimagedatamaylearngood\ngeneralvisualrepresentations/features,andweadapttherepresentationsto\nsolvebiomedicalimaginingtasks.\nWeformalizethetwophasesbelow.\nPretraining.\nSupposewehaveanunlabeledpretrainingdataset\nf\nx\n(1)\n;x\n(2)\n\n;x\n(\nn\n)\ng\nthatconsistsof\nn\nexamplesin\nR\nd\n.Let\n\u02da\n\nbeamodelthat\nisparameterizedby\n\nandmapstheinput\nx\ntosome\nm\n-dimensionalrepresen-\ntation\n\u02da\n\n(\nx\n).(Peoplealsocall\n\u02da\n\n(\nx\n)\n2\nR\nm\ntheembeddingorfeaturesofthe\nexample\nx\n.)Wepretrainthemodel\n\nwithapretrainingloss,whichisoften\nanaverageoflossfunctionsonalltheexamples:\nL\npre\n(\n\n)=\n1\nn\nP\nn\ni\n=1\n`\npre\n(\n;x\n(\ni\n)\n).\nHere\n`\npre\nisaso-calledself-supervisedlossonasingledatapoint\nx\n(\ni\n)\n,because\nasshownlater,e.g.,inSection14.3,the\\supervision\"comesfromthedata\npoint\nx\n(\ni\n)\nitself.Itisalsopossiblethatthepretraininglossisnotasum\noflossesonindividualexamples.Wewilldiscusstwopretraininglossesin\nSection14.2andSection14.3.\nWeusesomeoptimizers(mostlylikelySGDorADAM[KingmaandBa,\n2014])tominimize\nL\npre\n(\n\n).Wedenotetheobtainedpretrainedmodelby\n^\n\n.\nAdaptation.\nForadownstreamtask,weusuallyhavealabeleddataset\nf\n(\nx\n(1)\ntask\n;y\n(1)\ntask\n)\n;\n\n;\n(\nx\n(\nn\ntask\n)\ntask\n;y\n(\nn\ntask\n)\ntask\n)\ng\nwith\nn\ntask\nexamples.Thesettingwhen\nn\ntask\n=0iscalledzero-shotlearning|thedownstreamtaskdoesn'thaveany\nlabeledexamples.When\nn\ntask\nisrelativelysmall(say,between1and50),the\nsettingiscalledfew-shotlearning.It'salsoprettycommontohavealarger\nn\ntask\nontheorderofrangingfromhundredstotensofthousands.\nAnadaptationalgorithmgenerallytakesinadownstreamdatasetandthe\npretrainedmodel\n^\n\n,andoutputsavariantof\n^\n\nthatsolvesthedownstream\ntask.Wewilldiscussbelowtwopopularandgeneraladaptationmethods,\nlinearprobeandInaddition,twoothermethodssptolan-\nguageproblemsareintroducedin14.3.1.\nThelinearprobeapproachusesalinearheadontopoftherepresentation\ntopredictthedownstreamlabels.Mathematically,theadaptedmodelout-\nputs\nw\n>\n\u02da\n^\n\n(\nx\n),where\nw\n2\nR\nm\nisaparametertobelearned,and\n^\n\nisexactly\nthepretrainedmodelWecanuseSGD(orotheroptimizers)totrain\n"}, {"page_number": 169, "text": "169\nw\nonthedownstreamtasklosstopredictthetasklabel\nmin\nw\n2\nR\nm\n1\nn\ntask\nn\ntask\nX\ni\n=1\n`\ntask\n(\ny\n(\ni\n)\ntask\n;w\n>\n\u02da\n^\n\n(\nx\n(\ni\n)\ntask\n))(14.1)\nE.g.,ifthedownstreamtaskisaregressionproblem,wewillhave\n`\ntask\n(\ny\ntask\n;w\n>\n\u02da\n^\n\n(\nx\ntask\n))=(\ny\ntask\n\nw\n>\n\u02da\n^\n\n(\nx\ntask\n))\n2\n.\nThealgorithmusesasimilarstructureforthedownstream\npredictionmodel,butalsofurtherthepretrainedmodel(instead\nofkeepingitConcretely,thepredictionmodelis\nw\n>\n\u02da\n\n(\nx\n)withpa-\nrameters\nw\nand\n:\nWeoptimizeboth\nw\nand\n\ntothedownstreamdata,\nbutinitialize\n\nwiththepretrainedmodel\n^\n\n.Thelinearhead\nw\nisusually\ninitializedrandomly.\nminimize\nw\n1\nn\ntask\nn\ntask\nX\ni\n=1\n`\ntask\n(\ny\n(\ni\n)\ntask\n;w\n>\n\u02da\n\n(\nx\n(\ni\n)\ntask\n))(14.2)\nwithinitialization\nw\n \nrandomvector(14.3)\n\n \n^\n\n(14.4)\nVariousotheradaptationmethodsexistsandaresometimesspecialized\ntotheparticularpretrainingmethods.WewilldiscussoneoftheminSec-\ntion14.3.1.\n14.2Pretrainingmethodsincomputervision\nThissectionintroducestwoconcretepretrainingmethodsforcomputervi-\nsion:supervisedpretrainingandcontrastivelearning.\nSupervisedpretraining.\nHere,thepretrainingdatasetisalarge-scale\nlabeled\ndataset(e.g.,ImageNet),andthepretrainedmodelsaresimplya\nneuralnetworktrainedwithvanillasupervisedlearning(withthelastlayer\nbeingremoved).Concretely,supposewewritethelearnedneuralnetworkas\nU\u02da\n^\n\n(\nx\n),where\nU\nisthelast(fully-connected)layerparameters,\n^\n\ncorresponds\ntotheparametersofalltheotherlayers,and\n\u02da\n^\n\n(\nx\n)arethepenultimate\nactivationslayer(whichservesastherepresentation).Wesimplydiscard\nU\nanduse\n\u02da\n^\n\n(\nx\n)asthepretrainedmodel.\nContrastivelearning.\nContrastivelearningisaself-supervisedpretraining\nmethodthatusesonlyunlabeleddata.Themainintuitionisthatagood\nrepresentationfunction\n\u02da\n\n(\n\n)shouldmapsemanticallysimilarimagestosim-\nilarrepresentations,andthatrandompairofimagesshouldgenerallyhave\n"}, {"page_number": 170, "text": "170\ndistinctrepresentations.E.g.,wemaywanttomapimagesoftwohuskiesto\nsimilarrepresentations,butahuskyandanelephantshouldhavedt\nrepresentations.Oneofsimilarityisthatimagesfromthesame\nclassaresimilar.Usingthiswillresultintheso-calledsupervised\ncontrastivealgorithmsthatworkwellwhenlabeledpretrainingdatasetsare\navailable.\nWithoutlabeleddata,wecanusedataaugmentationtogenerateapair\nof\\similar\"augmentedimagesgivenanoriginalimage\nx\n.Dataaugmenta-\ntiontypicallymeansthatweapply\nrandom\ncropping,and/orcolor\ntransformationontheoriginalimage\nx\ntogenerateavariant.Wecantake\ntworandomaugmentations,denotedby^\nx\nand~\nx\n,ofthesameoriginalimage\nx\n,andcallthemapositivepair.Weobservethatpositivepairsofimages\nareoftensemanticallyrelatedbecausetheyareaugmentationsofthesame\nimage.Wewilldesignalossfunctionfor\n\nsuchthattherepresentationsof\napositivepair,\n\u02da\n\n(^\nx\n)\n;\u02da\n\n(~\nx\n),asclosetoeachotheraspossible.\nOntheotherhand,wecanalsotakeanotherrandomimage\nz\nfromthe\npretrainingdatasetandgenerateanaugmentation^\nz\nfrom\nz\n.Notethat(^\nx;\n^\nz\n)\narefromtimages;therefore,withagoodchance,theyarenotseman-\nticallyrelated.Wecall(^\nx;\n^\nz\n)anegativeorrandompair.\n2\nWewilldesigna\nlosstopushtherepresentationofrandompairs,\n\u02da\n\n(^\nx\n)\n;\u02da\n\n(^\nz\n),farawayfrom\neachother.\nTherearemanyrecentalgorithmsbasedonthecontrastivelearningprin-\nciple,andhereweintroduceSIMCLR[Chenetal.,2020]asanconcrete\nexample.Thelossfunctionisdeonabatchofexamples(\nx\n1\n;\n\n;x\n(\nB\n)\n)\nwithbatchsize\nB\n.Thealgorithmcomputestworandomaugmentationsfor\neachexample\nx\n(\ni\n)\ninthebatch,denotedby^\nx\n(\ni\n)\nand~\nx\n(\ni\n)\n:\nAsaresult,we\nhavetheaugmentedbatchof2\nB\nexamples:^\nx\n1\n;\n\n;\n^\nx\n(\nB\n)\n,~\nx\n1\n;\n\n;\n~\nx\n(\nB\n)\n.The\nSIMCLRlossisas\n3\nL\npre\n(\n\n)=\n\nB\nX\ni\n=1\nlog\nexp\n\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\ni\n)\n)\n\nexp(\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\ni\n)\n))+\nP\nj\n6\n=\ni\nexp(\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\nj\n)\n))\n:\nTheintuitionisasfollows.Thelossisincreasingin\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\nj\n)\n),and\nthusminimizingthelossencourages\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\nj\n)\n)tobesmall,making\n\u02da\n\n(^\nx\n(\ni\n)\n)farawayfrom\n\u02da\n\n(~\nx\n(\nj\n)\n).Ontheotherhand,thelossisdecreasingin\n2\nRandompairmaybeamoreaccuratetermbecauseit'sstillpossible(thoughnot\nlikely)that\nx\nand\nz\naresemanticallyrelated,soare^\nx\nand^\nz\n.Butintheliterature,the\ntermnegativepairseemstobealsocommon.\n3\nThisisavariantandoftheoriginallossthatdoesnotchangetheessence\n(butmaychangetheslightly).\n"}, {"page_number": 171, "text": "171\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\ni\n)\n),andthusminimizingthelossencourages\n\u02da\n\n(^\nx\n(\ni\n)\n)\n>\n\u02da\n\n(~\nx\n(\ni\n)\n)\ntobelarge,resultingin\n\u02da\n\n(^\nx\n(\ni\n)\n)and\n\u02da\n\n(~\nx\n(\ni\n)\n)tobeclose.\n4\n14.3Pretrainedlargelanguagemodels\nNaturallanguageprocessingisanotherareawherepretrainingmodelsare\nparticularlysuccessful.Inlanguageproblems,anexamplestypicallycor-\nrespondstoadocumentorgenerallyasequence/trunkofwords,\n5\ndenoted\nby\nx\n=(\nx\n1\n;\n\n;x\nT\n)where\nT\nisthelengthofthedocument/sequence,\nx\ni\n2f\n1\n;\n\n;V\ng\narewordsinthedocument,and\nV\nisthevocabularysize.\n6\nAlanguagemodelisaprobabilisticmodelrepresentingtheprobabilityof\nadocument,denotedby\np\n(\nx\n1\n;\n\n;x\nT\n)\n:\nThisprobabilitydistributionisvery\ncomplexbecauseitssupportsizeis\nV\nT\n|exponentialinthelengthofthe\ndocument.Insteadofmodelingthedistributionofadocumentitself,wecan\napplythechainruleofconditionalprobabilitytodecomposeitasfollows:\np\n(\nx\n1\n;\n\n;x\nT\n)=\np\n(\nx\n1\n)\np\n(\nx\n2\nj\nx\n1\n)\n\np\n(\nx\nT\nj\nx\n1\n;\n\n;x\nT\n\n1\n)\n:\n(14.5)\nNowthesupportofeachoftheconditionalprobability\np\n(\nx\nt\nj\nx\n1\n;\n\n;x\nt\n\n1\n)is\nV\n.\nWewillmodeltheconditionalprobability\np\n(\nx\nt\nj\nx\n1\n;\n\n;x\nt\n\n1\n)withsome\nparameterizedform.Tothisend,weturnthediscretewordsintoword\nembeddings.\nLet\ne\ni\n2\nR\nd\nbetheembeddingoftheword\ni\n2f\n1\n;\n2\n;\n\n;V\ng\n:\nWecall\n[\ne\n1\n;\n\n;e\nV\n]\n2\nR\nd\n\nV\ntheembeddingmatrix.Themostcommonlyusedmodel\nistransformer[Vaswanietal.,2017].Wewillintroducethebasicconcepts\nregardingtheinputsandoutputsofatransformer,buttreattheinterme-\ndiatecomputationintransformerasablackbox.Wereferthestudentsto\nmoreadvancedcoursesortheoriginalpaperformoredetails.Thehigh-level\npipelineisvisualizedinFigure14.1.Givenadocument(\nx\n1\n;\n\n;x\nT\n),we\ncomputethecorrespondingwordembeddings(\ne\nx\n1\n;\n\n;e\nx\nT\n).Then,theword\nembeddingsispassedtoatransformermodel,whichtakesinasequenceof\n4\nToseethis,youcanverifythatthefunction\n\nlog\np\np\n+\nq\nisdecreasingin\np\n,andincreasing\nin\nq\nwhen\np;q>\n0\n:\n5\nInthepracticalimplementations,typicallyallthedataareconcatenatedintoasingle\nsequenceinsomeorder,andeachexampletypicallycorrespondsasub-sequenceofconsec-\nutivewordswhichmaycorrespondstoasubsetofadocumentormayspanacrossmultiple\ndocuments.\n6\nTechnically,wordsmaybedecomposedintotokenswhichcouldbewordsorsub-words\n(combinationsofletters),butthisnoteomitsthistechnicality.Infactmostcommonswords\nareasingletokenthemselves.\n"}, {"page_number": 172, "text": "172\nvectorsandoutputsasequenceofvectors(\nc\n2\n;\n\n;c\nT\n+1\n)\n:\nInparticular,here\nweusetheautoregressiveversionofthetransformerswhichmakessurethat\nc\nt\nonlydependson\nx\n1\n;\n\n;x\nt\n\n1\n.\n7\nThe\nc\nt\n'sareoftencalledtherepresentations\northecontextualizedembeddings,andwewrite\nc\nt\n=\n\u02da\n\n(\nx\n1\n;:::;x\nt\n\n1\n)where\n\u02da\n\ndenotesthemappingfromtheinputtotherepresentations.\nFigure14.1:Theinput-outputinterfaceofatransformer.\nTolearntheparameters\n\ninthetransformer,weuse\nc\nt\ntopredict\ntheconditionalprobability\np\n(\nx\nt\nj\nx\n1\n;\n\n;x\nt\n\n1\n).\n8\nWeparameterize\np\n(\nx\nt\nj\nx\n1\n;\n\n;x\nt\n\n1\n)by\n2\n6\n6\n6\n4\np\n(\nx\nt\n=1\nj\nx\n1\n\n;x\nt\n\n1\n)\np\n(\nx\nt\n=2\nj\nx\n1\n\n;x\nt\n\n1\n)\n.\n.\n.\np\n(\nx\nt\n=\nV\nj\nx\n1\n\n;x\nt\n\n1\n)\n3\n7\n7\n7\n5\n=softmax(\nW\nt\nc\nt\n)\n2\nR\nV\n(14.6)\n=softmax(\nW\nt\n\u02da\n\n(\nx\n1\n;\n\n;x\nt\n\n1\n))\n;\n(14.7)\nwhere\nW\n2\nR\nV\n\nd\nisaweightmatrixthatmapsthecontextualizedembedding\nc\nt\ntothelogits.Inotherwords,\nW\nt\nisanadditionallinearlayerforthe\npredictionoftheconditionalprobability.Recallthatsoftmax(\n\n):\nR\nV\n7!\nR\nV\nmapsthelogitstotheprobabilities:\nsoftmax(\nu\n)=\n2\n6\n6\n4\nexp(\nu\n1\n)\nP\nV\ni\n=1\nexp(\nu\ni\n)\n.\n.\n.\nexp(\nu\nV\n)\nP\nV\ni\n=1\nexp(\nu\ni\n)\n3\n7\n7\n5\n(14.8)\n7\nThispropertynolongerholdsinmaskedlanguagemodels[Devlinetal.,2019]where\nthelossesarealsot.\n8\nHere\nt\n\n2andweomitthelossforpredicting\np\n(\nx\n1\n)forsimplicity(whichalsodoesn't\ntheperformancemuch).Toformallymodel\np\n(\nx\n1\n),anoptionistoprependaspecial\ntoken\nx\n0\n=\n?\ntothesequence,andthenaskthelanguagemodeltopredict\np\n(\nx\n1\nj\nx\n0\n=\n?\n).\n"}, {"page_number": 173, "text": "173\nWetrainalltheparameters\n\ninthetransformersaswellasthe\nparameters\nW\n=(\nW\n1\n;:::;W\nT\n)bythecrossentropyloss.Let\np\nt\n=\nsoftmax(\nW\nt\n\u02da\n\n(\nx\n1\n;\n\n;x\nt\n\n1\n))\n2\nR\nV\nbethepredictedconditionalprobability\natposition\nt\n.Let\nW\nbetheconcatenationof\nW\n1\n;\n\n;W\nT\n.Thelossfunction\nisoftencalledlanguagemodelinglossandas\nL\n(\nW;\n)=\nT\nX\nt\n=2\n(crossentropylossatposition\nt\n)\n=\nT\nX\nt\n=2\n\nlog\np\nt;x\nt\n;\n(14.9)\nwhere\np\nt;j\ndenotesthe\nj\n-thentryoftheprobabilityvector\np\nt\n.\n14.3.1Zero-shotlearningandin-contextlearning\nForlanguagemodels,therearemanywaystoadaptapretrainedmodelto\ndownstreamtasks.Inthisnotes,wediscussthreeofthem:zero-\nshotlearning,andin-contextlearning.\nFinetuning\nisnotverycommonfortheautoregressivelanguagemodelsthat\nweintroducedinSection14.3butmuchmorecommonforothervariants\nsuchasmaskedlanguagemodelswhichhassimilarinput-outputinterfaces\nbutarepretrainedrently[Devlinetal.,2019].Themethodis\nthesameasintroducedgenerallyinSection14.1|theonlyquestionishow\nwethepredictiontaskwithanadditionallinearhead.Oneoption\nistotreat\nc\nT\n+1\n=\n\u02da\n\n(\nx\n1\n;\n\n;x\nT\n)astherepresentationanduse\nw\n>\nc\nT\n+1\n=\nw\n>\n\u02da\n\n(\nx\n1\n;\n\n;x\nT\n)topredicttasklabel.AsdescribedinSection14.1,we\ninitialize\n\ntothepretrainedmodel\n^\n\nandthenoptimizeboth\nw\nand\n\n.\nZero-shot\nadaptationorzero-shotlearningisthesettingwherethereisno\ninput-outputpairsfromthedownstreamtasks.Forlanguageproblemstasks,\ntypicallythetaskisformattedasaquestionoraclozetestformvianatural\nlanguage.Forexample,wecanformatanexampleasaquestion:\nx\ntask\n=(\nx\ntask\n;\n1\n;\n\n;x\ntask\n;T\n)=\\Isthespeedoflightauniversalconstant?\"\nThen,wecomputethemostlikelynextwordpredictedbythelan-\nguagemodelgiventhisquestion,thatis,computingargmax\nx\nT\n+1\np\n(\nx\nT\n+1\nj\nx\ntask\n;\n1\n;\n\n;x\ntask\n;T\n).Inthiscase,ifthemostlikelynextword\nx\nT\n+1\nis\\No\",\nthenwesolvethetask.(Thespeedoflightisonlyaconstantinvacuum).\nWenotethattherearemanywaystodecodetheanswerfromthelanguage\n"}, {"page_number": 174, "text": "174\nmodels,e.g.,insteadofcomputingtheargmax,wemayusethelanguage\nmodeltogenerateafewwordsword.Itisanactiveresearchquestionto\nthebestwaytoutilizethelanguagemodels.\nIn-contextlearning\nismostlyusedforfew-shotsettingswherewehavea\nfewlabeledexamples(\nx\n(1)\ntask\n;y\n(1)\ntask\n)\n;\n\n;\n(\nx\n(\nn\ntask\n)\ntask\n;y\n(\nn\ntask\n)\ntask\n).Givenatestexample\nx\ntest\n,weconstructadocument(\nx\n1\n;\n\n;x\nT\n),whichismorecommonlycalled\na\\prompt\"inthiscontext,byconcatenatingthelabeledexamplesandthe\ntextexampleinsomeformat.Forexample,wemayconstructthepromptas\nfollows\nx\n1\n;\n\n;x\nT\n=\\Q:2\n\u02d8\n3=?\nx\n(1)\ntask\nA:5\ny\n(1)\ntask\nQ:6\n\u02d8\n7=?\nx\n(2)\ntask\nA:13\ny\n(2)\ntask\n\nQ:15\n\u02d8\n2=?\"\nx\ntest\nThen,weletthepretrainedmodelgeneratethemostlikely\nx\nT\n+1\n;x\nT\n+2\n;\n\n:\nInthiscase,ifthemodelcan\\learn\"thatthesymbol\n\u02d8\nmeansadditionfrom\nthefewexamples,wewillobtainthefollowingwhichsuggeststheansweris\n17.\nx\nT\n+1\n;x\nT\n+2\n;\n\n=\n\\A:17\"\n:\nTheareaoffoundationmodelsisverynewandquicklygrowing.Thenotes\nhereonlyattempttointroducethesemodelsonaconceptuallevelwitha\ntamountofsimWereferthereaderstoothermaterials,\ne.g.,Bommasanietal.[2021],formoredetails.\n"}, {"page_number": 175, "text": "PartV\nReinforcementLearningand\nControl\n175\n"}, {"page_number": 176, "text": "Chapter15\nReinforcementlearning\nWenowbeginourstudyofreinforcementlearningandadaptivecontrol.\nInsupervisedlearning,wesawalgorithmsthattriedtomaketheiroutputs\nmimicthelabels\ny\ngiveninthetrainingset.Inthatsetting,thelabelsgave\nanunambiguous\\rightanswer\"foreachoftheinputs\nx\n.Incontrast,for\nmanysequentialdecisionmakingandcontrolproblems,itisveryto\nprovidethistypeofexplicitsupervisiontoalearningalgorithm.Forexample,\nifwehavejustbuiltafour-leggedrobotandaretryingtoprogramittowalk,\ntheninitiallywehavenoideawhatthe\\correct\"actionstotakearetomake\nitwalk,andsodonotknowhowtoprovideexplicitsupervisionforalearning\nalgorithmtotrytomimic.\nInthereinforcementlearningframework,wewillinsteadprovideoural-\ngorithmsonlyarewardfunction,whichindicatestothelearningagentwhen\nitisdoingwell,andwhenitisdoingpoorly.Inthefour-leggedwalkingex-\nample,therewardfunctionmightgivetherobotpositiverewardsformoving\nforwards,andnegativerewardsforeithermovingbackwardsorfallingover.\nItwillthenbethelearningalgorithm'sjobtoouthowtochooseactions\novertimesoastoobtainlargerewards.\nReinforcementlearninghasbeensuccessfulinapplicationsasdiverseas\nautonomoushelicoptert,robotleggedlocomotion,cell-phonenetwork\nrouting,marketingstrategyselection,factorycontrol,andtweb-page\nindexing.Ourstudyofreinforcementlearningwillbeginwithaof\nthe\nMarkovdecisionprocesses(MDP)\n,whichprovidestheformalismin\nwhichRLproblemsareusuallyposed.\n176\n"}, {"page_number": 177, "text": "177\n15.1Markovdecisionprocesses\nAMarkovdecisionprocessisatuple(\nS;A;\nf\nP\nsa\ng\n;;R\n),where:\n\u2039\nS\nisasetof\nstates\n.(Forexample,inautonomoushelicoptert,\nS\nmightbethesetofallpossiblepositionsandorientationsoftheheli-\ncopter.)\n\u2039\nA\nisasetof\nactions\n.(Forexample,thesetofallpossibledirectionsin\nwhichyoucanpushthehelicopter'scontrolsticks.)\n\u2039\nP\nsa\narethestatetransitionprobabilities.Foreachstate\ns\n2\nS\nand\naction\na\n2\nA\n,\nP\nsa\nisadistributionoverthestatespace.We'llsaymore\naboutthislater,but,\nP\nsa\ngivesthedistributionoverwhatstates\nwewilltransitiontoifwetakeaction\na\ninstate\ns\n.\n\u2039\n\n2\n[0\n;\n1)iscalledthe\ndiscountfactor\n.\n\u2039\nR\n:\nS\n\nA\n7!\nR\nisthe\nrewardfunction\n.(Rewardsaresometimesalso\nwrittenasafunctionofastate\nS\nonly,inwhichcasewewouldhave\nR\n:\nS\n7!\nR\n).\nThedynamicsofanMDPproceedsasfollows:Westartinsomestate\ns\n0\n,\nandgettochoosesomeaction\na\n0\n2\nA\ntotakeintheMDP.Asaresultofour\nchoice,thestateoftheMDPrandomlytransitionstosomesuccessorstate\ns\n1\n,drawnaccordingto\ns\n1\n\u02d8\nP\ns\n0\na\n0\n.Then,wegettopickanotheraction\na\n1\n.\nAsaresultofthisaction,thestatetransitionsagain,nowtosome\ns\n2\n\u02d8\nP\ns\n1\na\n1\n.\nWethenpick\na\n2\n,andsoon....Pictorially,wecanrepresentthisprocessas\nfollows:\ns\n0\na\n0\n!\ns\n1\na\n1\n!\ns\n2\na\n2\n!\ns\n3\na\n3\n!\n:::\nUponvisitingthesequenceofstates\ns\n0\n;s\n1\n;:::\nwithactions\na\n0\n;a\n1\n;:::\n,our\ntotalpayisgivenby\nR\n(\ns\n0\n;a\n0\n)+\nR\n(\ns\n1\n;a\n1\n)+\n\n2\nR\n(\ns\n2\n;a\n2\n)+\n\n:\nOr,whenwearewritingrewardsasafunctionofthestatesonly,thisbecomes\nR\n(\ns\n0\n)+\nR\n(\ns\n1\n)+\n\n2\nR\n(\ns\n2\n)+\n\n:\nFormostofourdevelopment,wewillusethesimplerstate-rewards\nR\n(\ns\n),\nthoughthegeneralizationtostate-actionrewards\nR\n(\ns;a\n)nospecial\n\n"}, {"page_number": 178, "text": "178\nOurgoalinreinforcementlearningistochooseactionsovertimesoasto\nmaximizetheexpectedvalueofthetotalpay\nE\n\nR\n(\ns\n0\n)+\nR\n(\ns\n1\n)+\n\n2\nR\n(\ns\n2\n)+\n\n\nNotethattherewardattimestep\nt\nis\ndiscounted\nbyafactorof\n\nt\n.Thus,to\nmakethisexpectationlarge,wewouldliketoaccruepositiverewardsassoon\naspossible(andpostponenegativerewardsaslongaspossible).Ineconomic\napplicationswhere\nR\n(\n\n)istheamountofmoneymade,\n\nalsohasanatural\ninterpretationintermsoftheinterestrate(whereadollartodayisworth\nmorethanadollartomorrow).\nA\npolicy\nisanyfunction\n\u02c7\n:\nS\n7!\nA\nmappingfromthestatestothe\nactions.Wesaythatweare\nexecuting\nsomepolicy\n\u02c7\nif,wheneverweare\ninstate\ns\n,wetakeaction\na\n=\n\u02c7\n(\ns\n).Wealsothe\nvaluefunction\nfor\napolicy\n\u02c7\naccordingto\nV\n\u02c7\n(\ns\n)=E\n\nR\n(\ns\n0\n)+\nR\n(\ns\n1\n)+\n\n2\nR\n(\ns\n2\n)+\n\n\n\ns\n0\n=\ns;\u02c7\n]\n:\nV\n\u02c7\n(\ns\n)issimplytheexpectedsumofdiscountedrewardsuponstartingin\nstate\ns\n,andtakingactionsaccordingto\n\u02c7\n.\n1\nGivenapolicy\n\u02c7\n,itsvaluefunction\nV\n\u02c7\nthe\nBellmanequa-\ntions\n:\nV\n\u02c7\n(\ns\n)=\nR\n(\ns\n)+\n\nX\ns\n0\n2\nS\nP\ns\u02c7\n(\ns\n)\n(\ns\n0\n)\nV\n\u02c7\n(\ns\n0\n)\n:\nThissaysthattheexpectedsumofdiscountedrewards\nV\n\u02c7\n(\ns\n)forstarting\nin\ns\nconsistsoftwoterms:First,the\nimmediatereward\nR\n(\ns\n)thatweget\nrightawaysimplyforstartinginstate\ns\n,andsecond,theexpectedsumof\nfuturediscountedrewards.Examiningthesecondterminmoredetail,we\nseethatthesummationtermabovecanberewrittenE\ns\n0\n\u02d8\nP\ns\u02c7\n(\ns\n)\n[\nV\n\u02c7\n(\ns\n0\n)].This\nistheexpectedsumofdiscountedrewardsforstartinginstate\ns\n0\n,where\ns\n0\nisdistributedaccording\nP\ns\u02c7\n(\ns\n)\n,whichisthedistributionoverwherewewill\nendupaftertakingtheaction\n\u02c7\n(\ns\n)intheMDPfromstate\ns\n.Thus,the\nsecondtermabovegivestheexpectedsumofdiscountedrewardsobtained\nafter\nthestepintheMDP.\nBellman'sequationscanbeusedtotlysolvefor\nV\n\u02c7\n.Sp,\ninaMDP(\nj\nS\nj\n<\n1\n),wecanwritedownonesuchequationfor\nV\n\u02c7\n(\ns\n)foreverystate\ns\n.Thisgivesusasetof\nj\nS\nj\nlinearequationsin\nj\nS\nj\nvariables(theunknown\nV\n\u02c7\n(\ns\n)'s,oneforeachstate),whichcanbetly\nsolvedforthe\nV\n\u02c7\n(\ns\n)'s.\n1\nThisnotationinwhichweconditionon\n\u02c7\nisn'ttechnicallycorrectbecause\n\u02c7\nisn'ta\nrandomvariable,butthisisquitestandardintheliterature.\n"}, {"page_number": 179, "text": "179\nWealsothe\noptimalvaluefunction\naccordingto\nV\n\n(\ns\n)=max\n\u02c7\nV\n\u02c7\n(\ns\n)\n:\n(15.1)\nInotherwords,thisisthebestpossibleexpectedsumofdiscountedrewards\nthatcanbeattainedusinganypolicy.ThereisalsoaversionofBellman's\nequationsfortheoptimalvaluefunction:\nV\n\n(\ns\n)=\nR\n(\ns\n)+max\na\n2\nA\n\nX\ns\n0\n2\nS\nP\nsa\n(\ns\n0\n)\nV\n\n(\ns\n0\n)\n:\n(15.2)\nThesttermaboveistheimmediaterewardasbefore.Thesecondterm\nisthemaximumoverallactions\na\noftheexpectedfuturesumofdiscounted\nrewardswe'llgetuponafteraction\na\n.Youshouldmakesureyouunderstand\nthisequationandseewhyitmakessense.\nWealsoapolicy\n\u02c7\n\n:\nS\n7!\nA\nasfollows:\n\u02c7\n\n(\ns\n)=argmax\na\n2\nA\nX\ns\n0\n2\nS\nP\nsa\n(\ns\n0\n)\nV\n\n(\ns\n0\n)\n:\n(15.3)\nNotethat\n\u02c7\n\n(\ns\n)givestheaction\na\nthatattainsthemaximuminthe\\max\"\ninEquation(15.2).\nItisafactthatforeverystate\ns\nandeverypolicy\n\u02c7\n,wehave\nV\n\n(\ns\n)=\nV\n\u02c7\n\n(\ns\n)\n\nV\n\u02c7\n(\ns\n)\n:\nTheequalitysaysthatthe\nV\n\u02c7\n\n,thevaluefunctionfor\n\u02c7\n\n,isequaltothe\noptimalvaluefunction\nV\n\nforeverystate\ns\n.Further,theinequalityabove\nsaysthat\n\u02c7\n\n'svalueisatleastalargeasthevalueofanyotherotherpolicy.\nInotherwords,\n\u02c7\n\nasinEquation(15.3)istheoptimalpolicy.\nNotethat\n\u02c7\n\nhastheinterestingpropertythatitistheoptimalpolicy\nfor\nall\nstates\ns\n.Sp,itisnotthecasethatifwewerestartingin\nsomestate\ns\nthenthere'dbesomeoptimalpolicyforthatstate,andifwe\nwerestartinginsomeotherstate\ns\n0\nthenthere'dbesomeotherpolicythat's\noptimalpolicyfor\ns\n0\n.Thesamepolicy\n\u02c7\n\nattainsthemaximuminEqua-\ntion(15.1)for\nall\nstates\ns\n.Thismeansthatwecanusethesamepolicy\n\u02c7\n\nnomatterwhattheinitialstateofourMDPis.\n15.2Valueiterationandpolicyiteration\nWenowdescribetwotalgorithmsforsolvingMDPs.For\nnow,wewillconsideronlyMDPswithstateandactionspaces(\nj\nS\nj\n<\n"}, {"page_number": 180, "text": "180\n1\n;\nj\nA\nj\n<\n1\n).Inthissection,wewillalsoassumethatweknowthestate\ntransitionprobabilities\nf\nP\nsa\ng\nandtherewardfunction\nR\n.\nThealgorithm,\nvalueiteration\n,isasfollows:\nAlgorithm6\nValueIteration\n1:\nForeachstate\ns\n,initialize\nV\n(\ns\n):=0.\n2:\nfor\nuntilconvergence\ndo\n3:\nForeverystate,update\nV\n(\ns\n):=\nR\n(\ns\n)+max\na\n2\nA\n\nX\ns\n0\nP\nsa\n(\ns\n0\n)\nV\n(\ns\n0\n)\n:\n(15.4)\nThisalgorithmcanbethoughtofasrepeatedlytryingtoupdatethe\nestimatedvaluefunctionusingBellmanEquations(15.2).\nTherearetwopossiblewaysofperformingtheupdatesintheinnerloopof\nthealgorithm.Inthewecancomputethenewvaluesfor\nV\n(\ns\n)for\neverystate\ns\n,andthenoverwritealltheoldvalueswiththenewvalues.This\niscalleda\nsynchronous\nupdate.Inthiscase,thealgorithmcanbeviewedas\nimplementinga\\Bellmanbackupoperator\"thattakesacurrentestimateof\nthevaluefunction,andmapsittoanewestimate.(Seehomeworkproblem\nfordetails.)Alternatively,wecanalsoperform\nasynchronous\nupdates.\nHere,wewouldloopoverthestates(insomeorder),updatingthevaluesone\natatime.\nUndereithersynchronousorasynchronousupdates,itcanbeshownthat\nvalueiterationwillcause\nV\ntoconvergeto\nV\n\n.Havingfound\nV\n\n,wecan\nthenuseEquation(15.3)tondtheoptimalpolicy.\nApartfromvalueiteration,thereisasecondstandardalgorithmfor\ninganoptimalpolicyforanMDP.The\npolicyiteration\nalgorithmproceeds\nasfollows:\nThus,theinner-looprepeatedlycomputesthevaluefunctionforthecur-\nrentpolicy,andthenupdatesthepolicyusingthecurrentvaluefunction.\n(Thepolicy\n\u02c7\nfoundinstep(b)isalsocalledthepolicythatis\ngreedywith\nrespectto\nV\n.)Notethatstep(a)canbedoneviasolvingBellman'sequa-\ntionsasdescribedearlier,whichinthecaseofapolicy,isjustasetof\nj\nS\nj\nlinearequationsin\nj\nS\nj\nvariables.\nAfteratmosta\n\nnumberofiterationsofthisalgorithm,\nV\nwillcon-\nvergeto\nV\n\n,and\n\u02c7\nwillconvergeto\n\u02c7\n\n.\n2\n2\nNotethatvalueiterationcannotreachtheexact\nV\n\ninanumberofiterations,\n"}, {"page_number": 181, "text": "181\nAlgorithm7\nPolicyIteration\n1:\nInitialize\n\u02c7\nrandomly.\n2:\nfor\nuntilconvergence\ndo\n3:\nLet\nV\n:=\nV\n\u02c7\n.\n.\ntypicallybylinearsystemsolver\n4:\nForeachstate\ns\n,let\n\u02c7\n(\ns\n):=argmax\na\n2\nA\nX\ns\n0\nP\nsa\n(\ns\n0\n)\nV\n(\ns\n0\n)\n:\nBothvalueiterationandpolicyiterationarestandardalgorithmsforsolv-\ningMDPs,andthereisn'tcurrentlyuniversalagreementoverwhichalgo-\nrithmisbetter.ForsmallMDPs,policyiterationisoftenveryfatsand\nconvergeswithveryfewiterations.However,forMDPswithlargestate\nspaces,solvingfor\nV\n\u02c7\nexplicitlywouldinvolvesolvingalargesystemoflin-\nearequations,andcouldbe(andnotethatonehastosolvethe\nlinearsystemmultipletimesinpolicyiteration).Intheseproblems,value\niterationmaybepreferred.Forthisreason,inpracticevalueiterationseems\ntobeusedmoreoftenthanpolicyiteration.Forsomemorediscussionson\nthecomparisonandconnectionofvalueiterationandpolicyiteration,please\nseeSection15.5.\n15.3LearningamodelforanMDP\nSofar,wehavediscussedMDPsandalgorithmsforMDPsassumingthatthe\nstatetransitionprobabilitiesandrewardsareknown.Inmanyrealisticprob-\nlems,wearenotgivenstatetransitionprobabilitiesandrewardsexplicitly,\nbutmustinsteadestimatethemfromdata.(Usually,\nS;A\nand\n\nareknown.)\nForexample,supposethat,fortheinvertedpendulumproblem(seeprob-\nwhereaspolicyiterationwithanexactlinearsystemsolver,can.Thisisbecausewhen\ntheactionsspaceandpolicyspacearediscreteandandoncethepolicyreachesthe\noptimalpolicyinpolicyiteration,thenitwillnotchangeatall.Ontheotherhand,even\nthoughvalueiterationwillconvergetothe\nV\n\n,butthereisalwayssomenon-zeroerrorin\nthelearnedvaluefunction.\n"}, {"page_number": 182, "text": "182\nlemset4),wehadanumberoftrialsintheMDP,thatproceededasfollows:\ns\n(1)\n0\na\n(1)\n0\n!\ns\n(1)\n1\na\n(1)\n1\n!\ns\n(1)\n2\na\n(1)\n2\n!\ns\n(1)\n3\na\n(1)\n3\n!\n:::\ns\n(2)\n0\na\n(2)\n0\n!\ns\n(2)\n1\na\n(2)\n1\n!\ns\n(2)\n2\na\n(2)\n2\n!\ns\n(2)\n3\na\n(2)\n3\n!\n:::\n:::\nHere,\ns\n(\nj\n)\ni\nisthestatewewereattime\ni\noftrial\nj\n,and\na\n(\nj\n)\ni\nisthecor-\nrespondingactionthatwastakenfromthatstate.Inpractice,eachofthe\ntrialsabovemightberununtiltheMDPterminates(suchasifthepolefalls\noverintheinvertedpendulumproblem),oritmightberunforsomelarge\nbutnumberoftimesteps.\nGiventhis\\experience\"intheMDPconsistingofanumberoftrials,\nwecantheneasilyderivethemaximumlikelihoodestimatesforthestate\ntransitionprobabilities:\nP\nsa\n(\ns\n0\n)=\n#timestookweaction\na\ninstate\ns\nandgotto\ns\n0\n#timeswetookactionainstate\ns\n(15.5)\nOr,iftheratioaboveis\\0/0\"|correspondingtothecaseofneverhaving\ntakenaction\na\ninstate\ns\nbefore|thewemightsimplyestimate\nP\nsa\n(\ns\n0\n)tobe\n1\n=\nj\nS\nj\n.(I.e.,estimate\nP\nsa\ntobetheuniformdistributionoverallstates.)\nNotethat,ifwegainmoreexperience(observemoretrials)intheMDP,\nthereisantwaytoupdateourestimatedstatetransitionprobabilities\nusingthenewexperience.Sp,ifwekeeparoundthecountsforboth\nthenumeratoranddenominatortermsof(15.5),thenasweobservemore\ntrials,wecansimplykeepaccumulatingthosecounts.Computingtheratio\nofthesecountsthengivenourestimateof\nP\nsa\n.\nUsingasimilarprocedure,if\nR\nisunknown,wecanalsopickourestimate\noftheexpectedimmediatereward\nR\n(\ns\n)instate\ns\ntobetheaveragereward\nobservedinstate\ns\n.\nHavinglearnedamodelfortheMDP,wecanthenuseeithervalueit-\nerationorpolicyiterationtosolvetheMDPusingtheestimatedtransition\nprobabilitiesandrewards.Forexample,puttingtogethermodellearningand\nvalueiteration,hereisonepossiblealgorithmforlearninginanMDPwith\nunknownstatetransitionprobabilities:\n1.\nInitialize\n\u02c7\nrandomly.\n2.\nRepeat\nf\n(a)\nExecute\n\u02c7\nintheMDPforsomenumberoftrials.\n"}, {"page_number": 183, "text": "183\n(b)\nUsingtheaccumulatedexperienceintheMDP,updateouresti-\nmatesfor\nP\nsa\n(and\nR\n,ifapplicable).\n(c)\nApplyvalueiterationwiththeestimatedstatetransitionprobabil-\nitiesandrewardstogetanewestimatedvaluefunction\nV\n.\n(d)\nUpdate\n\u02c7\ntobethegreedypolicywithrespectto\nV\n.\ng\nWenotethat,forthisparticularalgorithm,thereisonesimpleoptimiza-\ntionthatcanmakeitrunmuchmorequickly.Sply,intheinnerloop\nofthealgorithmwhereweapplyvalueiteration,ifinsteadofinitializingvalue\niterationwith\nV\n=0,weinitializeitwiththesolutionfoundduringthepre-\nviousiterationofouralgorithm,thenthatwillprovidevalueiterationwith\namuchbetterinitialstartingpointandmakeitconvergemorequickly.\n15.4ContinuousstateMDPs\nSofar,we'vefocusedourattentiononMDPswithanumberofstates.\nWenowdiscussalgorithmsforMDPsthatmayhaveannumberof\nstates.Forexample,foracar,wemightrepresentthestateas(\nx;y;;\n_\nx;\n_\ny;\n_\n\n),\ncomprisingitsposition(\nx;y\n);orientation\n\n;velocityinthe\nx\nand\ny\ndirections\n_\nx\nand_\ny\n;andangularvelocity\n_\n\n.Hence,\nS\n=\nR\n6\nisansetofstates,\nbecausethereisannumberofpossiblepositionsandorientations\nforthecar.\n3\nSimilarly,theinvertedpendulumyousawinPS4hasstates\n(\nx;;\n_\nx;\n_\n\n),where\n\nistheangleofthepole.And,ahelicopterin3d\nspacehasstatesoftheform(\nx;y;z;\u02da;; ;\n_\nx;\n_\ny;\n_\nz;\n_\n\u02da;\n_\n;\n_\n \n),whereheretheroll\n\u02da\n,pitch\n\n,andyaw\n \nanglesspecifythe3dorientationofthehelicopter.\nInthissection,wewillconsidersettingswherethestatespaceis\nS\n=\nR\nd\n,\nanddescribewaysforsolvingsuchMDPs.\n15.4.1Discretization\nPerhapsthesimplestwaytosolveacontinuous-stateMDPistodiscretize\nthestatespace,andthentouseanalgorithmlikevalueiterationorpolicy\niteration,asdescribedpreviously.\nForexample,ifwehave2dstates(\ns\n1\n;s\n2\n),wecanuseagridtodiscretize\nthestatespace:\n3\nTechnically,\n\nisanorientationandsotherangeof\n\nisbetterwritten\n\n2\n[\n\n\u02c7;\u02c7\n)than\n\n2\nR\n;butforourpurposes,thisdistinctionisnotimportant.\n"}, {"page_number": 184, "text": "184\n[t]\nHere,eachgridcellrepresentsaseparatediscretestate\ns\n.Wecan\nthenapproximatethecontinuous-stateMDPviaadiscrete-stateone\n(\n\nS;A;\nf\nP\n\nsa\ng\n;;R\n),where\n\nS\nisthesetofdiscretestates,\nf\nP\n\nsa\ng\nareourstate\ntransitionprobabilitiesoverthediscretestates,andsoon.Wecanthenuse\nvalueiterationorpolicyiterationtosolveforthe\nV\n\n(\ns\n)and\n\u02c7\n\n(\ns\n)inthe\ndiscretestateMDP(\n\nS;A;\nf\nP\n\nsa\ng\n;;R\n).Whenouractualsystemisinsome\ncontinuous-valuedstate\ns\n2\nS\nandweneedtopickanactiontoexecute,we\ncomputethecorrespondingdiscretizedstate\ns\n,andexecuteaction\n\u02c7\n\n(\ns\n).\nThisdiscretizationapproachcanworkwellformanyproblems.However,\ntherearetwodownsides.First,itusesafairlynaiverepresentationfor\nV\n\n(and\n\u02c7\n\n).Sp,itassumesthatthevaluefunctionistakesaconstant\nvalueovereachofthediscretizationintervals(i.e.,thatthevaluefunctionis\npiecewiseconstantineachofthegridcells).\nTobetterunderstandthelimitationsofsucharepresentation,considera\nsupervisedlearning\nproblemofgafunctiontothisdataset:\n[t]\n"}, {"page_number": 185, "text": "185\nClearly,linearregressionwoulddoonthisproblem.However,ifwe\ninsteaddiscretizethe\nx\n-axis,andthenusearepresentationthatispiecewise\nconstantineachofthediscretizationintervals,thenourtothedatawould\nlooklikethis:\n[t]\nThispiecewiseconstantrepresentationjustisn'tagoodrepresentationfor\nmanysmoothfunctions.Itresultsinlittlesmoothingovertheinputs,andno\ngeneralizationoverthetgridcells.Usingthissortofrepresentation,\nwewouldalsoneedaverydiscretization(verysmallgridcells)togeta\ngoodapproximation.\nAseconddownsideofthisrepresentationiscalledthe\ncurseofdimen-\nsionality\n.Suppose\nS\n=\nR\nd\n,andwediscretizeeachofthe\nd\ndimensionsofthe\nstateinto\nk\nvalues.Thenthetotalnumberofdiscretestateswehaveis\nk\nd\n.\nThisgrowsexponentiallyquicklyinthedimensionofthestatespace\nd\n,and\nthusdoesnotscalewelltolargeproblems.Forexample,witha10dstate,if\nwediscretizeeachstatevariableinto100values,wewouldhave100\n10\n=10\n20\ndiscretestates,whichisfartoomanytorepresentevenonamoderndesktop\ncomputer.\nAsaruleofthumb,discretizationusuallyworksextremelywellfor1d\nand2dproblems(andhastheadvantageofbeingsimpleandquicktoim-\nplement).Perhapswithalittlebitofclevernessandsomecareinchoosing\nthediscretizationmethod,itoftenworkswellforproblemswithupto4d\nstates.Ifyou'reextremelyclever,andsomewhatlucky,youmayevengetit\ntoworkforsome6dproblems.Butitveryrarelyworksforproblemsany\nhigherdimensionalthanthat.\n"}, {"page_number": 186, "text": "186\n15.4.2Valuefunctionapproximation\nWenowdescribeanalternativemethodforpoliciesincontinuous-\nstateMDPs,inwhichweapproximate\nV\n\ndirectly,withoutresortingtodis-\ncretization.Thisapproach,calledvaluefunctionapproximation,hasbeen\nsuccessfullyappliedtomanyRLproblems.\nUsingamodelorsimulator\nTodevelopavaluefunctionapproximationalgorithm,wewillassumethat\nwehavea\nmodel\n,or\nsimulator\n,fortheMDP.Informally,asimulatoris\nablack-boxthattakesasinputany(continuous-valued)state\ns\nt\nandaction\na\nt\n,andoutputsanext-state\ns\nt\n+1\nsampledaccordingtothestatetransition\nprobabilities\nP\ns\nt\na\nt\n:\n[t]\nThereareseveralwaysthatonecangetsuchamodel.Oneistouse\nphysicssimulation.Forexample,thesimulatorfortheinvertedpendulum\ninPS4wasobtainedbyusingthelawsofphysicstocalculatewhatposition\nandorientationthecart/polewillbeinattime\nt\n+1,giventhecurrentstate\nattime\nt\nandtheaction\na\ntaken,assumingthatweknowalltheparameters\nofthesystemsuchasthelengthofthepole,themassofthepole,andso\non.Alternatively,onecanalsouseanphysicssimulationsoftware\npackagewhichtakesasinputacompletephysicaldescriptionofamechanical\nsystem,thecurrentstate\ns\nt\nandaction\na\nt\n,andcomputesthestate\ns\nt\n+1\nofthe\nsystemasmallfractionofasecondintothefuture.\n4\nAnalternativewaytogetamodelistolearnonefromdatacollectedin\ntheMDP.Forexample,supposeweexecute\nn\ntrials\ninwhichwerepeatedly\ntakeactionsinanMDP,eachtrialfor\nT\ntimesteps.Thiscanbedonepicking\nactionsatrandom,executingsomesppolicy,orviasomeotherwayof\n4\nOpenDynamicsEngine(http://www.ode.com)isoneexampleofafree/open-source\nphysicssimulatorthatcanbeusedtosimulatesystemsliketheinvertedpendulum,and\nthathasbeenareasonablypopularchoiceamongRLresearchers.\n"}, {"page_number": 187, "text": "187\nchoosingactions.Wewouldthenobserve\nn\nstatesequenceslikethefollowing:\ns\n(1)\n0\na\n(1)\n0\n!\ns\n(1)\n1\na\n(1)\n1\n!\ns\n(1)\n2\na\n(1)\n2\n!\na\n(1)\nT\n\n1\n!\ns\n(1)\nT\ns\n(2)\n0\na\n(2)\n0\n!\ns\n(2)\n1\na\n(2)\n1\n!\ns\n(2)\n2\na\n(2)\n2\n!\na\n(2)\nT\n\n1\n!\ns\n(2)\nT\n\ns\n(\nn\n)\n0\na\n(\nn\n)\n0\n!\ns\n(\nn\n)\n1\na\n(\nn\n)\n1\n!\ns\n(\nn\n)\n2\na\n(\nn\n)\n2\n!\na\n(\nn\n)\nT\n\n1\n!\ns\n(\nn\n)\nT\nWecanthenapplyalearningalgorithmtopredict\ns\nt\n+1\nasafunctionof\ns\nt\nand\na\nt\n.\nForexample,onemaychoosetolearnalinearmodeloftheform\ns\nt\n+1\n=\nAs\nt\n+\nBa\nt\n;\n(15.6)\nusinganalgorithmsimilartolinearregression.Here,theparametersofthe\nmodelarethematrices\nA\nand\nB\n,andwecanestimatethemusingthedata\ncollectedfromour\nn\ntrials,bypicking\nargmin\nA;B\nn\nX\ni\n=1\nT\n\n1\nX\nt\n=0\n\n\n\ns\n(\ni\n)\nt\n+1\n\n\nAs\n(\ni\n)\nt\n+\nBa\n(\ni\n)\nt\n\n\n\n\n2\n2\n:\nWecouldalsopotentiallyuseotherlossfunctionsforlearningthemodel.\nForexample,ithasbeenfoundinrecentworkLuoetal.[2018]thatusing\nkk\n2\nnorm(withoutthesquare)maybehelpfulincertaincases.\nHavinglearned\nA\nand\nB\n,oneoptionistobuilda\ndeterministic\nmodel,\ninwhichgivenaninput\ns\nt\nand\na\nt\n,theoutput\ns\nt\n+1\nisexactlydetermined.\nSp,wealwayscompute\ns\nt\n+1\naccordingtoEquation(15.6).Alter-\nnatively,wemayalsobuilda\nstochastic\nmodel,inwhich\ns\nt\n+1\nisarandom\nfunctionoftheinputs,bymodelingitas\ns\nt\n+1\n=\nAs\nt\n+\nBa\nt\n+\n\nt\n;\nwherehere\n\nt\nisanoiseterm,usuallymodeledas\n\nt\n\u02d8N\n(0\n;\n(Thecovari-\nancematrixcanalsobeestimatedfromdatainastraightforwardway.)\nHere,we'vewrittenthenext-state\ns\nt\n+1\nasalinearfunctionofthecurrent\nstateandaction;butofcourse,non-linearfunctionsarealsopossible.Specif-\nically,onecanlearnamodel\ns\nt\n+1\n=\nA\u02da\ns\n(\ns\nt\n)+\nB\u02da\na\n(\na\nt\n),where\n\u02da\ns\nand\n\u02da\na\nare\nsomenon-linearfeaturemappingsofthestatesandactions.Alternatively,\nonecanalsousenon-linearlearningalgorithms,suchaslocallyweightedlin-\nearregression,tolearntoestimate\ns\nt\n+1\nasafunctionof\ns\nt\nand\na\nt\n.These\napproachescanalsobeusedtobuildeitherdeterministicorstochasticsim-\nulatorsofanMDP.\n"}, {"page_number": 188, "text": "188\nFittedvalueiteration\nWenowdescribethe\nvalueiteration\nalgorithmforapproximating\nthevaluefunctionofacontinuousstateMDP.Inthesequel,wewillassume\nthattheproblemhasacontinuousstatespace\nS\n=\nR\nd\n,butthattheaction\nspace\nA\nissmallanddiscrete.\n5\nRecallthatinvalueiteration,wewouldliketoperformtheupdate\nV\n(\ns\n):=\nR\n(\ns\n)+\n\nmax\na\nZ\ns\n0\nP\nsa\n(\ns\n0\n)\nV\n(\ns\n0\n)\nds\n0\n(15.7)\n=\nR\n(\ns\n)+\n\nmax\na\nE\ns\n0\n\u02d8\nP\nsa\n[\nV\n(\ns\n0\n)]\n(15.8)\n(InSection15.2,wehadwrittenthevalueiterationupdatewithasummation\nV\n(\ns\n):=\nR\n(\ns\n)+\n\nmax\na\nP\ns\n0\nP\nsa\n(\ns\n0\n)\nV\n(\ns\n0\n)ratherthananintegraloverstates;\nthenewnotationtsthatwearenowworkingincontinuousstatesrather\nthandiscretestates.)\nThemainideaofvalueiterationisthatwearegoingtoapproxi-\nmatelycarryoutthisstep,overasampleofstates\ns\n(1)\n;:::;s\n(\nn\n)\n.Specif-\nically,wewilluseasupervisedlearningalgorithm|linearregressioninour\ndescriptionbelow|toapproximatethevaluefunctionasalinearornon-linear\nfunctionofthestates:\nV\n(\ns\n)=\n\nT\n\u02da\n(\ns\n)\n:\nHere,\n\u02da\nissomeappropriatefeaturemappingofthestates.\nForeachstate\ns\ninoursampleof\nn\nstates,valueiteration\nwillcomputeaquantity\ny\n(\ni\n)\n,whichwillbeourapproximationto\nR\n(\ns\n)+\n\nmax\na\nE\ns\n0\n\u02d8\nP\nsa\n[\nV\n(\ns\n0\n)](therighthandsideofEquation15.8).Then,itwill\napplyasupervisedlearningalgorithmtotrytoget\nV\n(\ns\n)closeto\nR\n(\ns\n)+\n\nmax\na\nE\ns\n0\n\u02d8\nP\nsa\n[\nV\n(\ns\n0\n)](or,inotherwords,totrytoget\nV\n(\ns\n)closeto\ny\n(\ni\n)\n).\nIndetail,thealgorithmisasfollows:\n1.\nRandomlysample\nn\nstates\ns\n(1)\n;s\n(2)\n;:::s\n(\nn\n)\n2\nS\n.\n2.\nInitialize\n\n:=0.\n3.\nRepeat\nf\nFor\ni\n=1\n;:::;n\nf\n5\nInpractice,mostMDPshavemuchsmalleractionspacesthanstatespaces.E.g.,acar\nhasa6dstatespace,anda2dactionspace(steeringandvelocitycontrols);theinverted\npendulumhasa4dstatespace,anda1dactionspace;ahelicopterhasa12dstatespace,\nanda4dactionspace.So,discretizingthissetofactionsisusuallylessofaproblemthan\ndiscretizingthestatespacewouldhavebeen.\n"}, {"page_number": 189, "text": "189\nForeachaction\na\n2\nA\nf\nSample\ns\n0\n1\n;:::;s\n0\nk\n\u02d8\nP\ns\n(\ni\n)\na\n(usingamodeloftheMDP).\nSet\nq\n(\na\n)=\n1\nk\nP\nk\nj\n=1\nR\n(\ns\n(\ni\n)\n)+\nV\n(\ns\n0\nj\n)\n==\nHence,\nq\n(\na\n)isanestimateof\nR\n(\ns\n(\ni\n)\n)+\n\nE\ns\n0\n\u02d8\nP\ns\n(\ni\n)\na\n[\nV\n(\ns\n0\n)].\ng\nSet\ny\n(\ni\n)\n=max\na\nq\n(\na\n).\n==\nHence,\ny\n(\ni\n)\nisanestimateof\nR\n(\ns\n(\ni\n)\n)+\n\nmax\na\nE\ns\n0\n\u02d8\nP\ns\n(\ni\n)\na\n[\nV\n(\ns\n0\n)].\ng\n==\nIntheoriginalvalueiterationalgorithm(overdiscretestates)\n==\nweupdatedthevaluefunctionaccordingto\nV\n(\ns\n(\ni\n)\n):=\ny\n(\ni\n)\n.\n==\nInthisalgorithm,wewant\nV\n(\ns\n(\ni\n)\n)\n\u02c7\ny\n(\ni\n)\n,whichwe'llachieve\n==\nusingsupervisedlearning(linearregression).\nSet\n\n:=argmin\n\n1\n2\nP\nn\ni\n=1\n\n\nT\n\u02da\n(\ns\n(\ni\n)\n)\n\ny\n(\ni\n)\n\n2\ng\nAbove,wehadwrittenoutvalueiterationusinglinearregression\nasthealgorithmtotrytomake\nV\n(\ns\n(\ni\n)\n)closeto\ny\n(\ni\n)\n.Thatstepofthealgo-\nrithmiscompletelyanalogoustoastandardsupervisedlearning(regression)\nprobleminwhichwehaveatrainingset(\nx\n(1)\n;y\n(1)\n)\n;\n(\nx\n(2)\n;y\n(2)\n)\n;:::;\n(\nx\n(\nn\n)\n;y\n(\nn\n)\n),\nandwanttolearnafunctionmappingfrom\nx\nto\ny\n;theonlyisthat\nhere\ns\nplaystheroleof\nx\n.Eventhoughourdescriptionaboveusedlinearre-\ngression,clearlyotherregressionalgorithms(suchaslocallyweightedlinear\nregression)canalsobeused.\nUnlikevalueiterationoveradiscretesetofstates,valueiteration\ncannotbeprovedtoalwaystoconverge.However,inpractice,itoftendoes\nconverge(orapproximatelyconverge),andworkswellformanyproblems.\nNotealsothatifweareusingadeterministicsimulator/modeloftheMDP,\nthenvalueiterationcanbebysetting\nk\n=1inthealgorithm.\nThisisbecausetheexpectationinEquation(15.8)becomesanexpectation\noveradeterministicdistribution,andsoasingleexampleistto\nexactlycomputethatexpectation.Otherwise,inthealgorithmabove,we\nhadtodraw\nk\nsamples,andaveragetotrytoapproximatethatexpectation\n(seetheof\nq\n(\na\n),inthealgorithmpseudo-code).\n"}, {"page_number": 190, "text": "190\nFinally,valueiterationoutputs\nV\n,whichisanapproximationto\nV\n\n.Thisimplicitlyourpolicy.Sp,whenoursystemisin\nsomestate\ns\n,andweneedtochooseanaction,wewouldliketochoosethe\naction\nargmax\na\nE\ns\n0\n\u02d8\nP\nsa\n[\nV\n(\ns\n0\n)](15.9)\nTheprocessforcomputing/approximatingthisissimilartotheinner-loopof\nvalueiteration,whereforeachaction,wesample\ns\n0\n1\n;:::;s\n0\nk\n\u02d8\nP\nsa\nto\napproximatetheexpectation.(Andagain,ifthesimulatorisdeterministic,\nwecanset\nk\n=1.)\nInpractice,thereareoftenotherwaystoapproximatethisstepaswell.\nForexample,oneverycommoncaseisifthesimulatorisoftheform\ns\nt\n+1\n=\nf\n(\ns\nt\n;a\nt\n)+\n\nt\n,where\nf\nissomedeterministicfunctionofthestates(suchas\nf\n(\ns\nt\n;a\nt\n)=\nAs\nt\n+\nBa\nt\n),and\n\niszero-meanGaussiannoise.Inthiscase,we\ncanpicktheactiongivenby\nargmax\na\nV\n(\nf\n(\ns;a\n))\n:\nInotherwords,herewearejustsetting\n\nt\n=0(i.e.,ignoringthenoisein\nthesimulator),andsetting\nk\n=1.Equivalent,thiscanbederivedfrom\nEquation(15.9)usingtheapproximation\nE\ns\n0\n[\nV\n(\ns\n0\n)]\n\u02c7\nV\n(E\ns\n0\n[\ns\n0\n])\n(15.10)\n=\nV\n(\nf\n(\ns;a\n))\n;\n(15.11)\nwhereheretheexpectationisovertherandom\ns\n0\n\u02d8\nP\nsa\n.Solongasthenoise\nterms\n\nt\naresmall,thiswillusuallybeareasonableapproximation.\nHowever,forproblemsthatdon'tlendthemselvestosuchapproximations,\nhavingtosample\nk\nj\nA\nj\nstatesusingthemodel,inordertoapproximatethe\nexpectationabove,canbecomputationallyexpensive.\n15.5ConnectionsbetweenPolicyandValue\nIteration(Optional)\nInthepolicyiteration,line3ofAlgorithm7,wetypicallyuselinearsystem\nsolvertocompute\nV\n\u02c7\n.Alternatively,onecanalsotheiterativeBellman\nupdates,similarlytothevalueiteration,toevaluate\nV\n\u02c7\n,asintheProcedure\nVE(\n\n)inLine1ofAlgorithm8below.Hereifwetakeoption1inLine2of\ntheProcedureVE,thentherencebetweentheProcedureVEfromthe\n"}, {"page_number": 191, "text": "191\nAlgorithm8\nVariantofPolicyIteration\n1:\nprocedure\nVE\n(\n\u02c7\n,\nk\n)\n.\nToevaluate\nV\n\u02c7\n2:\nOption1:initialize\nV\n(\ns\n):=0;Option2:Initializefromthecurrent\nV\ninthemainalgorithm.\n3:\nfor\ni\n=0to\nk\n\n1\ndo\n4:\nForeverystate\ns\n,update\nV\n(\ns\n):=\nR\n(\ns\n)+\n\nX\ns\n0\nP\ns\u02c7\n(\ns\n)\n(\ns\n0\n)\nV\n(\ns\n0\n)\n:\n(15.12)\nreturn\nV\n5:\nRequire:\nhyperparameter\nk\n.\n6:\nInitialize\n\u02c7\nrandomly.\n7:\nfor\nuntilconvergence\ndo\n8:\nLet\nV\n=VE(\n\u02c7;k\n).\n9:\nForeachstate\ns\n,let\n\u02c7\n(\ns\n):=argmax\na\n2\nA\nX\ns\n0\nP\nsa\n(\ns\n0\n)\nV\n(\ns\n0\n)\n:\n(15.13)\n"}, {"page_number": 192, "text": "192\nvalueiteration(Algorithm6)isthatonline4,theprocedureisusingthe\nactionfrom\n\u02c7\ninsteadofthegreedyaction.\nUsingtheProcedureVE,wecanbuildAlgorithm8,whichisavariant\nofpolicyiterationthatservesanintermediatealgorithmthatconnectspol-\nicyiterationandvalueiteration.Herewearegoingtouseoption2inVE\ntomaximizethere-useofknowledgelearnedbefore.Onecanverifyindeed\nthatifwetake\nk\n=1anduseoption2inLine2inAlgorithm8,thenAlgo-\nrithm8issemanticallyequivalenttovalueiteration(Algorithm6).Inother\nwords,bothAlgorithm8andvalueiterationinterleavetheupdatesin(15.13)\nand(15.12).Algorithm8alternatebetween\nk\nstepsofupdate(15.12)and\nonestepof(15.13),whereasvalueiterationalternatesbetween1stepsofup-\ndate(15.12)andonestepof(15.13).ThereforegenerallyAlgorithm8should\nnotbefasterthanvalueiteration,becauseassumingthatupdate(15.12)\nand(15.13)areequallyusefulandtime-consuming,thentheoptimalbalance\noftheupdatefrequenciescouldbejust\nk\n=1or\nk\n\u02c7\n1.\nOntheotherhand,if\nk\nstepsofupdate(15.12)canbedonemuchfaster\nthan\nk\ntimesasinglestepof(15.12),thentakingadditionalstepsofequa-\ntion(15.12)ingroupmightbeuseful.Thisiswhatpolicyiterationislever-\naging|thelinearsystemsolvercangiveustheresultofProcedureVEwith\nk\n=\n1\nmuchfasterthanusingtheProcedureVEforalarge\nk\n.Onthe\nside,whensuchaspeeding-upnolongerexists,e.g.,,whenthestate\nspaceislargeandlinearsystemsolverisalsonotfast,thenvalueiterationis\nmorepreferable.\n"}, {"page_number": 193, "text": "Chapter16\nLQR,DDPandLQG\n16.1Finite-horizonMDPs\nInChapter15,weMarkovDecisionProcesses(MDPs)andcovered\nValueIteration/PolicyIterationinasetting.Morespwe\nintroducedthe\noptimalBellmanequation\nthattheoptimalvalue\nfunction\nV\n\u02c7\n\noftheoptimalpolicy\n\u02c7\n\n.\nV\n\u02c7\n\n(\ns\n)=\nR\n(\ns\n)+max\na\n2A\n\nX\ns\n0\n2\nS\nP\nsa\n(\ns\n0\n)\nV\n\u02c7\n\n(\ns\n0\n)\nRecallthatfromtheoptimalvaluefunction,wewereabletorecoverthe\noptimalpolicy\n\u02c7\n\nwith\n\u02c7\n\n(\ns\n)=argmax\na\n2A\nX\ns\n0\n2S\nP\nsa\n(\ns\n0\n)\nV\n\n(\ns\n0\n)\nInthischapter,we'llplaceourselvesinamoregeneralsetting:\n1.\nWewanttowriteequationsthatmakesenseforboththediscreteand\nthecontinuouscase.We'llthereforewrite\nE\ns\n0\n\u02d8\nP\nsa\n\nV\n\u02c7\n\n(\ns\n0\n)\n\ninsteadof\nX\ns\n0\n2\nS\nP\nsa\n(\ns\n0\n)\nV\n\u02c7\n\n(\ns\n0\n)\nmeaningthatwetaketheexpectationofthevaluefunctionatthenext\nstate.Inthecase,wecanrewritetheexpectationasasumover\n193\n"}, {"page_number": 194, "text": "194\nstates.Inthecontinuouscase,wecanrewritetheexpectationasan\nintegral.Thenotation\ns\n0\n\u02d8\nP\nsa\nmeansthatthestate\ns\n0\nissampledfrom\nthedistribution\nP\nsa\n.\n2.\nWe'llassumethattherewardsdependon\nbothstatesandactions\n.In\notherwords,\nR\n:\nSA!\nR\n.Thisimpliesthatthepreviousmechanism\nforcomputingtheoptimalactionischangedinto\n\u02c7\n\n(\ns\n)=argmax\na\n2A\nR\n(\ns;a\n)+\n\nE\ns\n0\n\u02d8\nP\nsa\n\nV\n\u02c7\n\n(\ns\n0\n)\n\n3.\nInsteadofconsideringanhorizonMDP,we'llassumethatwe\nhavea\nhorizonMDP\nthatwillbeasatuple\n(\nS\n;\nA\n;P\nsa\n;T;R\n)\nwith\nT>\n0the\ntimehorizon\n(forinstance\nT\n=100).Inthissetting,\nourofpayisgoingtobe(slightly)t:\nR\n(\ns\n0\n;a\n0\n)+\nR\n(\ns\n1\n;a\n1\n)+\n\n+\nR\n(\ns\nT\n;a\nT\n)\ninsteadofehorizoncase)\nR\n(\ns\n0\n;a\n0\n)+\nR\n(\ns\n1\n;a\n1\n)+\n\n2\nR\n(\ns\n2\n;a\n2\n)+\n:::\n1\nX\nt\n=0\nR\n(\ns\nt\n;a\nt\n)\n\nt\nWhathappenedtothediscountfactor\n\n?\nRememberthattheintro-\nductionof\n\nwas(partly)bythenecessityofmakingsurethat\nthesumwouldbeandwIftherewardsare\nboundedbyaconstant\n\nR\n,thepayisindeedboundedby\nj\n1\nX\nt\n=0\nR\n(\ns\nt\n)\n\nt\nj\n\nR\n1\nX\nt\n=0\n\nt\nandwerecognizeageometricsum!Here,asthepayisasum,\nthediscountfactor\n\nisnotnecessaryanymore.\n"}, {"page_number": 195, "text": "195\nInthisnewsetting,thingsbehavequitetly.First,theoptimal\npolicy\n\u02c7\n\nmightbenon-stationary,meaningthat\nitchangesovertime\n.\nInotherwords,nowwehave\n\u02c7\n(\nt\n)\n:\nS!A\nwherethesuperscript(\nt\n)denotesthepolicyattimestep\nt\n.Thedynam-\nicsofthehorizonMDPfollowingpolicy\n\u02c7\n(\nt\n)\nproceedsasfollows:\nwestartinsomestate\ns\n0\n,takesomeaction\na\n0\n:=\n\u02c7\n(0)\n(\ns\n0\n)accordingto\nourpolicyattimestep0.TheMDPtransitionstoasuccessor\ns\n1\n,drawn\naccordingto\nP\ns\n0\na\n0\n.Then,wegettopickanotheraction\na\n1\n:=\n\u02c7\n(1)\n(\ns\n1\n)\nfollowingournewpolicyattimestep1andsoon...\nWhydoestheoptimalpolicyhappentobenon-stationaryinthe\nhorizonsetting?\nIntuitively,aswehaveanumbersofactionsto\ntake,wemightwanttoadopttstrategiesdependingonwhere\nweareintheenvironmentandhowmuchtimewehaveleft.Imagine\nagridwith2goalswithrewards+1and+10.Atthebeginning,we\nmightwanttotakeactionstoaimforthe+10goal.Butifaftersome\nsteps,dynamicssomehowpushedusclosertothe+1goalandwedon't\nhaveenoughstepslefttobeabletoreachthe+10goal,thenabetter\nstrategywouldbetoaimforthe+1goal...\n4.\nThisobservationallowsustouse\ntimedependentdynamics\ns\nt\n+1\n\u02d8\nP\n(\nt\n)\ns\nt\n;a\nt\nmeaningthatthetransition'sdistribution\nP\n(\nt\n)\ns\nt\n;a\nt\nchangesovertime.The\nsamethingcanbesaidabout\nR\n(\nt\n)\n.Notethatthissettingisabetter\nmodelforreallife.Inacar,thegastankempties,changes,\netc.Combiningthepreviousremarks,we'llusethefollowinggeneral\nformulationforourhorizonMDP\n\nS\n;\nA\n;P\n(\nt\n)\nsa\n;T;R\n(\nt\n)\n\nRemark\n:noticethattheaboveformulationwouldbeequivalentto\naddingthetimeintothestate.\n"}, {"page_number": 196, "text": "196\nThevaluefunctionattime\nt\nforapolicy\n\u02c7\nisthendeinthesame\nwayasbefore,asanexpectationovertrajectoriesgeneratedfollowing\npolicy\n\u02c7\nstartinginstate\ns\n.\nV\nt\n(\ns\n)=\nE\n\nR\n(\nt\n)\n(\ns\nt\n;a\nt\n)+\n\n+\nR\n(\nT\n)\n(\ns\nT\n;a\nT\n)\nj\ns\nt\n=\ns;\u02c7\n\nNow,thequestionis\nInthissetting,howdowetheoptimalvaluefunction\nV\n\nt\n(\ns\n)=max\n\u02c7\nV\n\u02c7\nt\n(\ns\n)\nItturnsoutthatBellman'sequationforValueIterationismadefor\nDy-\nnamicProgramming\n.ThismaycomeasnosurpriseasBellmanisoneof\nthefathersofdynamicprogrammingandtheBellmanequationisstrongly\nrelatedtotheTounderstandhowwecansimplifytheproblemby\nadoptinganiteration-basedapproach,wemakethefollowingobservations:\n1.\nNoticethatattheendofthegame(fortimestep\nT\n),theoptimalvalue\nisobvious\n8\ns\n2S\n:\nV\n\nT\n(\ns\n):=max\na\n2A\nR\n(\nT\n)\n(\ns;a\n)(16.1)\n2.\nForanothertimestep0\n\nt<T\n,ifwesupposethatweknowthe\noptimalvaluefunctionforthenexttimestep\nV\n\nt\n+1\n,thenwehave\n8\nt<T;s\n2S\n:\nV\n\nt\n(\ns\n):=max\na\n2A\nh\nR\n(\nt\n)\n(\ns;a\n)+\nE\ns\n0\n\u02d8\nP\n(\nt\n)\nsa\n\nV\n\nt\n+1\n(\ns\n0\n)\n\ni\n(16.2)\nWiththeseobservationsinmind,wecancomeupwithacleveralgorithm\ntosolvefortheoptimalvaluefunction:\n1.\ncompute\nV\n\nT\nusingequation(16.1).\n2.\nfor\nt\n=\nT\n\n1\n;:::;\n0:\ncompute\nV\n\nt\nusing\nV\n\nt\n+1\nusingequation(16.2)\n"}, {"page_number": 197, "text": "197\nSidenote\nWecaninterpretstandardvalueiterationasaspecialcase\nofthisgeneralcase,butwithoutkeepingtrackoftime.Itturnsoutthat\ninthestandardsetting,ifwerunvalueiterationforTsteps,wegeta\n\nT\napproximationoftheoptimalvalueiteration(geometricconvergence).See\nproblemset4foraproofofthefollowingresult:\nTheorem\nLet\nB\ndenotetheBellmanupdateand\njj\nf\n(\nx\n)\njj\n1\n:=sup\nx\nj\nf\n(\nx\n)\nj\n.\nIf\nV\nt\ndenotesthevaluefunctionatthe\nt\n-thstep,then\njj\nV\nt\n+1\n\nV\n\njj\n1\n=\njj\nB\n(\nV\nt\n)\n\nV\n\njj\n1\n\n\njj\nV\nt\n\nV\n\njj\n1\n\n\nt\njj\nV\n1\n\nV\n\njj\n1\nInotherwords,theBellmanoperator\nB\nisa\n\n-contractingoperator.\n16.2LinearQuadraticRegulation(LQR)\nInthissection,we'llcoveraspecialcaseofthesettingdescribed\ninSection16.1,forwhichthe\nexactsolution\nis(easily)tractable.This\nmodeliswidelyusedinrobotics,andacommontechniqueinmanyproblems\nistoreducetheformulationtothisframework.\nFirst,let'sdescribethemodel'sassumptions.Weplaceourselvesinthe\ncontinuoussetting,with\nS\n=\nR\nd\n;\nA\n=\nR\nd\nandwe'llassume\nlineartransitions\n(withnoise)\ns\nt\n+1\n=\nA\nt\ns\nt\n+\nB\nt\na\nt\n+\nw\nt\nwhere\nA\nt\n2\nR\nd\n\nd\n;B\nt\n2\nR\nd\n\nd\narematricesand\nw\nt\n\u02d8N\n(0\n;\n\nt\n)issome\ngaussiannoise(with\nzero\nmean).Aswe'llshowinthefollowingparagraphs,\nitturnsoutthatthenoise,aslongasithaszeromean,doesnotimpactthe\noptimalpolicy!\nWe'llalsoassume\nquadraticrewards\nR\n(\nt\n)\n(\ns\nt\n;a\nt\n)=\n\ns\n>\nt\nU\nt\ns\nt\n\na\n>\nt\nW\nt\na\nt\n"}, {"page_number": 198, "text": "198\nwhere\nU\nt\n2\nR\nd\n\nn\n;W\nt\n2\nR\nd\n\nd\narepositiveitematrices(meaningthat\ntherewardisalways\nnegative\n).\nRemark\nNotethatthequadraticformulationoftherewardisequivalent\ntosayingthatwewantourstatetobeclosetotheorigin(wherethereward\nishigher).Forexample,if\nU\nt\n=\nI\nd\n(theidentitymatrix)and\nW\nt\n=\nI\nd\n,then\nR\nt\n=\n\ns\nt\njj\n2\njj\na\nt\njj\n2\n,meaningthatwewanttotakesmoothactions(small\nnormof\na\nt\n)togobacktotheorigin(smallnormof\ns\nt\n).Thiscouldmodela\ncartryingtostayinthemiddleoflanewithoutmakingimpulsivemoves...\nNowthatwehavetheassumptionsofourLQRmodel,let'scover\nthe2stepsoftheLQRalgorithm\nstep1\nsupposethatwedon'tknowthematrices\nA;B;\nToesti-\nmatethem,wecanfollowtheideasoutlinedintheValueAp-\nproximationsectionoftheRLnotes.First,collecttransitions\nfromanarbitrarypolicy.Then,uselinearregressionto\nargmin\nA;B\nP\nn\ni\n=1\nP\nT\n\n1\nt\n=0\n\n\n\ns\n(\ni\n)\nt\n+1\n\n\nAs\n(\ni\n)\nt\n+\nBa\n(\ni\n)\nt\n\n\n\n\n2\n.Finally,useatech-\nniqueseeninGaussianDiscriminantAnalysistolearn\nstep2\nassumingthattheparametersofourmodelareknown(givenoresti-\nmatedwithstep1),wecanderivetheoptimalpolicyusingdynamic\nprogramming.\nInotherwords,given\n(\ns\nt\n+1\n=\nA\nt\ns\nt\n+\nB\nt\na\nt\n+\nw\nt\nA\nt\n;B\nt\n;U\nt\n;W\nt\n;\n\nt\nknown\nR\n(\nt\n)\n(\ns\nt\n;a\nt\n)=\n\ns\n>\nt\nU\nt\ns\nt\n\na\n>\nt\nW\nt\na\nt\nwewanttocompute\nV\n\nt\n.Ifwegobacktosection16.1,wecanapply\ndynamicprogramming,whichyields\n1.\nInitializationstep\nForthelasttimestep\nT\n,\nV\n\nT\n(\ns\nT\n)=max\na\nT\n2A\nR\nT\n(\ns\nT\n;a\nT\n)\n=max\na\nT\n2A\n\ns\n>\nT\nU\nT\ns\nT\n\na\n>\nT\nW\nt\na\nT\n=\n\ns\n>\nT\nU\nt\ns\nT\n(maximizedfor\na\nT\n=0)\n"}, {"page_number": 199, "text": "199\n2.\nRecurrencestep\nLet\nt<T\n.Supposeweknow\nV\n\nt\n+1\n.\nFact1:\nItcanbeshownthatif\nV\n\nt\n+1\nisaquadraticfunctionin\ns\nt\n,then\nV\n\nt\nisalsoaquadraticfunction.Inotherwords,thereexistssomematrix\nandsomescalarsuchthat\nif\nV\n\nt\n+1\n(\ns\nt\n+1\n)=\ns\n>\nt\n+1\n\nt\n+1\ns\nt\n+1\n+\nt\n+1\nthen\nV\n\nt\n(\ns\nt\n)=\ns\n>\nt\n\nt\ns\nt\n+\nt\nFortimestep\nt\n=\nT\n,wehad\nt\n=\n\nU\nT\nand\nT\n=0.\nFact2:\nWecanshowthattheoptimalpolicyisjustalinearfunctionof\nthestate.\nKnowing\nV\n\nt\n+1\nisequivalenttoknowing\nt\n+1\nand\nt\n+1\n,sowejustneed\ntoexplainhowwecompute\nt\nand\nt\nfrom\nt\n+1\nand\nt\n+1\nandtheother\nparametersoftheproblem.\nV\n\nt\n(\ns\nt\n)=\ns\n>\nt\n\nt\ns\nt\n+\nt\n=max\na\nt\nh\nR\n(\nt\n)\n(\ns\nt\n;a\nt\n)+\nE\ns\nt\n+1\n\u02d8\nP\n(\nt\n)\ns\nt\n;a\nt\n[\nV\n\nt\n+1\n(\ns\nt\n+1\n)]\ni\n=max\na\nt\n\n\ns\n>\nt\nU\nt\ns\nt\n\na\n>\nt\nV\nt\na\nt\n+\nE\ns\nt\n+1\n\u02d8N\n(\nA\nt\ns\nt\n+\nB\nt\na\nt\n;\n\nt\n)\n[\ns\n>\nt\n+1\n\nt\n+1\ns\nt\n+1\n+\nt\n+1\n]\n\nwherethesecondlineisjusttheoftheoptimalvaluefunction\nandthethirdlineisobtainedbyplugginginthedynamicsofourmodel\nalongwiththequadraticassumption.Noticethatthelastexpressionis\naquadraticfunctionin\na\nt\nandcanthusbe(easily)optimized\n1\n.Weget\ntheoptimalaction\na\n\nt\na\n\nt\n=\n\n(\nB\n>\nt\n\nt\n+1\nB\nt\n\nV\nt\n)\n\n1\nB\nt\n\nt\n+1\nA\nt\n\n\ns\nt\n=\nL\nt\n\ns\nt\nwhere\nL\nt\n:=\n\n(\nB\n>\nt\n\nt\n+1\nB\nt\n\nW\nt\n)\n\n1\nB\nt\n\nt\n+1\nA\nt\n\n1\nUsetheidentity\nE\n\nw\n>\nt\n\nt\n+1\nw\nt\n\n=T\nt\n\nt\n+1\n)with\nw\nt\n\u02d8N\n(0\n;\n\nt\n)\n"}, {"page_number": 200, "text": "200\nwhichisanimpressiveresult:ouroptimalpolicyis\nlinearin\ns\nt\n.Given\na\n\nt\nwecansolvefor\nt\nand\nt\n.Wegetthe\nDiscreteRicatti\nequations\n\nt\n=\nA\n>\nt\n\n\nt\n+1\n\n\nt\n+1\nB\nt\n\nB\n>\nt\n\nt\n+1\nB\nt\n\nW\nt\n\n\n1\nB\nt\n\nt\n+1\n\nA\nt\n\nU\nt\n\nt\n=\n\ntr\nt\n\nt\n+1\n)+\nt\n+1\nFact3:\nwenoticethat\nt\ndependsonneithernorthenoise\nt\n!As\nL\nt\nisafunctionof\nA\nt\n;B\nt\nand\nt\n+1\n,itimpliesthattheoptimalpolicyalso\ndoesnotdependonthenoise\n!(But\nt\ndoesdependon\nt\n,which\nimpliesthat\nV\n\nt\ndependson\nt\n.)\nThen,tosummarize,theLQRalgorithmworksasfollows\n1.\n(ifnecessary)estimateparameters\nA\nt\n;B\nt\n;\n\nt\n2.\ninitialize\nT\n:=\n\nU\nT\nand\nT\n:=0.\n3.\niteratefrom\nt\n=\nT\n\n1\n:::\n0toupdate\nt\nand\nt\nusing\nt\n+1\nand\nt\n+1\nusingthediscreteRicattiequations.Ifthereexistsapolicythatdrives\nthestatetowardszero,thenconvergenceisguaranteed!\nUsingFact3\n,wecanbeevenmorecleverandmakeouralgorithmrun\n(slightly)faster!Astheoptimalpolicydoesnotdependon\nt\n,andthe\nupdateof\nt\nonlydependson\nt\n,itisttoupdate\nonly\n\nt\n!\n16.3Fromnon-lineardynamicstoLQR\nItturnsoutthatalotofproblemscanbereducedtoLQR,evenifdynamics\narenon-linear.WhileLQRisaniceformulationbecauseweareabletocome\nupwithaniceexactsolution,itisfarfrombeinggeneral.Let'stakefor\ninstancethecaseoftheinvertedpendulum.Thetransitionsbetweenstates\nlooklike\n0\nB\nB\n@\nx\nt\n+1\n_\nx\nt\n+1\n\nt\n+1\n_\n\nt\n+1\n1\nC\nC\nA\n=\nF\n0\nB\nB\n@\n0\nB\nB\n@\nx\nt\n_\nx\nt\n\nt\n_\n\nt\n1\nC\nC\nA\n;a\nt\n1\nC\nC\nA\nwherethefunction\nF\ndependsonthecosoftheangleetc.Now,the\nquestionwemayaskis\nCanwelinearizethissystem?\n"}, {"page_number": 201, "text": "201\n16.3.1Linearizationofdynamics\nLet'ssupposethatattime\nt\n,thesystemspendsmostofitstimeinsomestate\n\ns\nt\nandtheactionsweperformarearound\na\nt\n.Fortheinvertedpendulum,if\nwereachedsomekindofoptimal,thisistrue:ouractionsaresmallandwe\ndon'tdeviatemuchfromthevertical.\nWearegoingtouseTaylorexpansiontolinearizethedynamics.Inthe\nsimplecasewherethestateisone-dimensionalandthetransitionfunction\nF\ndoesnotdependontheaction,wewouldwritesomethinglike\ns\nt\n+1\n=\nF\n(\ns\nt\n)\n\u02c7\nF\n(\ns\nt\n)+\nF\n0\n(\ns\nt\n)\n\n(\ns\nt\n\n\ns\nt\n)\nInthemoregeneralsetting,theformulalooksthesame,withgradients\ninsteadofsimplederivatives\ns\nt\n+1\n\u02c7\nF\n(\ns\nt\n;\n\na\nt\n)+\nr\ns\nF\n(\ns\nt\n;\n\na\nt\n)\n\n(\ns\nt\n\n\ns\nt\n)+\nr\na\nF\n(\ns\nt\n;\n\na\nt\n)\n\n(\na\nt\n\n\na\nt\n)(16.3)\nandnow,\ns\nt\n+1\nislinearin\ns\nt\nand\na\nt\n,becausewecanrewriteequation(16.3)\nas\ns\nt\n+1\n\u02c7\nAs\nt\n+\nBs\nt\n+\n\nwhere\n\nissomeconstantand\nA;B\narematrices.Now,thiswritinglooks\nawfullysimilartotheassumptionsmadeforLQR.Wejusthavetogetrid\noftheconstantterm\n\n!Itturnsoutthattheconstanttermcanbeabsorbed\ninto\ns\nt\nbyincreasingthedimensionbyone.Thisisthesametrick\nthatweusedatthebeginningoftheclassforlinearregression...\n16.3.2tialDynamicProgramming(DDP)\nThepreviousmethodworkswellforcaseswherethegoalistostayaround\nsomestate\ns\n\n(thinkabouttheinvertedpendulum,oracarhavingtostay\ninthemiddleofalane).However,insomecases,thegoalcanbemore\ncomplicated.\nWe'llcoveramethodthatapplieswhenoursystemhastofollowsome\ntrajectory(thinkaboutarocket).Thismethodisgoingtodiscretizethe\ntrajectoryintodiscretetimesteps,andcreateintermediarygoalsaround\nwhichwewillbeabletousetheprevioustechnique!Thismethodiscalled\ntialDynamicProgramming\n.Themainstepsare\n"}, {"page_number": 202, "text": "202\nstep1\ncomeupwithanominaltrajectoryusinganaivecontroller,thatapprox-\nimatethetrajectorywewanttofollow.Inotherwords,ourcontroller\nisabletoapproximatethegoldtrajectorywith\ns\n\n0\n;a\n\n0\n!\ns\n\n1\n;a\n\n1\n!\n:::\nstep2\nlinearizethedynamicsaroundeachtrajectorypoint\ns\n\nt\n,inotherwords\ns\nt\n+1\n\u02c7\nF\n(\ns\n\nt\n;a\n\nt\n)+\nr\ns\nF\n(\ns\n\nt\n;a\n\nt\n)(\ns\nt\n\ns\n\nt\n)+\nr\na\nF\n(\ns\n\nt\n;a\n\nt\n)(\na\nt\n\na\n\nt\n)\nwhere\ns\nt\n;a\nt\nwouldbeourcurrentstateandaction.Nowthatwehave\nalinearapproximationaroundeachofthesepoints,wecanusethe\nprevioussectionandrewrite\ns\nt\n+1\n=\nA\nt\n\ns\nt\n+\nB\nt\n\na\nt\n(noticethatinthatcase,weusethenon-stationarydynamicssetting\nthatwementionedatthebeginningoftheselecturenotes)\nNote\nWecanapplyasimilarderivationforthereward\nR\n(\nt\n)\n,witha\nsecond-orderTaylorexpansion.\nR\n(\ns\nt\n;a\nt\n)\n\u02c7\nR\n(\ns\n\nt\n;a\n\nt\n)+\nr\ns\nR\n(\ns\n\nt\n;a\n\nt\n)(\ns\nt\n\ns\n\nt\n)+\nr\na\nR\n(\ns\n\nt\n;a\n\nt\n)(\na\nt\n\na\n\nt\n)\n+\n1\n2\n(\ns\nt\n\ns\n\nt\n)\n>\nH\nss\n(\ns\nt\n\ns\n\nt\n)+(\ns\nt\n\ns\n\nt\n)\n>\nH\nsa\n(\na\nt\n\na\n\nt\n)\n+\n1\n2\n(\na\nt\n\na\n\nt\n)\n>\nH\naa\n(\na\nt\n\na\n\nt\n)\nwhere\nH\nxy\nreferstotheentryoftheHessianof\nR\nwithrespectto\nx\nand\ny\nevaluatedin(\ns\n\nt\n;a\n\nt\n)(omittedforreadability).Thisexpressioncanbe\nre-writtenas\nR\nt\n(\ns\nt\n;a\nt\n)=\n\ns\n>\nt\nU\nt\ns\nt\n\na\n>\nt\nW\nt\na\nt\nforsomematrices\nU\nt\n;W\nt\n,withthesametrickofaddinganextradimen-\nsionofones.Toconvinceyourself,noticethat\n\n1\nx\n\n\n\nab\nbc\n\n\n\n1\nx\n\n=\na\n+2\nbx\n+\ncx\n2\n"}, {"page_number": 203, "text": "203\nstep3\nNow,youcanconvinceyourselfthatourproblemis\nstrictly\nre-written\nintheLQRframework.Let'sjustuseLQRtotheoptimalpolicy\n\u02c7\nt\n.Asaresult,ournewcontrollerwill(hopefully)bebetter!\nNote:\nSomeproblemsmightariseiftheLQRtrajectorydeviatestoo\nmuchfromthelinearizedapproximationofthetrajectory,butthatcan\nbeedwithreward-shaping...\nstep4\nNowthatwegetanewcontroller(ournewpolicy\n\u02c7\nt\n),weuseitto\nproduceanewtrajectory\ns\n\n0\n;\u02c7\n0\n(\ns\n\n0\n)\n!\ns\n\n1\n;\u02c7\n1\n(\ns\n\n1\n)\n!\n:::\n!\ns\n\nT\nnotethatwhenwegeneratethisnewtrajectory,weusethereal\nF\nand\nnotitslinearapproximationtocomputetransitions,meaningthat\ns\n\nt\n+1\n=\nF\n(\ns\n\nt\n;a\n\nt\n)\nthen,gobacktostep2andrepeatuntilsomestoppingcriterion.\n16.4LinearQuadraticGaussian(LQG)\nOften,intherealword,wedon'tgettoobservethefullstate\ns\nt\n.Forexample,\nanautonomouscarcouldreceiveanimagefromacamera,whichismerely\nan\nobservation\n,andnotthefullstateoftheworld.Sofar,weassumed\nthatthestatewasavailable.Asthismightnotholdtrueformostofthe\nreal-worldproblems,weneedanewtooltomodelthissituation:\nPartially\nObservableMDPs\n.\nAPOMDPisanMDPwithanextraobservationlayer.Inotherwords,\nweintroduceanewvariable\no\nt\n,thatfollowssomeconditionaldistribution\ngiventhecurrentstate\ns\nt\no\nt\nj\ns\nt\n\u02d8\nO\n(\no\nj\ns\n)\nFormally,aPOMDPisgivenbyatuple\n(\nS\n;\nO\n;\nA\n;P\nsa\n;T;R\n)\nWithinthisframework,thegeneralstrategyistomaintaina\nbeliefstate\n(distributionoverstates)basedontheobservation\no\n1\n;:::;o\nt\n.Then,apolicy\ninaPOMDPmapsthisbeliefstatestoactions.\n"}, {"page_number": 204, "text": "204\nInthissection,we'llpresentaextensionofLQRtothisnewsetting.\nAssumethatweobserve\ny\nt\n2\nR\nn\nwith\nm<n\nsuchthat\n(\ny\nt\n=\nC\n\ns\nt\n+\nv\nt\ns\nt\n+1\n=\nA\n\ns\nt\n+\nB\n\na\nt\n+\nw\nt\nwhere\nC\n2\nR\nn\n\nd\nisacompressionmatrixand\nv\nt\nisthesensornoise(also\ngaussian,like\nw\nt\n).Notethattherewardfunction\nR\n(\nt\n)\nisleftunchanged,asa\nfunctionofthestate(nottheobservation)andaction.Also,asdistributions\naregaussian,thebeliefstateisalsogoingtobegaussian.Inthisnewframe-\nwork,let'sgiveanoverviewofthestrategywearegoingtoadopttothe\noptimalpolicy:\nstep1\ncomputethedistributiononthepossiblestates(thebeliefstate),\nbasedontheobservationswehave.Inotherwords,wewanttocompute\nthemean\ns\nt\nj\nt\nandthecovariance\nt\nj\nt\nof\ns\nt\nj\ny\n1\n;:::;y\nt\n\u02d8N\n\ns\nt\nj\nt\n;\n\nt\nj\nt\n\ntoperformthecomputationtlyovertime,we'llusethe\nKalman\nFilter\nalgorithm(usedon-boardApolloLunarModule!).\nstep2\nnowthatwehavethedistribution,we'llusethemean\ns\nt\nj\nt\nasthebest\napproximationfor\ns\nt\nstep3\nthensettheaction\na\nt\n:=\nL\nt\ns\nt\nj\nt\nwhere\nL\nt\ncomesfromtheregularLQR\nalgorithm.\nIntuitively,tounderstandwhythisworks,noticethat\ns\nt\nj\nt\nisanoisyap-\nproximationof\ns\nt\n(equivalenttoaddingmorenoisetoLQR)butweproved\nthatLQRisindependentofthenoise!\nStep1needstobeexplicated.We'llcoverasimplecasewherethereis\nnoactiondependenceinourdynamics(butthegeneralcasefollowsthesame\nidea).Supposethat\n(\ns\nt\n+1\n=\nA\n\ns\nt\n+\nw\nt\n;w\nt\n\u02d8\nN\n(0\n;\n\ns\n)\ny\nt\n=\nC\n\ns\nt\n+\nv\nt\n;v\nt\n\u02d8\nN\n(0\n;\n\ny\n)\nAsnoisesareGaussians,wecaneasilyprovethatthejointdistributionis\nalsoGaussian\n"}, {"page_number": 205, "text": "205\n0\nB\nB\nB\nB\nB\nB\nB\n@\ns\n1\n.\n.\n.\ns\nt\ny\n1\n.\n.\n.\ny\nt\n1\nC\nC\nC\nC\nC\nC\nC\nA\n\u02d8N\n(\n\nforsome\n\n\nthen,usingthemarginalformulasofgaussians(seeFactorAnalysisnotes),\nwewouldget\ns\nt\nj\ny\n1\n;:::;y\nt\n\u02d8N\n\ns\nt\nj\nt\n;\n\nt\nj\nt\n\nHowever,computingthemarginaldistributionparametersusingthese\nformulaswouldbecomputationallyexpensive!Itwouldrequiremanipulating\nmatricesofshape\nt\n\nt\n.Recallthatinvertingamatrixcanbedonein\nO\n(\nt\n3\n),\nanditwouldthenhavetoberepeatedoverthetimesteps,yieldingacostin\nO\n(\nt\n4\n)!\nThe\nKalman\nalgorithmprovidesamuchbetterwayofcomputing\nthemeanandvariance,byupdatingthemovertimein\nconstanttimein\nt\n!Thekalmanisbasedontwobasicssteps.Assumethatweknowthe\ndistributionof\ns\nt\nj\ny\n1\n;:::;y\nt\n:\npredictstep\ncompute\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\nupdatestep\ncompute\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\n+1\nanditerateovertimesteps!Thecombinationofthepredictandupdate\nstepsupdatesourbeliefstates.Inotherwords,theprocesslookslike\n(\ns\nt\nj\ny\n1\n;:::;y\nt\n)\npredict\n!\n(\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\n)\nupdate\n!\n(\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\n+1\n)\npredict\n!\n:::\npredictstep\nSupposethatweknowthedistributionof\ns\nt\nj\ny\n1\n;:::;y\nt\n\u02d8N\n\ns\nt\nj\nt\n;\n\nt\nj\nt\n\nthen,thedistributionoverthenextstateisalsoagaussiandistribution\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\n\u02d8N\n\ns\nt\n+1\nj\nt\n;\n\nt\n+1\nj\nt\n\nwhere\n"}, {"page_number": 206, "text": "206\n(\ns\nt\n+1\nj\nt\n=\nA\n\ns\nt\nj\nt\n\nt\n+1\nj\nt\n=\nA\n\n\nt\nj\nt\n\nA\n>\n+\ns\nupdatestep\ngiven\ns\nt\n+1\nj\nt\nand\nt\n+1\nj\nt\nsuchthat\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\n\u02d8N\n\ns\nt\n+1\nj\nt\n;\n\nt\n+1\nj\nt\n\nwecanprovethat\ns\nt\n+1\nj\ny\n1\n;:::;y\nt\n+1\n\u02d8N\n\ns\nt\n+1\nj\nt\n+1\n;\n\nt\n+1\nj\nt\n+1\n\nwhere\n(\ns\nt\n+1\nj\nt\n+1\n=\ns\nt\n+1\nj\nt\n+\nK\nt\n(\ny\nt\n+1\n\nCs\nt\n+1\nj\nt\n)\n\nt\n+1\nj\nt\n+1\n=\nt\n+1\nj\nt\n\nK\nt\n\nC\n\n\nt\n+1\nj\nt\nwith\nK\nt\n:=\nt\n+1\nj\nt\nC\n>\n(\nC\n\nt\n+1\nj\nt\nC\n>\n+\ny\n)\n\n1\nThematrix\nK\nt\niscalledthe\nKalmangain\n.\nNow,ifwehaveacloserlookattheformulas,wenoticethatwedon't\nneedtheobservationspriortotimestep\nt\n!Theupdatestepsonlydepends\nonthepreviousdistribution.Puttingitalltogether,thealgorithmruns\naforwardpasstocomputethe\nK\nt\n,\nt\nj\nt\nand\ns\nt\nj\nt\n(sometimesreferredtoas\n^\ns\nintheliterature).Then,itrunsabackwardpass(theLQRupdates)to\ncomputethequantities\nt\n;\n\nt\nand\nL\nt\n.Finally,werecovertheoptimalpolicy\nwith\na\n\nt\n=\nL\nt\ns\nt\nj\nt\n.\n"}, {"page_number": 207, "text": "Chapter17\nPolicyGradient\n(REINFORCE)\nWewillpresentamodel-freealgorithmcalledREINFORCEthatdoesnot\nrequirethenotionofvaluefunctionsand\nQ\nfunctions.Itturnsouttobemore\nconvenienttointroduceREINFORCEinthehorizoncase,whichwill\nbeassumedthroughoutthisnote:weuse\n\u02dd\n=(\ns\n0\n;a\n0\n;:::;s\nT\n\n1\n;a\nT\n\n1\n;s\nT\n)to\ndenoteatrajectory,where\nT<\n1\nisthelengthofthetrajectory.Moreover,\nREINFORCEonlyappliestolearninga\nrandomizedpolicy\n.Weuse\n\u02c7\n\n(\na\nj\ns\n)\ntodenotetheprobabilityofthepolicy\n\u02c7\n\noutputtingtheaction\na\natstate\ns\n.\nTheothernotationswillbethesameasinpreviouslecturenotes.\nTheadvantageofapplyingREINFORCEisthatweonlyneedtoassume\nthatwecansamplefromthetransitionprobabilities\nf\nP\nsa\ng\nandcanquerythe\nrewardfunction\nR\n(\ns;a\n)atstate\ns\nandaction\na\n,\n1\nbutwedonotneedtoknow\ntheanalyticalformofthetransitionprobabilitiesortherewardfunction.\nWedonotexplicitlylearnthetransitionprobabilitiesortherewardfunction\neither.\nLet\ns\n0\nbesampledfromsomedistribution\n\n.Weconsideroptimizingthe\nexpectedtotalpayofthepolicy\n\u02c7\n\novertheparameter\n\nas.\n\n(\n\n)\n,\nE\n\"\nT\n\n1\nX\nt\n=0\n\nt\nR\n(\ns\nt\n;a\nt\n)\n#\n(17.1)\nRecallthat\ns\nt\n\u02d8\nP\ns\nt\n\n1\na\nt\n\n1\nand\na\nt\n\u02d8\n\u02c7\n\n(\n\ns\nt\n).Alsonotethat\n\n(\n\n)=\nE\ns\n0\n\u02d8\nP\n[\nV\n\u02c7\n\n(\ns\n0\n)]ifweignorethebetweenandhori-\nzon.\n1\nInthisnoteswewillworkwiththegeneralsettingwheretherewarddependsonboth\nthestateandtheaction.\n207\n"}, {"page_number": 208, "text": "208\nWeaimtousegradientascenttomaximize\n\n(\n\n).Themainchallenge\nwefacehereistocompute(orestimate)thegradientof\n\n(\n\n)withoutthe\nknowledgeoftheformoftherewardfunctionandthetransitionprobabilities.\nLet\nP\n\n(\n\u02dd\n)denotethedistributionof\n\u02dd\n(generatedbythepolicy\n\u02c7\n\n),and\nlet\nf\n(\n\u02dd\n)=\nP\nT\n\n1\nt\n=0\n\nt\nR\n(\ns\nt\n;a\nt\n).Wecanrewrite\n\n(\n\n)as\n\n(\n\n)=E\n\u02dd\n\u02d8\nP\n\n[\nf\n(\n\u02dd\n)](17.2)\nWefaceasimilarsituationsinthevariationalauto-encoder(VAE)setting\ncoveredinthepreviouslectures,wheretheweneedtotakethegradientw.r.t\ntoavariablethatshowsupundertheexpectation|thedistribution\nP\n\ndependson\n\n.RecallthatinVAE,weusedthere-parametrizationtechniques\ntoaddressthisproblem.Howeveritdoesnotapplyherebecausewedo\nknownothowtocomputethegradientofthefunction\nf\n.(Weonlyhave\nantwaytoevaluatethefunction\nf\nbytakingaweightedsumofthe\nobservedrewards,butwedonotnecessarilyknowtherewardfunctionitself\ntocomputethegradient.)\nTheREINFORCEalgorithmusesananotherapproachtoestimatethe\ngradientof\n\n(\n\n).Westartwiththefollowingderivation:\nr\n\nE\n\u02dd\n\u02d8\nP\n\n[\nf\n(\n\u02dd\n)]=\nr\n\nZ\nP\n\n(\n\u02dd\n)\nf\n(\n\u02dd\n)\nd\u02dd\n=\nZ\nr\n\n(\nP\n\n(\n\u02dd\n)\nf\n(\n\u02dd\n))\nd\u02dd\n(swapintegrationwithgradient)\n=\nZ\n(\nr\n\nP\n\n(\n\u02dd\n))\nf\n(\n\u02dd\n)\nd\u02dd\n(becaue\nf\ndoesnotdependon\n\n)\n=\nZ\nP\n\n(\n\u02dd\n)(\nr\n\nlog\nP\n\n(\n\u02dd\n))\nf\n(\n\u02dd\n)\nd\u02dd\n(because\nr\nlog\nP\n\n(\n\u02dd\n)=\nr\nP\n\n(\n\u02dd\n)\nP\n\n(\n\u02dd\n)\n)\n=E\n\u02dd\n\u02d8\nP\n\n[(\nr\n\nlog\nP\n\n(\n\u02dd\n))\nf\n(\n\u02dd\n)](17.3)\nNowwehaveasample-basedestimatorfor\nr\n\nE\n\u02dd\n\u02d8\nP\n\n[\nf\n(\n\u02dd\n)].Let\n\u02dd\n(1)\n;:::;\u02dd\n(\nn\n)\nbe\nn\nempiricalsamplesfrom\nP\n\n(whichareobtainedbyrunningthepolicy\n\u02c7\n\nfor\nn\ntimes,with\nT\nstepsforeachrun).Wecanestimatethegradientof\n\n(\n\n)by\nr\n\nE\n\u02dd\n\u02d8\nP\n\n[\nf\n(\n\u02dd\n)]=E\n\u02dd\n\u02d8\nP\n\n[(\nr\n\nlog\nP\n\n(\n\u02dd\n))\nf\n(\n\u02dd\n)](17.4)\n\u02c7\n1\nn\nn\nX\ni\n=1\n(\nr\n\nlog\nP\n\n(\n\u02dd\n(\ni\n)\n))\nf\n(\n\u02dd\n(\ni\n)\n)(17.5)\n"}, {"page_number": 209, "text": "209\nThenextquestionishowtocomputelog\nP\n\n(\n\u02dd\n).Wederiveananalyt-\nicalformulaforlog\nP\n\n(\n\u02dd\n)andcomputeitsgradientw.r.t\n\n(usingauto-\ntiation).Usingtheof\n\u02dd\n,wehave\nP\n\n(\n\u02dd\n)=\n\n(\ns\n0\n)\n\u02c7\n\n(\na\n0\nj\ns\n0\n)\nP\ns\n0\na\n0\n(\ns\n1\n)\n\u02c7\n\n(\na\n1\nj\ns\n1\n)\nP\ns\n1\na\n1\n(\ns\n2\n)\n\nP\ns\nT\n\n1\na\nT\n\n1\n(\ns\nT\n)(17.6)\nHererecallthat\n\ntousedtodenotethedensityofthedistributionof\ns\n0\n.It\nfollowsthat\nlog\nP\n\n(\n\u02dd\n)=log\n\n(\ns\n0\n)+log\n\u02c7\n\n(\na\n0\nj\ns\n0\n)+log\nP\ns\n0\na\n0\n(\ns\n1\n)+log\n\u02c7\n\n(\na\n1\nj\ns\n1\n)\n+log\nP\ns\n1\na\n1\n(\ns\n2\n)+\n\n+log\nP\ns\nT\n\n1\na\nT\n\n1\n(\ns\nT\n)(17.7)\nTakinggradientw.r.tto\n\n,weobtain\nr\n\nlog\nP\n\n(\n\u02dd\n)=\nr\n\nlog\n\u02c7\n\n(\na\n0\nj\ns\n0\n)+\nr\n\nlog\n\u02c7\n\n(\na\n1\nj\ns\n1\n)+\n\n+\nr\n\nlog\n\u02c7\n\n(\na\nT\n\n1\nj\ns\nT\n\n1\n)\nNotethatmanyofthetermsdisappearbecausetheydon'tdependon\n\nand\nthushavezerogradients.(Thisissomewhatimportant|wedon'tknowhow\ntoevaluatethosetermssuchaslog\nP\ns\n0\na\n0\n(\ns\n1\n)becausewedon'thaveaccessto\nthetransitionprobabilities,butluckilythosetermshavezerogradients!)\nPluggingtheequationaboveintoequation(17.4),weconcludethat\nr\n\n\n(\n\n)=\nr\n\nE\n\u02dd\n\u02d8\nP\n\n[\nf\n(\n\u02dd\n)]=E\n\u02dd\n\u02d8\nP\n\n\" \nT\n\n1\nX\nt\n=0\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n!\n\nf\n(\n\u02dd\n)\n#\n=E\n\u02dd\n\u02d8\nP\n\n\" \nT\n\n1\nX\nt\n=0\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n!\n\n \nT\n\n1\nX\nt\n=0\n\nt\nR\n(\ns\nt\n;a\nt\n)\n!#\n(17.8)\nWeestimatetheRHSoftheequationabovebyempiricalsampletrajectories,\nandtheestimateisunbiased.ThevanillaREINFORCEalgorithmiteratively\nupdatestheparameterbygradientascentusingtheestimatedgradients.\nInterpretationofthepolicygradientformula\n(17.8)\n.\nThequantity\nr\n\nP\n\n(\n\u02dd\n)=\nP\nT\n\n1\nt\n=0\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)isintuitivelythedirectionofthechange\nof\n\nthatwillmakethetrajectory\n\u02dd\nmorelikelytooccur(orincreasethe\nprobabilityofchoosingaction\na\n0\n;:::;a\nt\n\n1\n),and\nf\n(\n\u02dd\n)isthetotalpayof\nthistrajectory.Thus,bytakingagradientstep,intuitivelywearetryingto\nimprovethelikelihoodofallthetrajectories,butwithatemphasis\norweightforeach\n\u02dd\n(orforeachsetofactions\na\n0\n;a\n1\n;:::;a\nt\n\n1\n).If\n\u02dd\nisvery\nrewarding(thatis,\nf\n(\n\u02dd\n)islarge),wetryveryhardtomoveinthedirection\n"}, {"page_number": 210, "text": "210\nthatcanincreasetheprobabilityofthetrajectory\n\u02dd\n(orthedirectionthat\nincreasestheprobabilityofchoosing\na\n0\n;:::;a\nt\n\n1\n),andif\n\u02dd\nhaslowpay\nwetrylesshardwithasmallerweight.\nAninterestingfactthatfollowsfromformula(17.3)isthat\nE\n\u02dd\n\u02d8\nP\n\n\"\nT\n\n1\nX\nt\n=0\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n#\n=0(17.9)\nToseethis,wetake\nf\n(\n\u02dd\n)=1(thatis,therewardisalwaysaconstant),\nthentheLHSof(17.8)iszerobecausethepayisalwaysaconstant\nP\nT\nt\n=0\n\nt\n.ThustheRHSof(17.8)isalsozero,whichimplies(17.9).\nInfact,onecanverifythatE\na\nt\n\u02d8\n\u02c7\n\n(\n\ns\nt\n)\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)=0forany\nt\nand\ns\nt\n.\n2\nThisfacthastwoconsequences.First,wecansimplifyformula(17.8)\nto\nr\n\n\n(\n\n)=\nT\n\n1\nX\nt\n=0\nE\n\u02dd\n\u02d8\nP\n\n\"\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\n \nT\n\n1\nX\nj\n=0\n\nj\nR\n(\ns\nj\n;a\nj\n)\n!#\n=\nT\n\n1\nX\nt\n=0\nE\n\u02dd\n\u02d8\nP\n\n\"\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\n \nT\n\n1\nX\nj\n\nt\n\nj\nR\n(\ns\nj\n;a\nj\n)\n!#\n(17.10)\nwherethesecondequalityfollowsfrom\nE\n\u02dd\n\u02d8\nP\n\n\"\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\n \nX\n0\n\nj<t\n\nj\nR\n(\ns\nj\n;a\nj\n)\n!#\n=E\n\"\nE[\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\nj\ns\n0\n;a\n0\n;:::;s\nt\n\n1\n;a\nt\n\n1\n;s\nt\n]\n\n \nX\n0\n\nj<t\n\nj\nR\n(\ns\nj\n;a\nj\n)\n!#\n=0(becauseE[\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\nj\ns\n0\n;a\n0\n;:::;s\nt\n\n1\n;a\nt\n\n1\n;s\nt\n]=0)\nNotethathereweusedthelawoftotalexpectation.Theouterexpecta-\ntioninthesecondlineaboveisovertherandomnessof\ns\n0\n;a\n0\n;:::;a\nt\n\n1\n;s\nt\n,\nwhereastheinnerexpectationisovertherandomnessof\na\nt\n(conditionedon\ns\n0\n;a\n0\n;:::;a\nt\n\n1\n;s\nt\n.)Weseethatwe'vemadetheestimatorslightlysimpler.\nThesecondconsequenceofE\na\nt\n\u02d8\n\u02c7\n\n(\n\ns\nt\n)\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)=0isthefollowing:for\nanyvalue\nB\n(\ns\nt\n)thatonlydependson\ns\nt\n,itholdsthat\nE\n\u02dd\n\u02d8\nP\n\n[\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\nB\n(\ns\nt\n)]\n=E[E[\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\nj\ns\n0\n;a\n0\n;:::;s\nt\n\n1\n;a\nt\n\n1\n;s\nt\n]\nB\n(\ns\nt\n)]\n=0(becauseE[\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\nj\ns\n0\n;a\n0\n;:::;s\nt\n\n1\n;a\nt\n\n1\n;s\nt\n]=0)\n2\nIngeneral,it'struethatE\nx\n\u02d8\np\n\n[\nr\nlog\np\n\n(\nx\n)]=0.\n"}, {"page_number": 211, "text": "211\nAgainhereweusedthelawoftotalexpectation.Theouterexpecta-\ntioninthesecondlineaboveisovertherandomnessof\ns\n0\n;a\n0\n;:::;a\nt\n\n1\n;s\nt\n,\nwhereastheinnerexpectationisovertherandomnessof\na\nt\n(conditionedon\ns\n0\n;a\n0\n;:::;a\nt\n\n1\n;s\nt\n.)Itfollowsfromequation(17.10)andtheequationabove\nthat\nr\n\n\n(\n\n)=\nT\n\n1\nX\nt\n=0\nE\n\u02dd\n\u02d8\nP\n\n\"\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\n \nT\n\n1\nX\nj\n\nt\n\nj\nR\n(\ns\nj\n;a\nj\n)\n\n\nt\nB\n(\ns\nt\n)\n!#\n=\nT\n\n1\nX\nt\n=0\nE\n\u02dd\n\u02d8\nP\n\n\"\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\n\nt\n \nT\n\n1\nX\nj\n\nt\n\nj\n\nt\nR\n(\ns\nj\n;a\nj\n)\n\nB\n(\ns\nt\n)\n!#\n(17.11)\nTherefore,wewillgetatestimatorforestimatingthe\nr\n\n(\n\n)witha\nchoiceof\nB\n(\n\n).Thebeofintroducingaproper\nB\n(\n\n)|which\nisoftenreferredtoasa\nbaseline\n|isthatithelpsreducethevarianceofthe\nestimator.\n3\nItturnsoutthatanearoptimalestimatorwouldbetheexpected\nfuturepayE\nh\nP\nT\n\n1\nj\n\nt\n\nj\n\nt\nR\n(\ns\nj\n;a\nj\n)\nj\ns\nt\ni\n,whichisprettymuchthesameasthe\nvaluefunction\nV\n\u02c7\n\n(\ns\nt\n)(ifweignorethebetweenand\nhorizon.)Hereonecouldestimatethevaluefunction\nV\n\u02c7\n\n(\n\n)inacrudeway,\nbecauseitsprecisevaluedoesn'tthemeanoftheestimatorbutonly\nthevariance.Thisleadstoapolicygradientalgorithmwithbaselinesstated\ninAlgorithm9.\n4\n3\nAsaheuristicbutillustratingexample,supposefora\nt\n,thefuturereward\nP\nT\n\n1\nj\n\nt\n\nj\n\nt\nR\n(\ns\nj\n;a\nj\n)randomlytakestwovalues1000+1and1000\n\n2withequalproba-\nbility,andthecorrespondingvaluesfor\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)arevector\nz\nand\n\nz\n.(Notethat\nbecauseE[\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)]=0,if\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)canonlytaketwovaluesuniformly,\nthenthetwovalueshavetotwovectorsinanoppositedirection.)Inthiscase,without\nsubtractingthebaseline,theestimatorstaketwovalues(1000+1)\nz\nand\n\n(1000\n\n2)\nz\n,\nwhereasaftersubtractingabaselineof1000,theestimatorhastwovalues\nz\nand2\nz\n.The\nlatterestimatorhasmuchlowervariancecomparedtotheoriginalestimator.\n4\nWenotethattheestimatorofthegradientinthealgorithmdoesnotexactlymatch\ntheequation17.11.Ifwemultiply\n\nt\ninthesummandofequation(17.13),thentheywill\nexactlymatch.Removingsuchdiscountfactorsempiricallyworkswellbecauseitgivesa\nlargeupdate.\n"}, {"page_number": 212, "text": "212\nAlgorithm9\nVanillapolicygradientwithbaseline\nfor\ni\n=1\n;\n\ndo\nCollectasetoftrajectoriesbyexecutingthecurrentpolicy.Use\nR\n\nt\nasashorthandfor\nP\nT\n\n1\nj\n\nt\n\nj\n\nt\nR\n(\ns\nj\n;a\nj\n)\nFitthebaselinebyafunction\nB\nthatminimizes\nX\n\u02dd\nX\nt\n(\nR\n\nt\n\nB\n(\ns\nt\n))\n2\n(17.12)\nUpdatethepolicyparameter\n\nwiththegradientestimator\nX\n\u02dd\nX\nt\nr\n\nlog\n\u02c7\n\n(\na\nt\nj\ns\nt\n)\n\n(\nR\n\nt\n\nB\n(\ns\nt\n))(17.13)\n"}, {"page_number": 213, "text": "Bibliography\nMikhailBelkin,DanielHsu,SiyuanMa,andSoumikMandal.Reconciling\nmodernmachine-learningpracticeandtheclassicalbias{variancetrade-\n\nProceedingsoftheNationalAcademyofSciences\n,116(32):15849{15854,\n2019.\nMikhailBelkin,DanielHsu,andJiXu.Twomodelsofdoubledescentfor\nweakfeatures.\nSIAMJournalonMathematicsofDataScience\n,2(4):1167{\n1180,2020.\nDavidMBlei,AlpKucukelbir,andJonDVariationalinference:\nAreviewforstatisticians.\nJournaloftheAmericanStatisticalAssociation\n,\n112(518):859{877,2017.\nRishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,Simran\nArora,SydneyvonArx,MichaelSBernstein,JeannetteBohg,Antoine\nBosselut,EmmaBrunskill,etal.Ontheopportunitiesandrisksoffoun-\ndationmodels.\narXivpreprintarXiv:2108.07258\n,2021.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKa-\nplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSas-\ntry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.\nAdvances\ninneuralinformationprocessingsystems\n,33:1877{1901,2020.\nTingChen,SimonKornblith,MohammadNorouzi,andHinton.\nAsimpleframeworkforcontrastivelearningofvisualrepresentations.In\nInternationalConferenceonMachineLearning\n,pages1597{1607.PMLR,\n2020.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.Bert:\nPre-trainingofdeepbidirectionaltransformersforlanguageunderstand-\ning.In\nProceedingsofthe2019ConferenceoftheNorthAmericanChapter\noftheAssociationforComputationalLinguistics:HumanLanguageTech-\nnologies,Volume1(LongandShortPapers)\n,pages4171{4186,2019.\n213\n"}, {"page_number": 214, "text": "214\nZHaoChen,ColinWei,JasonDLee,andTengyuMa.Shapematters:\nUnderstandingtheimplicitbiasofthenoisecovariance.\narXivpreprint\narXiv:2006.08680\n,2020.\nTrevorHastie,AndreaMontanari,SaharonRosset,andRyanJTibshirani.\nSurprisesinhigh-dimensionalridgelessleastsquaresinterpolation.2019.\nTrevorHastie,AndreaMontanari,SaharonRosset,andRyanJTibshirani.\nSurprisesinhigh-dimensionalridgelessleastsquaresinterpolation.\nThe\nAnnalsofStatistics\n,50(2):949{986,2022.\nGarethJames,DanielaWitten,TrevorHastie,andRobertTibshirani.\nAn\nintroductiontostatisticallearning,secondedition\n,volume112.Springer,\n2021.\nDiederikPKingmaandJimmyBa.Adam:Amethodforstochasticopti-\nmization.\narXivpreprintarXiv:1412.6980\n,2014.\nDiederikPKingmaandMaxWelling.Auto-encodingvariationalbayes.\narXiv\npreprintarXiv:1312.6114\n,2013.\nYupingLuo,HuazheXu,YuanzhiLi,YuandongTian,TrevorDarrell,and\nTengyuMa.Algorithmicframeworkformodel-baseddeepreinforcement\nlearningwiththeoreticalguarantees.In\nInternationalConferenceonLearn-\ningRepresentations\n,2018.\nSongMeiandAndreaMontanari.Thegeneralizationerrorofrandomfeatures\nregression:Preciseasymptoticsandthedoubledescentcurve.\nCommuni-\ncationsonPureandAppliedMathematics\n,75(4):667{766,2022.\nPreetumNakkiran.Moredatacanhurtforlinearregression:Sample-wise\ndoubledescent.2019.\nPreetumNakkiran,PrayaagVenkat,ShamKakade,andTengyuMa.Optimal\nregularizationcanmitigatedoubledescent.2020.\nManfredOpper.Statisticalmechanicsoflearning:Generalization.\nThehand-\nbookofbraintheoryandneuralnetworks\n,pages922{925,1995.\nManfredOpper.Learningtogeneralize.\nFrontiersofLife\n,3(part2):763{775,\n2001.\nAshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,\nAidanNGomez,LukaszKaiser,andIlliaPolosukhin.Attentionisallyou\nneed.\narXivpreprintarXiv:1706.03762\n,2017.\n"}, {"page_number": 215, "text": "215\nBlakeWoodworth,SuriyaGunasekar,JasonDLee,EdwardMoroshko,Pe-\ndroSavarese,ItayGolan,DanielSoudry,andNathanSrebro.Kerneland\nrichregimesinoverparametrizedmodels.\narXivpreprintarXiv:2002.09277\n,\n2020.\n"}]}